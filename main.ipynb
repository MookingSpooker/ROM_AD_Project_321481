{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546ff8e7",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0bd42",
   "metadata": {},
   "source": [
    "### Project initialization and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be51f045",
   "metadata": {},
   "source": [
    "Importing all of the libraries that will be used. In the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3a1060f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T20:20:41.749195Z",
     "start_time": "2025-12-01T20:20:40.974484Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71b394f",
   "metadata": {},
   "source": [
    "Display options (make this clearer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c0bed393cbc4ef33",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T20:20:42.932449Z",
     "start_time": "2025-12-01T20:20:42.929880Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc3901c",
   "metadata": {},
   "source": [
    "Explain what this is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f7c42098",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T20:20:44.616519Z",
     "start_time": "2025-12-01T20:20:44.611949Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected successfully!\n"
     ]
    }
   ],
   "source": [
    "DB_PATH = \"viewer_interactions.db\"\n",
    "\n",
    "try:\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    print(\"Connected successfully!\")\n",
    "except sqlite3.Error as e:\n",
    "    print(\"Connection failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7754d30f62a03707",
   "metadata": {},
   "source": [
    "Listing all the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dc58ea789f34c04d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T20:21:30.792794Z",
     "start_time": "2025-12-01T20:21:30.758825Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in the database:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [name]\n",
       "Index: []"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tables_query = \"\"\"\n",
    "               SELECT name\n",
    "               FROM sqlite_master\n",
    "               WHERE type='table'\n",
    "               ORDER BY name; \\\n",
    "               \"\"\"\n",
    "\n",
    "tables_df = pd.read_sql_query(tables_query, conn)\n",
    "print(\"Tables in the database:\")\n",
    "display(tables_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0b27cbe9adf61ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T20:33:23.240670Z",
     "start_time": "2025-12-01T20:33:23.209602Z"
    }
   },
   "outputs": [],
   "source": [
    "table_names = tables_df[\"name\"].tolist()\n",
    "\n",
    "schemas = {}\n",
    "\n",
    "for table in table_names:\n",
    "    pragma_query = f\"PRAGMA table_info({table});\"\n",
    "    schema_df = pd.read_sql_query(pragma_query, conn)\n",
    "    schemas[table] = schema_df\n",
    "    print(f\"\\nSchema for table '{table}':\")\n",
    "    display(schema_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d012ac3d32f843",
   "metadata": {},
   "source": [
    "Creating a dictionary of type table_name -> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a171008d9bf37415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = {\n",
    "#    \"interactions\": DataFrame with columns [user_id, movie_id, rating, timestamp, ...],\n",
    "#    \"movies\":       DataFrame with columns [movie_id, title, genres, year, ...],\n",
    "#    \"users\":        DataFrame with columns [user_id, age, country, ...]\n",
    "# }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23dfb5ab",
   "metadata": {},
   "source": [
    "Data frame shape where shape is the number of rows and the second number is the number of columns. We are specifically grabbing the names of the sets of tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d9e3c8ef2e5554b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T00:54:30.892497Z",
     "start_time": "2025-12-02T00:54:25.630180Z"
    }
   },
   "outputs": [],
   "source": [
    "dfs = {}\n",
    "\n",
    "for t in table_names:\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {t};\", conn)\n",
    "    dfs[t] = df\n",
    "    print(f\"\\nLoaded table '{t}' with shape {df.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34599363753164d6",
   "metadata": {},
   "source": [
    "Example of using the dfs dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5d963a2b260481b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:29:35.086495Z",
     "start_time": "2025-12-01T21:29:35.063593Z"
    }
   },
   "outputs": [],
   "source": [
    "def search_by_parameter(table_name, key, value):\n",
    "    df = dfs[table_name]\n",
    "\n",
    "    if value is None:\n",
    "        return df[df[key].isna()]\n",
    "\n",
    "    return df[df[key] == value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7634d35d",
   "metadata": {},
   "source": [
    "Giacomo thing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d97951c7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-01T21:31:50.716554Z",
     "start_time": "2025-12-01T21:31:50.689838Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DATA DICTIONARY ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables = pd.read_sql(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\",\n",
    "    conn\n",
    "    )['name'].tolist()\n",
    "\n",
    "print(\"=== DATA DICTIONARY ===\\n\")\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"Table: {table}\")\n",
    "    print(\"-\" * (7 + len(table)))\n",
    "\n",
    "    # Get actual column info from PRAGMA but filter to nice output\n",
    "    schema = pd.read_sql(f\"PRAGMA table_info('{table}')\", conn)\n",
    "\n",
    "    # Keep only real schema fields you want (remove cid, default, pk if desired)\n",
    "    clean_schema = schema[['name', 'type', ]]\n",
    "\n",
    "    print(clean_schema.to_string(index=False))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "def25638f8eb473b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T22:04:06.498498Z",
     "start_time": "2025-12-02T22:04:06.203752Z"
    }
   },
   "outputs": [],
   "source": [
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name} missing values (%):\")\n",
    "    missing_pct = df.isna().mean() * 100\n",
    "    display(missing_pct.to_frame(\"missing_%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfcc80826007560",
   "metadata": {},
   "source": [
    "Counting all missing values, diagnostics purposes only"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78394717c371ac27",
   "metadata": {},
   "source": [
    "A function to calculate missing std. ratings of films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1395d0b962d46cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T22:04:01.400990Z",
     "start_time": "2025-12-02T22:04:00.712349Z"
    }
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'viewer_ratings'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 21\u001b[39m\n\u001b[32m     18\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m film_stats[[\u001b[33m'\u001b[39m\u001b[33mmovie_id\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33mstd_rating\u001b[39m\u001b[33m'\u001b[39m]]\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# Compute std for all films\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m viewer_ratings = \u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mviewer_ratings\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     22\u001b[39m film_std = compute_film_std(viewer_ratings)\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# Load movies_statistics\u001b[39;00m\n",
      "\u001b[31mKeyError\u001b[39m: 'viewer_ratings'"
     ]
    }
   ],
   "source": [
    "def compute_film_std(df):\n",
    "    film_stats = (\n",
    "        df.groupby('movie_id')['rating']\n",
    "        .apply(list)\n",
    "        .reset_index(name='ratings')\n",
    "    )\n",
    "\n",
    "    def manual_std(ratings):\n",
    "        ratings = np.array(ratings)\n",
    "        n = len(ratings)\n",
    "        if n <= 1:\n",
    "            return 0.0\n",
    "        mean = ratings.mean()\n",
    "        return np.sqrt(((ratings - mean) ** 2).mean())\n",
    "\n",
    "    film_stats['std_rating'] = film_stats['ratings'].apply(manual_std)\n",
    "\n",
    "    return film_stats[['movie_id', 'std_rating']]\n",
    "\n",
    "# Compute std for all films\n",
    "viewer_ratings = dfs['viewer_ratings']\n",
    "film_std = compute_film_std(viewer_ratings)\n",
    "\n",
    "# Load movies_statistics\n",
    "movies_stats = dfs[\"movie_statistics\"]\n",
    "\n",
    "# Compute the old percentage before merging\n",
    "old_null_pct = dfs[\"movie_statistics\"][\"std_rating\"].isna().mean() * 100\n",
    "\n",
    "# Merge new std values\n",
    "movies_stats = movies_stats.merge(\n",
    "    film_std,\n",
    "    on=\"movie_id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_new\")\n",
    ")\n",
    "\n",
    "# Replace old std_rating with the new one\n",
    "movies_stats[\"std_rating\"] = movies_stats[\"std_rating_new\"]\n",
    "movies_stats.drop(columns=[\"std_rating_new\"], inplace=True)\n",
    "\n",
    "# Save updated table\n",
    "dfs[\"movie_statistics\"] = movies_stats\n",
    "\n",
    "# Compute new percentage ---\n",
    "new_null_pct = movies_stats[\"std_rating\"].isna().mean() * 100\n",
    "\n",
    "# absolute improvement (percentage points)\n",
    "improvement_abs = old_null_pct - new_null_pct\n",
    "\n",
    "# relative improvement (how many percent of the original NaNs we removed)\n",
    "improvement_rel = (improvement_abs / old_null_pct) * 100 if old_null_pct > 0 else 0\n",
    "\n",
    "print(f\"Missing values reduced from {old_null_pct:.2f}% to {new_null_pct:.2f}%.\")\n",
    "print(f\"Absolute improvement: {improvement_abs:.2f}%\")\n",
    "print(f\"Relative improvement: {improvement_rel:.2f}% better than before.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "541a627d968fd125",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'movie_statistics'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m#Figure out how to drop na values in general\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m movie_stats = \u001b[43mdfs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmovie_statistics\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m#print(movie_stats[movie_stats['std_rating'] == pd.isnull(movie_stats['std_rating'])])\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m#movie_stats = movie_stats[movie_stats['std_rating'].notna()]\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m#print(movie_stats)\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mBefore cleaning: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(movie_stats)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m movies\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mKeyError\u001b[39m: 'movie_statistics'"
     ]
    }
   ],
   "source": [
    "#Figure out how to drop na values in general\n",
    "movie_stats = dfs['movie_statistics']\n",
    "#print(movie_stats[movie_stats['std_rating'] == pd.isnull(movie_stats['std_rating'])])\n",
    "#movie_stats = movie_stats[movie_stats['std_rating'].notna()]\n",
    "#print(movie_stats)\n",
    "\n",
    "print(f\"Before cleaning: {len(movie_stats)} movies\")\n",
    "movie_stats = movie_stats.dropna(subset=['std_rating'])\n",
    "dfs['movie_statistics'] = movie_stats\n",
    "print(f\"After removing single-rating movies: {len(movie_stats)} movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4518d",
   "metadata": {},
   "source": [
    "Calculating the missing total_ratings of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6c1bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T22:03:54.329686Z",
     "start_time": "2025-12-02T22:03:52.551810Z"
    }
   },
   "outputs": [],
   "source": [
    "movie_stats = dfs[\"movie_statistics\"]\n",
    "viewer_ratings = dfs[\"viewer_ratings\"]\n",
    "\n",
    "# Collecting all the movies with absent total_rating in a dictionary\n",
    "missing_dict = {}\n",
    "\n",
    "missing = search_by_parameter('movie_statistics', 'total_ratings', None)\n",
    "missing_dict = {row.movie_id: 0 for row in missing.itertuples(index=False)}\n",
    "\n",
    "# Iterating through viewer_ratings and manually counting the ratings for each film\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "    if movie_id in missing_dict:\n",
    "        missing_dict[movie_id] += 1\n",
    "\n",
    "# Update movie_stats\n",
    "for row in movie_stats.itertuples(index=True):\n",
    "    if row.movie_id in missing_dict:\n",
    "        movie_stats.at[row.Index, \"total_ratings\"] = missing_dict[row.movie_id]\n",
    "\n",
    "dfs[\"movie_statistics\"] = movie_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9afb8",
   "metadata": {},
   "source": [
    "Calculating Missing Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4756a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T22:03:43.006098Z",
     "start_time": "2025-12-02T22:03:40.956Z"
    }
   },
   "outputs": [],
   "source": [
    "# Finding movies with null avg_rating\n",
    "missing_avg = search_by_parameter('movie_statistics', 'avg_rating', None)\n",
    "\n",
    "# Creating a dict of type { movie_id : avg_rating }\n",
    "# set 0 as base value for now, might change it later\n",
    "missing_avg_dict = {row.movie_id: 0 for row in missing_avg.itertuples(index=False)}\n",
    "\n",
    "# Storing the sum of all ratings for each movie\n",
    "rating_sums = {movie_id: 0 for movie_id in missing_avg_dict}\n",
    "\n",
    "# Iterating through viewer ratings and adding to sum if movie_id matches\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if movie_id in missing_avg_dict:\n",
    "        rating_sums[movie_id] += rating\n",
    "\n",
    "for row in movie_statistics.itertuples():\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    if movie_id in rating_sums:\n",
    "        total = row.total_ratings # I'm assuming that my calculations of total_ratings per movie is correct ang i got rid of                            all null values\n",
    "\n",
    "        # IF FORE SOME MAGICAL REASON THERE IS STILL A NULL THEN IGNORE\n",
    "        if pd.isna(total) or total == 0:\n",
    "            avg = 0\n",
    "        else:\n",
    "            avg = rating_sums[movie_id] / total\n",
    "\n",
    "        movie_stats.at[row.Index, \"avg_rating\"] = avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21090df8",
   "metadata": {},
   "source": [
    "Calculating the missing min and max ratings for movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb36c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T22:03:46.247843Z",
     "start_time": "2025-12-02T22:03:46.109839Z"
    }
   },
   "outputs": [],
   "source": [
    "movie_stats = dfs[\"movie_statistics\"]\n",
    "viewer_ratings = dfs[\"viewer_ratings\"]\n",
    "\n",
    "# Find movies with missing min_rating and max_rating using your function\n",
    "missing_min = search_by_parameter('movie_statistics', 'min_rating', None)\n",
    "missing_max = search_by_parameter('movie_statistics', 'max_rating', None)\n",
    "\n",
    "# Combine them as some movies may be in both\n",
    "missing_ids = set(missing_min[\"movie_id\"]) | set(missing_max[\"movie_id\"])\n",
    "\n",
    "# Take only ratings for the movies we care\n",
    "relevant_ratings = viewer_ratings[viewer_ratings[\"movie_id\"].isin(missing_ids)]\n",
    "\n",
    "# Building a nested dict {movie_id : {\"min\": ..., \"max\": ...}}\n",
    "min_max_dict = {}\n",
    "\n",
    "for row in relevant_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if movie_id not in min_max_dict:\n",
    "        min_max_dict[movie_id] = {\"min\": rating, \"max\": rating}\n",
    "    else:\n",
    "        if rating < min_max_dict[movie_id][\"min\"]:\n",
    "            min_max_dict[movie_id][\"min\"] = rating\n",
    "        if rating > min_max_dict[movie_id][\"max\"]:\n",
    "            min_max_dict[movie_id][\"max\"] = rating\n",
    "\n",
    "# Update movie_statistics\n",
    "for row in movie_stats.itertuples(index=True):\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    if movie_id in min_max_dict:\n",
    "        if pd.isna(row.min_rating):\n",
    "            movie_stats.at[row.Index, \"min_rating\"] = min_max_dict[movie_id][\"min\"]\n",
    "        if pd.isna(row.max_rating):\n",
    "            movie_stats.at[row.Index, \"max_rating\"] = min_max_dict[movie_id][\"max\"]\n",
    "\n",
    "dfs[\"movie_statistics\"] = movie_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83870a25990a35e",
   "metadata": {},
   "source": [
    "Finding missing unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9dd911b77bbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_stats = dfs[\"movie_statistics\"]\n",
    "viewer_ratings = dfs[\"viewer_ratings\"]\n",
    "\n",
    "missing_unique = search_by_parameter('movie_statistics', 'unique_users', None)\n",
    "\n",
    "# creating my favorite movie set\n",
    "missing_movie_ids = {row.movie_id for row in missing_unique.itertuples(index=False)}\n",
    "\n",
    "print(f\"Фильмов с NULL unique_users: {len(missing_movie_ids)}\")\n",
    "\n",
    "# creating a dict movie_id: customer_id\n",
    "unique_users_dict = {movie_id: set() for movie_id in missing_movie_ids}\n",
    "\n",
    "# Gathering unique users\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    # as always im getting only those movies which have null for unique users\n",
    "    if movie_id in unique_users_dict:\n",
    "        unique_users_dict[movie_id].add(row.customer_id)\n",
    "\n",
    "# Counting unique users\n",
    "updated = 0\n",
    "for movie_id, users in unique_users_dict.items():\n",
    "    count = len(users)  # unique users count\n",
    "\n",
    "    movie_stats.loc[\n",
    "        movie_stats[\"movie_id\"] == movie_id,\n",
    "        \"unique_users\"\n",
    "    ] = count\n",
    "\n",
    "    updated += 1\n",
    "\n",
    "# just in case if a movie has 0 ratings im setting unique users to 0\n",
    "movie_stats[\"unique_users\"] = movie_stats[\"unique_users\"].fillna(0).astype(int)\n",
    "\n",
    "# Updating\n",
    "dfs[\"movie_statistics\"] = movie_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b587366",
   "metadata": {},
   "source": [
    "Checking for any duplicate movie ids and removing them from all datasets to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b446228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this must run before other code blocks\n",
    "rating_data = pd.read_sql(\"SELECT * FROM viewer_ratings\", conn)\n",
    "user_data = pd.read_sql(\"SELECT * FROM user_statistics\", conn)\n",
    "movie_data = pd.read_sql(\"SELECT * FROM movies\", conn)\n",
    "movie_statistics = pd.read_sql(\"SELECT * FROM movie_statistics\", conn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c36b6",
   "metadata": {},
   "source": [
    "Merging the Movies and movie statis filling in missing values on either dataset and converting all of the dates to type DateTime as well as all counts and years to integers.\n",
    "This is in order to clean our movie data before merging it with our user data to fill in any recoverable missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46319c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bad movie ids to remove:  [21756, 19549]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# this will print out the duplicate movie_ids (empty if none)\n",
    "bad_ids_movies =  movie_data[movie_data['movie_id'].duplicated (keep = False)]['movie_id'].unique().tolist()\n",
    "bad_ids_stats =  movie_statistics[movie_statistics['movie_id'].duplicated (keep = False)]['movie_id'].unique().tolist()\n",
    "bad_ids = list(set(bad_ids_movies + bad_ids_stats))\n",
    "print('bad movie ids to remove: ', bad_ids)\n",
    "\n",
    "# now these bad_ids will be removed from all of the data\n",
    "rating_data = rating_data[~rating_data['movie_id'].isin(bad_ids)]\n",
    "movie_data = movie_data[~movie_data['movie_id'].isin(bad_ids)]\n",
    "movie_statistics = movie_statistics[~movie_statistics['movie_id'].isin(bad_ids)]\n",
    "\n",
    "\n",
    "# changing types to int and datetime\n",
    "movie_data['year_of_release'] = movie_data['year_of_release'].astype('Int64')\n",
    "movie_statistics['total_ratings'] = movie_statistics['total_ratings'].astype('Int64')\n",
    "movie_statistics['unique_users'] = movie_statistics['unique_users'].astype('Int64')\n",
    "movie_statistics['year_of_release'] = movie_statistics['year_of_release'].astype('Int64')\n",
    "movie_statistics['first_rating_date'] = pd.to_datetime(movie_statistics['first_rating_date'], errors = 'coerce')\n",
    "movie_statistics['last_rating_date'] = pd.to_datetime(movie_statistics['last_rating_date'], errors = 'coerce')\n",
    "\n",
    "# merging the two datasets\n",
    "movie_full = movie_data.merge(movie_statistics, on = 'movie_id', how = 'outer', suffixes = ('_movies', '_stats'))\n",
    "# filling in missing values on either dataset to create a full movie dataset\n",
    "movie_full['title'] = movie_full['title_movies'].combine_first(movie_full['title_stats'])\n",
    "movie_full['year_of_release'] = movie_full['year_of_release_movies'].combine_first(movie_full['year_of_release_stats'])\n",
    "# dropping the temporary movies and stats columns\n",
    "movie_full = movie_full.drop(columns = ['title_movies', 'title_stats', 'year_of_release_movies', 'year_of_release_stats'])\n",
    "dfs['movie_full'] = movie_full"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec12db",
   "metadata": {},
   "source": [
    "- Converts the date parameter in viewer_ratings to datetime.\n",
    "- Merges viewer_ratings, movies, movie_statistics and user_statistics into one dataset as merged_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40bfb82470b10a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-02T00:26:07.274966Z",
     "start_time": "2025-12-02T00:26:00.537183Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "rating_data['date'] = pd.to_datetime(rating_data['date'], errors = 'coerce')\n",
    "rating_data.dtypes\n",
    "# movie_statistics['first_rating_date'] = \n",
    "movie_statistics['total_ratings'] = movie_statistics['total_ratings'].astype('Int64')\n",
    "movie_statistics['first_rating_date'] = pd.to_datetime(movie_statistics['first_rating_date'], errors = 'coerce')\n",
    "\n",
    "\n",
    "\n",
    "merged_data = rating_data.merge(movie_data, on = 'movie_id', how = 'left')\n",
    "merged_data = merged_data.merge(user_data, on = 'customer_id', how = 'left')\n",
    "merged_data = merged_data.merge(movie_statistics, on = 'movie_id', how = 'left')\n",
    "# avg rating standard rating mean rating\n",
    "'''\n",
    "want to merge\n",
    "- title\n",
    "- year of release\n",
    "want to keep independent\n",
    "- avg rating\n",
    "- std rating\n",
    "- min rating\n",
    "- max rating\n",
    "- first rating date\n",
    "- last rating date\n",
    "'''\n",
    "\n",
    "print(\"Total columns:\", len(merged_data.columns))\n",
    "list(merged_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908f360",
   "metadata": {},
   "source": [
    "Giacomo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ac23fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e71d80a8",
   "metadata": {},
   "source": [
    "Ryder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df1e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Markdown cell above\n",
    "\"\"\"\n",
    "### Global Rating Distribution\n",
    "\n",
    "There is a clear over positive bias. This can be due to the fact that this was done on a streaming platform where users are more likely to rate higher\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(11, 6)) # made it this size for a better fit in the read me\n",
    "ax = sns.histplot(\n",
    "    data=dfs['viewer_ratings'],\n",
    "    x='rating',\n",
    "    bins=5,\n",
    "    discrete=True, #tells seaborn to treat x as integer values\n",
    "    color=\"#0062ff\",\n",
    "    edgecolor='white',\n",
    "    alpha=0.85,  #transparency\n",
    "    linewidth=1.5\n",
    ")\n",
    "\n",
    "# Add exact counts on top of each bar\n",
    "for rect in ax.patches:\n",
    "    height = rect.get_height()\n",
    "    ax.text(\n",
    "        rect.get_x() + rect.get_width()/2., \n",
    "        height + 200_000,                    # a bit above the bar\n",
    "        f'{int(height):,}',                  # adds commas: 12,345,678\n",
    "        ha='center', va='bottom', fontsize=12, fontweight='bold'\n",
    "    )\n",
    "\n",
    "# Clean y-axis\n",
    "plt.ylabel('Number of Ratings (millions)', fontsize=12)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))\n",
    "\n",
    "plt.title('Distribution of Viewer Ratings\\n(Strong Positive Bias)', \n",
    "          fontsize=15, pad=20)\n",
    "plt.xlabel('Rating (1–5)', fontsize=12)\n",
    "plt.xticks(range(0, 7))\n",
    "plt.ylim(0, 13_500_000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c540e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test code block to commit"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
