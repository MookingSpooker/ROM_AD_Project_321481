{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546ff8e7",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0bd42",
   "metadata": {},
   "source": [
    "### Project initialization and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be51f045",
   "metadata": {},
   "source": [
    "Importing all of the libraries that will be used. In the project."
   ]
  },
  {
   "cell_type": "code",
   "id": "f3a1060f",
   "metadata": {},
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.mixture import GaussianMixture\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3bb93472f940c938",
   "metadata": {},
   "source": [
    "Connecting the DB to SQL"
   ]
  },
  {
   "cell_type": "code",
   "id": "58e0738c73228680",
   "metadata": {},
   "source": [
    "DB_PATH = \"viewer_interactions.db\"\n",
    "\n",
    "try:\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    print(\"Connected successfully!\")\n",
    "except sqlite3.Error as e:\n",
    "    print(\"Connection failed:\", e)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dcb72a1f7b4374a3",
   "metadata": {},
   "source": [
    "# Display options\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 120)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27aeae317f3195dc",
   "metadata": {},
   "source": [
    "Listing all the tables"
   ]
  },
  {
   "cell_type": "code",
   "id": "cf5dc5f8997e3c70",
   "metadata": {},
   "source": [
    "tables_query = \"\"\"\n",
    "               SELECT name\n",
    "               FROM sqlite_master\n",
    "               WHERE type='table'\n",
    "               ORDER BY name; \\\n",
    "               \"\"\"\n",
    "\n",
    "tables_df = pd.read_sql_query(tables_query, conn)\n",
    "print(\"Tables in the database:\")\n",
    "display(tables_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c46cee5dd503efb6",
   "metadata": {},
   "source": [
    "Creating a dictionary of type table_name -> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "id": "74de9cc40660ddfd",
   "metadata": {},
   "source": [
    "table_names = tables_df[\"name\"].tolist()\n",
    "\n",
    "schemas = {}\n",
    "\n",
    "for table in table_names:\n",
    "    pragma_query = f\"PRAGMA table_info({table});\"\n",
    "    schema_df = pd.read_sql_query(pragma_query, conn)\n",
    "    schemas[table] = schema_df\n",
    "    print(f\"\\nSchema for table '{table}':\")\n",
    "    display(schema_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "DF_MAP is the lookup table that links each global DataFrame name (for instance movies_stats) to its actual entry inside the dfs dictionary. It is filled by add_new_df function whenever a new dataframe is registered.\n",
   "id": "eb246e3a2c2abbb7"
  },
  {
   "cell_type": "code",
   "id": "40468af8",
   "metadata": {},
   "source": [
    "# creating a dictionary to map dataframes\n",
    "DF_MAP = {}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Methods",
   "id": "e3a9b52d9d7cfc5d"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Add new data frame function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ],
   "id": "ed380c3f39390a13"
  },
  {
   "cell_type": "markdown",
   "id": "65560f7e",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Registers a dataframe under a global variable name while keeping a main dfs dictionary as the source.\n",
    "\n",
    "**Inputs:**<br>\n",
    "Takes global_name (new global variable name), dfs_key (dictionary key in dfs), and optional dataframe (only used if dfs_key is not already present) as inputs.\n",
    "\n",
    "**Behaviour:**<br>\n",
    "If dfs_key already exists in dfs, it simply maps that existing dataframe to the requested global name and records the mapping in DF_MAP. Otherwise, it raises an error when no dataframe is provided, or it stores the new dataframe in both dfs and the global namespace, updating DF_MAP.\n",
    "\n",
    "**Output:**<br>\n",
    "Doesn't return anything, prints whether an existing or new dataframe was mapped.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8701b129",
   "metadata": {},
   "source": [
    "def add_new_df(global_name: str, dfs_key: str, dataframe=None):\n",
    "\n",
    "    # if dataframe already exists in dfs → ignore \"dataframe\" input\n",
    "    if dfs_key in dfs:\n",
    "        globals()[global_name] = dfs[dfs_key]\n",
    "        DF_MAP[global_name] = dfs_key\n",
    "        print(f\"Mapped existing dfs['{dfs_key}'] to global '{global_name}'\")\n",
    "        return\n",
    "\n",
    "    # new dataframe\n",
    "    if dataframe is None:\n",
    "        raise ValueError(\n",
    "            f\"dfs_key '{dfs_key}' does not exist in dfs and no dataframe was provided.\"\n",
    "        )\n",
    "\n",
    "    dfs[dfs_key] = dataframe\n",
    "    globals()[global_name] = dataframe\n",
    "    DF_MAP[global_name] = dfs_key\n",
    "\n",
    "    print(f\"Added NEW df: global '{global_name}' → dfs['{dfs_key}']\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Sync data frame function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ],
   "id": "52e00e810b2aa439"
  },
  {
   "cell_type": "markdown",
   "id": "e1ae02fc",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Refreshing the global dataframe variables so they always match the newest versions stored inside the dfs dictionary.\n",
    "\n",
    "**Inputs:**<br>\n",
    "One or more dataframe objects that currently exist as global variables.\n",
    "\n",
    "**Behavior:**<br>\n",
    "For each passed dataframe, it looks up the corresponding DF_MAP entry and replaces the global variable with the latest version from dfs. If a dataframe isn’t found in DF_MAP, it raises an error.\n",
    "\n",
    "**Output:**<br>\n",
    "Silently updates globals or raises on unknown inputs."
   ]
  },
  {
   "cell_type": "code",
   "id": "f1731d00b21cbbf",
   "metadata": {},
   "source": [
    "def sync_dataframe(*dfs_to_refresh):\n",
    "    globals_dict = globals()\n",
    "\n",
    "    for df_obj in dfs_to_refresh:\n",
    "        updated = False\n",
    "\n",
    "        for global_name, dfs_key in DF_MAP.items():\n",
    "            if df_obj is globals_dict[global_name]:\n",
    "                globals_dict[global_name] = dfs[dfs_key]\n",
    "                updated = True\n",
    "                break\n",
    "\n",
    "        if not updated:\n",
    "            raise ValueError(\"Unknown dataframe passed!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Search by parameter function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ],
   "id": "4705ab12e35df441"
  },
  {
   "cell_type": "markdown",
   "id": "1303d04b68150eb6",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Searches for required parameters in dataframes\n",
    "\n",
    "**Inputs:**<br>\n",
    "table_name (dataframe key in dfs), key (column name), and value (target value or None to locate nulls).\n",
    "\n",
    "**Behavior:**<br>\n",
    "Retrieves the dataframe from dfs, then returns rows where the column matches value. If value is None, it returns rows with nulls in that column.\n",
    "\n",
    "**Output:** <br>\n",
    "Returns a filtered dataframe matching the search condition."
   ]
  },
  {
   "cell_type": "code",
   "id": "5d963a2b260481b4",
   "metadata": {},
   "source": [
    "def search_by_parameter(table_name, key, value):\n",
    "    df = dfs[table_name]\n",
    "\n",
    "    if value is None:\n",
    "        return df[df[key].isna()]\n",
    "\n",
    "    return df[df[key] == value]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Manual std values calculation function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ],
   "id": "73af4bb57e56160a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Purpose:**<br>\n",
    "Computes the standard deviation, see the used formula in README\n",
    "\n",
    "**Inputs:**<br>\n",
    "Values iterable convertible to a NumPy array.\n",
    "\n",
    "**Behavior:**<br>\n",
    " Converts to float array, returns 0.0 for length is lees or equal to 1, otherwise calculates mean and square-root of mean squared deviation.\n",
    "\n",
    "**Output:**<br>\n",
    "A float representing the standard deviation"
   ],
   "id": "9be82224302a0ef8"
  },
  {
   "cell_type": "code",
   "id": "5b55fbbe328695f4",
   "metadata": {},
   "source": [
    "# make sure no nulls are included in the calculation!\n",
    "\n",
    "def manual_std(values):\n",
    "    arr = np.array(values, dtype=float)\n",
    "\n",
    "    n = len(arr)\n",
    "\n",
    "    if n <= 1:\n",
    "        return 0.0\n",
    "\n",
    "    mean = arr.mean()\n",
    "    return np.sqrt(((arr - mean) ** 2).mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Safe string to datetime convertion function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ],
   "id": "80b63fe0fc34e27c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Purpose:**<br>\n",
    "Parsing a \"YYYY-MM-DD\" formatted string into a datetime.\n",
    "\n",
    "**Inputs:**<br>\n",
    "String in a \"YYYY-MM-DD\" format.\n",
    "\n",
    "**Behavior:**<br>\n",
    "Converting string to a datetime.strptime type or None for null/NaN\n",
    "\n",
    "**Output:**<br>\n",
    "Returns a datetime object or None when parsing fails or input is missing."
   ],
   "id": "a8516a3c40143e36"
  },
  {
   "cell_type": "code",
   "id": "c3e541a4",
   "metadata": {},
   "source": [
    "def covert_string_to_date(date_str):\n",
    "    if date_str is None or pd.isna(date_str):\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    except:\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Elbow method</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ],
   "id": "394dcd26350a204a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Purpose:**<br>\n",
    "Evaluates KMeans inertia across a range of cluster counts to find an elbow point.\n",
    "\n",
    "**Inputs:**<br>\n",
    "Feature matrix X, cluster bounds k_min/k_max, random_state, and a \"plot\" bool to indicate necessity of plotting.\n",
    "\n",
    "**Behavior:**<br>\n",
    "Fits KMeans for each k, collects inertia values into a dataframe, and plots inertia vs k.\n",
    "\n",
    "**Output:**<br>\n",
    "A dataframe with columns k and inertia, optionally draws a plot for visual elbow inspection"
   ],
   "id": "d859cdffca32e6f7"
  },
  {
   "cell_type": "code",
   "id": "6d76218baeef6680",
   "metadata": {},
   "source": [
    "def elbow_method(X, k_min=1, k_max=10, random_state=42, plot=True):\n",
    "    results = []\n",
    "\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        kmeans = KMeans(\n",
    "            n_clusters=k,\n",
    "            random_state=random_state,\n",
    "            n_init=10\n",
    "        )\n",
    "        kmeans.fit(X)\n",
    "        results.append({\"k\": k, \"inertia\": kmeans.inertia_})\n",
    "\n",
    "    elbow_df = pd.DataFrame(results)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.plot(elbow_df[\"k\"], elbow_df[\"inertia\"], marker=\"o\")\n",
    "        plt.xlabel(\"Number of clusters (k)\")\n",
    "        plt.ylabel(\"Inertia\")\n",
    "        plt.title(\"Elbow method\")\n",
    "        plt.xticks(elbow_df[\"k\"])\n",
    "        plt.show()\n",
    "\n",
    "    return elbow_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Silhouette score computation function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ],
   "id": "40ede6253c9f2ddd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Purpose:**<br>\n",
    "Calculates silhouette score for clustering results\n",
    "\n",
    "**Inputs:**<br>\n",
    "Feature matrix X, cluster label array labels, optional sample_size, random_state, and distance metric.\n",
    "\n",
    "**Behavior:**<br>\n",
    "Validates matching lengths, then computes silhouette_score with the chosen metric.\n",
    "\n",
    "**Output:**<br>\n",
    "Returns a single float silhouette score summarizing cluster separation and cohesion."
   ],
   "id": "29b4363a923af0be"
  },
  {
   "cell_type": "code",
   "id": "6c10d4ecd680e8df",
   "metadata": {},
   "source": [
    "def compute_silhouette_score(X, labels, sample_size=None, random_state=42, metric=\"euclidean\"):\n",
    "    X = np.asarray(X)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    if X.shape[0] != labels.shape[0]:\n",
    "        raise ValueError(f\"X has {X.shape[0]} rows, but labels has {labels.shape[0]} elements\")\n",
    "\n",
    "    if sample_size is not None and sample_size < len(X):\n",
    "        rng = np.random.RandomState(random_state)\n",
    "        idx = rng.choice(len(X), size=sample_size, replace=False)\n",
    "        X_used = X[idx]\n",
    "        labels_used = labels[idx]\n",
    "    else:\n",
    "        X_used = X\n",
    "        labels_used = labels\n",
    "\n",
    "    score = silhouette_score(X_used, labels_used, metric=metric)\n",
    "    return score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Movie cluster inspection function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ],
   "id": "7e0890850b14e212"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Purpose:**<br>\n",
    "Lists movies belonging to a specific cluster.\n",
    "\n",
    "**Inputs:**<br>\n",
    "movies_df containing movie_id and cluster_id, merged_df with movie_id and title, and a target cluster_id to inspect.\n",
    "\n",
    "**Behavior:**<br>\n",
    "Looking for rows matching the cluster, displays descriptive stats, then merges titles onto those rows for readability.\n",
    "\n",
    "**Output:**<br>\n",
    "Returns a dataframe of cluster members with movie titles, along with a displayed statistical summary"
   ],
   "id": "e5da9fa697e891c7"
  },
  {
   "cell_type": "code",
   "id": "191c9313fa28c536",
   "metadata": {},
   "source": [
    "def inspect_movie_cluster(movies_df, merged_df, cluster_id):\n",
    "\n",
    "    cluster = movies_df[movies_df[\"cluster_id\"] == cluster_id]\n",
    "\n",
    "    display(cluster.describe())\n",
    "\n",
    "    detailed = cluster.merge(\n",
    "        merged_df[[\"movie_id\", \"title\"]].drop_duplicates(),\n",
    "        on=\"movie_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return detailed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Cluster visualization function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ],
   "id": "537073c7a8a02258"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Purpose:**<br>\n",
    "Visualizes clusters in two dimensions using PCA\n",
    "\n",
    "**Inputs:**<br>\n",
    "Scaled feature matrix X_scaled, cluster_labels, and optional cap on points per cluster to reduce clutter (only for visual purposes).\n",
    "\n",
    "**Behavior:**<br>\n",
    "Projects data to two principal components, builds a plotting dataframe, samples large clusters for readability, and scatters each cluster with size/edge styling based on cluster size. Adds variance percentages to axes, legend counts, and grid.\n",
    "\n",
    "**Output:**<br>\n",
    "Displays a PCA scatter plot showing cluster separation; no return value beyond visualization"
   ],
   "id": "7e122ccb388f5f52"
  },
  {
   "cell_type": "code",
   "id": "303dbd1509c90958",
   "metadata": {},
   "source": [
    "def plot_clusters_pca_2d(X_scaled, cluster_labels, max_points_per_cluster=1000):\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    df_plot = pd.DataFrame({\n",
    "        \"pc1\": X_pca[:, 0],\n",
    "        \"pc2\": X_pca[:, 1],\n",
    "        \"cluster\": cluster_labels\n",
    "    })\n",
    "\n",
    "    unique_clusters = sorted(df_plot[\"cluster\"].unique())\n",
    "\n",
    "    plt.figure(figsize=(9, 7))\n",
    "\n",
    "    for cid in unique_clusters:\n",
    "        cluster_points = df_plot[df_plot[\"cluster\"] == cid]\n",
    "\n",
    "        if len(cluster_points) > max_points_per_cluster:\n",
    "            cluster_points = cluster_points.sample(\n",
    "                max_points_per_cluster,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "        if len(df_plot[df_plot[\"cluster\"] == cid]) <= 50:\n",
    "            s = 20\n",
    "            edgecolor = \"black\"\n",
    "            linewidth = 0.7\n",
    "        else:\n",
    "            s = 20\n",
    "            edgecolor = \"none\"\n",
    "            linewidth = 0.0\n",
    "\n",
    "        plt.scatter(\n",
    "            cluster_points[\"pc1\"],\n",
    "            cluster_points[\"pc2\"],\n",
    "            s=s,\n",
    "            alpha=0.6,\n",
    "            label=f\"Cluster {cid} (n={len(df_plot[df_plot['cluster'] == cid])})\",\n",
    "            edgecolors=edgecolor,\n",
    "            linewidths=linewidth\n",
    "        )\n",
    "\n",
    "    var1, var2 = pca.explained_variance_ratio_\n",
    "    plt.xlabel(f\"PC1 ({var1:.1%} variance)\")\n",
    "    plt.ylabel(f\"PC2 ({var2:.1%} variance)\")\n",
    "    plt.title(\"K-means movie clusters (PCA 2D)\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c2f9cc01",
   "metadata": {},
   "source": [
    "### Database initialization"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>DFS</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ],
   "id": "20634283493ba272"
  },
  {
   "cell_type": "markdown",
   "id": "41f77086",
   "metadata": {},
   "source": [
    "**Definition:**<br>\n",
    "**dfs** is the central dictionary that caches every table from the SQLite database. During initialization each table name in table_names is read with pd.read_sql_query and stored under its table name key.\n",
    "\n",
    "**Behaviour:** <br>\n",
    "Takes a list of table names (table_names) and the SQL query results for each table as inputs. Then, pandas DataFrames is keyed by table name.\n",
    "\n",
    "**Usage:**<br>\n",
    "**dfs[key]** can be called to access tables, through registered global variables created with add_new_df function. Keeps globals in sync through sync_dataframe so analyses always reference the up-to-date dataframes"
   ]
  },
  {
   "cell_type": "code",
   "id": "f9fff70c",
   "metadata": {},
   "source": [
    "dfs = {} # dictionary that maps the name of the table to the related data frame\n",
    "\n",
    "for t in table_names:\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {t};\", conn)\n",
    "    dfs[t] = df\n",
    "    print(f\"\\nLoaded table '{t}' with shape {df.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2aebc136",
   "metadata": {},
   "source": [
    "# adding our base data frames\n",
    "add_new_df(\"movies_stats\", \"movie_statistics\", dfs[\"movie_statistics\"])\n",
    "add_new_df(\"movies\",       \"movies\",           dfs[\"movies\"])\n",
    "add_new_df(\"user_stats\",   \"user_statistics\",  dfs[\"user_statistics\"])\n",
    "add_new_df(\"viewer_ratings\", \"viewer_ratings\", dfs[\"viewer_ratings\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f1e8ddfc",
   "metadata": {},
   "source": [
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5d6109d206cea",
   "metadata": {},
   "source": "Iterates over every loaded DataFrame in dfs, printing the table name and computing the percentage of missing values per column. The loop uses df.isna().mean() * 100 to calculate column-wise null rates and displays them as a single-column DataFrame so missing values can quickly spot data quality gaps across all tables"
  },
  {
   "cell_type": "code",
   "id": "def25638f8eb473b",
   "metadata": {},
   "source": [
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name} missing values (%):\")\n",
    "    missing_pct = df.isna().mean() * 100\n",
    "    display(missing_pct.to_frame(\"missing_%\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Filters user_stats to the subset of users with std_rating still missing, tells how many such users exist, and then checks how many of their customer_id values are present in viewer_ratings table. Then prints the count of the total missing-std_rating users and how many of them appear in the ratings table, confirming whether the missing entries have corresponding viewer activity",
   "id": "ca1bb4066703bf66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "missing_after = user_stats[user_stats[\"std_rating\"].isna()].copy()\n",
    "\n",
    "print(\"All users with NaN std:\", len(missing_after))\n",
    "\n",
    "# checking if they are in viewer_ratings\n",
    "print(missing_after[\"customer_id\"].isin(viewer_ratings[\"customer_id\"]).sum())"
   ],
   "id": "3ce2621ae7448d16",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "95d469c6",
   "metadata": {},
   "source": [
    "### Movie Statistics Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78394717c371ac27",
   "metadata": {},
   "source": [
    "A function to calculate missing std. ratings of films"
   ]
  },
  {
   "cell_type": "code",
   "id": "1395d0b962d46cb5",
   "metadata": {},
   "source": [
    "# syncing\n",
    "sync_dataframe(movies_stats, viewer_ratings)\n",
    "\n",
    "# removing nan values\n",
    "viewer_ratings_clean = viewer_ratings[~viewer_ratings[\"rating\"].isna()]\n",
    "\n",
    "movie_std = (\n",
    "    viewer_ratings[~viewer_ratings[\"rating\"].isna()]\n",
    "    .groupby(\"movie_id\")[\"rating\"]\n",
    "    .apply(list)\n",
    "    .reset_index(name=\"ratings\")\n",
    ")\n",
    "\n",
    "# calculating std\n",
    "movie_std[\"std_rating\"] = movie_std[\"ratings\"].apply(manual_std)\n",
    "movie_std = movie_std[[\"movie_id\", \"std_rating\"]]\n",
    "\n",
    "# calulating old\n",
    "old_null_pct = dfs[\"movie_statistics\"][\"std_rating\"].isna().mean() * 100\n",
    "\n",
    "# merging\n",
    "movies_stats = movies_stats.merge(\n",
    "    movie_std,\n",
    "    on=\"movie_id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_new\")\n",
    ")\n",
    "\n",
    "# updating\n",
    "movies_stats[\"std_rating\"] = movies_stats[\"std_rating_new\"]\n",
    "movies_stats.drop(columns=[\"std_rating_new\"], inplace=True)\n",
    "\n",
    "# reasigning\n",
    "dfs[\"movie_statistics\"] = movies_stats\n",
    "\n",
    "# calculating new\n",
    "new_null_pct = movies_stats[\"std_rating\"].isna().mean() * 100\n",
    "\n",
    "# calculating improvements (diagnostics purposes only)\n",
    "improvement_abs = old_null_pct - new_null_pct\n",
    "improvement_rel = (improvement_abs / old_null_pct) * 100 if old_null_pct > 0 else 0\n",
    "\n",
    "print(f\"Missing values reduced from {old_null_pct:.2f}% to {new_null_pct:.2f}%.\")\n",
    "print(f\"Absolute improvement: {improvement_abs:.2f}%\")\n",
    "print(f\"Relative improvement: {improvement_rel:.2f}% better than before.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a5d4518d",
   "metadata": {},
   "source": [
    "Calculating the missing total_ratings of movies"
   ]
  },
  {
   "cell_type": "code",
   "id": "11b6c1bf",
   "metadata": {},
   "source": [
    "sync_dataframe(movies_stats)\n",
    "\n",
    "# Collecting all the movies with absent total_rating in a dictionary\n",
    "missing_dict = {}\n",
    "\n",
    "missing = search_by_parameter('movie_statistics', 'total_ratings', None)\n",
    "missing_dict = {row.movie_id: 0 for row in missing.itertuples(index=False)}\n",
    "\n",
    "# Iterating through viewer_ratings and manually counting the ratings for each film\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "    if movie_id in missing_dict:\n",
    "        missing_dict[movie_id] += 1\n",
    "\n",
    "# Update movie_stats\n",
    "for row in movies_stats.itertuples(index=True):\n",
    "    if row.movie_id in missing_dict:\n",
    "        movies_stats.at[row.Index, \"total_ratings\"] = missing_dict[row.movie_id]\n",
    "\n",
    "dfs[\"movie_statistics\"] = movies_stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "13f9afb8",
   "metadata": {},
   "source": [
    "Calculating Missing Averages"
   ]
  },
  {
   "cell_type": "code",
   "id": "63d4756a",
   "metadata": {},
   "source": [
    "sync_dataframe(movies_stats, viewer_ratings)\n",
    "\n",
    "# Finding movies with null avg_rating\n",
    "missing_avg = search_by_parameter('movie_statistics', 'avg_rating', None)\n",
    "\n",
    "# Creating a dict of type { movie_id : avg_rating }\n",
    "# set 0 as base value for now, might change it later\n",
    "missing_avg_dict = {row.movie_id: 0 for row in missing_avg.itertuples(index=False)}\n",
    "\n",
    "# Storing the sum of all ratings for each movie\n",
    "rating_sums = {movie_id: 0 for movie_id in missing_avg_dict}\n",
    "\n",
    "# Iterating through viewer ratings and adding to sum if movie_id matches\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if movie_id in missing_avg_dict:\n",
    "        rating_sums[movie_id] += rating\n",
    "\n",
    "for row in movies_stats.itertuples():\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    if movie_id in rating_sums:\n",
    "        total = row.total_ratings # I'm assuming that my calculations of total_ratings per movie is correct ang i got rid of                            all null values\n",
    "\n",
    "        # IF FORE SOME MAGICAL REASON THERE IS STILL A NULL THEN IGNORE\n",
    "        if pd.isna(total) or total == 0:\n",
    "            avg = 0\n",
    "        else:\n",
    "            avg = rating_sums[movie_id] / total\n",
    "\n",
    "        movies_stats.at[row.Index, \"avg_rating\"] = avg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21090df8",
   "metadata": {},
   "source": [
    "Calculating the missing min and max ratings for movies"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9bb36c2",
   "metadata": {},
   "source": [
    "sync_dataframe(movies_stats)\n",
    "\n",
    "# Find movies with missing min_rating and max_rating using your function\n",
    "missing_min = search_by_parameter('movie_statistics', 'min_rating', None)\n",
    "missing_max = search_by_parameter('movie_statistics', 'max_rating', None)\n",
    "\n",
    "# Combine them as some movies may be in both\n",
    "missing_ids = set(missing_min[\"movie_id\"]) | set(missing_max[\"movie_id\"])\n",
    "\n",
    "# Take only ratings for the movies we care\n",
    "relevant_ratings = viewer_ratings[viewer_ratings[\"movie_id\"].isin(missing_ids)]\n",
    "\n",
    "# Building a nested dict {movie_id : {\"min\": , \"max\": }}\n",
    "min_max_dict = {}\n",
    "\n",
    "for row in relevant_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if movie_id not in min_max_dict:\n",
    "        min_max_dict[movie_id] = {\"min\": rating, \"max\": rating}\n",
    "    else:\n",
    "        if rating < min_max_dict[movie_id][\"min\"]:\n",
    "            min_max_dict[movie_id][\"min\"] = rating\n",
    "        if rating > min_max_dict[movie_id][\"max\"]:\n",
    "            min_max_dict[movie_id][\"max\"] = rating\n",
    "\n",
    "# Update movie_statistics\n",
    "for row in movies_stats.itertuples(index=True):\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    if movie_id in min_max_dict:\n",
    "        if pd.isna(row.min_rating):\n",
    "            movies_stats.at[row.Index, \"min_rating\"] = min_max_dict[movie_id][\"min\"]\n",
    "        if pd.isna(row.max_rating):\n",
    "            movies_stats.at[row.Index, \"max_rating\"] = min_max_dict[movie_id][\"max\"]\n",
    "\n",
    "dfs[\"movie_statistics\"] = movies_stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a83870a25990a35e",
   "metadata": {},
   "source": [
    "Finding missing unique users"
   ]
  },
  {
   "cell_type": "code",
   "id": "65c9dd911b77bbd1",
   "metadata": {},
   "source": [
    "sync_dataframe(movies_stats)\n",
    "\n",
    "missing_unique = search_by_parameter('movie_statistics', 'unique_users', None)\n",
    "\n",
    "# creating my favorite movie set\n",
    "missing_movie_ids = {row.movie_id for row in missing_unique.itertuples(index=False)}\n",
    "\n",
    "# creating a dict movie_id: customer_id\n",
    "unique_users_dict = {movie_id: set() for movie_id in missing_movie_ids}\n",
    "\n",
    "# Gathering unique users\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    # as always im getting only those movies which have null for unique users\n",
    "    if movie_id in unique_users_dict:\n",
    "        unique_users_dict[movie_id].add(row.customer_id)\n",
    "\n",
    "# Counting unique users\n",
    "updated = 0\n",
    "for movie_id, users in unique_users_dict.items():\n",
    "    count = len(users)  # unique users count\n",
    "\n",
    "    movies_stats.loc[\n",
    "        movies_stats[\"movie_id\"] == movie_id,\n",
    "        \"unique_users\"\n",
    "    ] = count\n",
    "\n",
    "    updated += 1\n",
    "\n",
    "# just in case if a movie has 0 ratings im setting unique users to 0\n",
    "movies_stats[\"unique_users\"] = movies_stats[\"unique_users\"].fillna(0).astype(int)\n",
    "\n",
    "# Updating\n",
    "dfs[\"movie_statistics\"] = movies_stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e4807162c092ff91",
   "metadata": {},
   "source": [
    "### User_statistics calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0dc4f80d5c7f16",
   "metadata": {},
   "source": [
    "Calculating user's std rating"
   ]
  },
  {
   "cell_type": "code",
   "id": "9820aaa2583197",
   "metadata": {},
   "source": [
    "sync_dataframe(user_stats)\n",
    "\n",
    "# grouping by user id\n",
    "user_std = (\n",
    "    viewer_ratings[~viewer_ratings[\"rating\"].isna()]\n",
    "    .groupby(\"customer_id\")[\"rating\"]\n",
    "    .apply(list)\n",
    "    .reset_index(name=\"ratings\")\n",
    ")\n",
    "\n",
    "user_std[\"std_rating\"] = user_std[\"ratings\"].apply(manual_std)\n",
    "user_std = user_std[[\"customer_id\", \"std_rating\"]]\n",
    "\n",
    "# old percentage\n",
    "old_null_pct = user_stats[\"std_rating\"].isna().mean() * 100\n",
    "\n",
    "# merging\n",
    "user_stats = user_stats.merge(\n",
    "    user_std,\n",
    "    on=\"customer_id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_new\")\n",
    ")\n",
    "\n",
    "# replacing old std with new\n",
    "user_stats[\"std_rating\"] = user_stats[\"std_rating_new\"]\n",
    "user_stats.drop(columns=[\"std_rating_new\"], inplace=True)\n",
    "\n",
    "# updating\n",
    "dfs[\"user_statistics\"] = user_stats\n",
    "\n",
    "# calculating new percantages\n",
    "new_null_pct = user_stats[\"std_rating\"].isna().mean() * 100\n",
    "\n",
    "# calculating improvements\n",
    "improvement_abs = old_null_pct - new_null_pct\n",
    "improvement_rel = (improvement_abs / old_null_pct) * 100 if old_null_pct > 0 else 0\n",
    "\n",
    "print(f\"Missing values reduced from {old_null_pct:.2f}% to {new_null_pct:.2f}%.\")\n",
    "print(f\"Absolute improvement: {improvement_abs:.2f}%\")\n",
    "print(f\"Relative improvement: {improvement_rel:.2f}% better than before.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ed22d075b53743f9",
   "metadata": {},
   "source": [
    "Calculating user's total ratings"
   ]
  },
  {
   "cell_type": "code",
   "id": "3e6f6477f9abcc7b",
   "metadata": {},
   "source": [
    "sync_dataframe(user_stats)\n",
    "\n",
    "# Collecting all the users with absent total_ratings\n",
    "missing_dict = {}\n",
    "\n",
    "missing = search_by_parameter('user_statistics', 'total_ratings', None)\n",
    "missing_dict = {row.customer_id: 0 for row in missing.itertuples(index=False)}\n",
    "\n",
    "# Iterating through viewer_ratings and manually counting the ratings for each user\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    if pd.isna(row.rating):\n",
    "        continue\n",
    "\n",
    "    customer_id = row.customer_id\n",
    "    if customer_id in missing_dict:\n",
    "        missing_dict[customer_id] += 1\n",
    "\n",
    "# Update user_stats\n",
    "for row in user_stats.itertuples(index=True):\n",
    "    if row.customer_id in missing_dict:\n",
    "        user_stats.at[row.Index, \"total_ratings\"] = missing_dict[row.customer_id]\n",
    "\n",
    "dfs[\"user_statistics\"] = user_stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8b7f2348b331e259",
   "metadata": {},
   "source": [
    "Calculating user's avg rating"
   ]
  },
  {
   "cell_type": "code",
   "id": "4641d086358af2ec",
   "metadata": {},
   "source": [
    "sync_dataframe(user_stats)\n",
    "\n",
    "# Finding all users with null avg_rating\n",
    "missing_avg = search_by_parameter('user_statistics', 'avg_rating', None)\n",
    "\n",
    "# Creating a dict of type { customer_id : avg_rating }\n",
    "# set 0 as base value\n",
    "missing_avg_dict = {row.customer_id: 0 for row in missing_avg.itertuples(index=False)}\n",
    "\n",
    "# Storing the sum of all ratings for each movie\n",
    "rating_sums = {customer_id: 0 for customer_id in missing_avg_dict}\n",
    "\n",
    "# Iterating through viewer ratings and adding to sum if movie_id matches\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    if pd.isna(row.rating):\n",
    "        continue\n",
    "\n",
    "    customer_id = row.customer_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if customer_id in missing_avg_dict:\n",
    "        rating_sums[customer_id] += rating\n",
    "\n",
    "for row in user_stats.itertuples():\n",
    "    customer_id = row.customer_id\n",
    "\n",
    "    if customer_id in rating_sums:\n",
    "        total = row.total_ratings\n",
    "\n",
    "        if pd.isna(total) or total == 0:\n",
    "            avg = 0\n",
    "        else:\n",
    "            avg = rating_sums[customer_id] / total\n",
    "\n",
    "        user_stats.at[row.Index, \"avg_rating\"] = avg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34cdefa630a021fd",
   "metadata": {},
   "source": [
    "#activity days\n",
    "sync_dataframe(user_stats)\n",
    "\n",
    "missing = search_by_parameter('user_statistics', 'activity_days', None)\n",
    "\n",
    "# creating a dict\n",
    "missing_dict = {row.customer_id: 0 for row in missing.itertuples(index=False)}\n",
    "\n",
    "for row in user_stats.itertuples():\n",
    "    customer_id = row.customer_id\n",
    "\n",
    "    if customer_id not in missing_dict: # wroking only with users with missing activity_days\n",
    "        continue\n",
    "\n",
    "    first_date = covert_string_to_date(row.first_rating_date)\n",
    "    last_date = covert_string_to_date(row.last_rating_date)\n",
    "\n",
    "    # null exception\n",
    "    if first_date is None or last_date is None:\n",
    "        continue\n",
    "\n",
    "    # calcing the difference\n",
    "    difference = (last_date - last_date).days\n",
    "\n",
    "    # updating activity_days\n",
    "    user_stats.at[row.Index, 'activity_days'] = difference\n",
    "\n",
    "null_percentage = user_stats['activity_days'].isna().mean() * 100\n",
    "print(f\"Missing activity_days: {null_percentage}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fbb5f2e6dfe2b465",
   "metadata": {},
   "source": [
    "#minmax\n",
    "sync_dataframe(user_stats)\n",
    "\n",
    "# Find users with missing min_rating and max_rating using your function\n",
    "missing_min = search_by_parameter('user_statistics', 'min_rating', None)\n",
    "missing_max = search_by_parameter('user_statistics', 'max_rating', None)\n",
    "\n",
    "# Combine them as some users may be in both\n",
    "missing_ids = set(missing_min[\"customer_id\"]) | set(missing_max[\"customer_id\"])\n",
    "\n",
    "# Init dictionary for users we need to compute\n",
    "missing_users = {\n",
    "    customer_id: {\"min\": -1, \"max\": -1} #setting base to -1\n",
    "    for customer_id in missing_ids\n",
    "}\n",
    "\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    customer_id = row.customer_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if customer_id in missing_users:\n",
    "        # updating min\n",
    "        if missing_users[customer_id][\"min\"] == -1 or rating < missing_users[customer_id][\"min\"]:\n",
    "            missing_users[customer_id][\"min\"] = rating\n",
    "\n",
    "        # updating max\n",
    "        if missing_users[customer_id][\"max\"] == -1 or rating > missing_users[customer_id][\"max\"]:\n",
    "            missing_users[customer_id][\"max\"] = rating\n",
    "\n",
    "# Updating user stats\n",
    "for row in user_stats.itertuples():\n",
    "    customer_id = row.customer_id\n",
    "\n",
    "    if customer_id in missing_users:\n",
    "        min_val = missing_users[customer_id][\"min\"]\n",
    "        max_val = missing_users[customer_id][\"max\"]\n",
    "\n",
    "        # if the guy hasn't rated anything im making sure we set min or max to none so we get rid of them in the future\n",
    "        if min_val == -1 and max_val == -1:\n",
    "            min_val = None\n",
    "            max_val = None\n",
    "\n",
    "        user_stats.at[row.Index, \"min_rating\"] = min_val\n",
    "        user_stats.at[row.Index, \"max_rating\"] = max_val\n",
    "\n",
    "dfs[\"user_statistics\"] = user_stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "de42b238",
   "metadata": {},
   "source": [
    "sync_dataframe(user_stats)\n",
    "missing_unique = search_by_parameter('user_statistics', 'unique_movies', None)\n",
    "\n",
    "missing_ids = set(missing_unique[\"customer_id\"])\n",
    "\n",
    "unique_movies_dict = {customer_id: set() for customer_id in missing_ids}\n",
    "\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    user_id = row.customer_id\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    if user_id in unique_movies_dict:\n",
    "        unique_movies_dict[user_id].add(movie_id)\n",
    "\n",
    "for row in user_stats.itertuples():\n",
    "    user_id = row.customer_id\n",
    "\n",
    "    if user_id in unique_movies_dict:\n",
    "        count_unique = len(unique_movies_dict[user_id])\n",
    "\n",
    "        if count_unique == 0:\n",
    "            value = None\n",
    "        else:\n",
    "            value = count_unique\n",
    "\n",
    "        user_stats.at[row.Index, \"unique_movies\"] = value\n",
    "\n",
    "# updating\n",
    "dfs[\"user_statistics\"] = user_stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d12e52c5",
   "metadata": {},
   "source": [
    "### Merging datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c36b6",
   "metadata": {},
   "source": [
    "Merging the Movies and movie statis filling in missing values on either dataset and converting all of the dates to type DateTime as well as all counts and years to integers.\n",
    "This is in order to clean our movie data before merging it with our user data to fill in any recoverable missing values.\n",
    "Rows with missing values will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "id": "46319c89",
   "metadata": {},
   "source": [
    "movies = dfs['movies'].copy() # does this overwrite the movies we have right now? maybe call it something else.\n",
    "movie_stats = dfs['movie_statistics'].copy()\n",
    "viewer_ratings = dfs['viewer_ratings'].copy()\n",
    "\n",
    "bad_ids_movies = movies.loc[movies['movie_id'].duplicated(keep=False), 'movie_id'].unique().tolist()\n",
    "bad_ids_stats = movie_stats.loc[movie_stats['movie_id'].duplicated(keep=False), 'movie_id'].unique().tolist()\n",
    "bad_ids = list(set(bad_ids_movies + bad_ids_stats))\n",
    "print('bad movie ids to remove: ', bad_ids)\n",
    "\n",
    "viewer_ratings = viewer_ratings[~viewer_ratings['movie_id'].isin(bad_ids)]\n",
    "movies = movies[~movies['movie_id'].isin(bad_ids)]\n",
    "movie_stats = movie_stats[~movie_stats['movie_id'].isin(bad_ids)]\n",
    "\n",
    "movies['year_of_release'] = (pd.to_numeric(movies['year_of_release'], errors='coerce').astype('Int64'))\n",
    "movie_stats['total_ratings'] = (pd.to_numeric(movie_stats['total_ratings'], errors='coerce').astype('Int64'))\n",
    "movie_stats['unique_users'] = (pd.to_numeric(movie_stats['unique_users'], errors='coerce').astype('Int64'))\n",
    "movie_stats['year_of_release'] = (pd.to_numeric(movie_stats['year_of_release'], errors='coerce').astype('Int64'))\n",
    "\n",
    "movie_stats['first_rating_date'] = pd.to_datetime(movie_stats['first_rating_date'], errors='coerce')\n",
    "movie_stats['last_rating_date'] = pd.to_datetime(movie_stats['last_rating_date'], errors='coerce')\n",
    "\n",
    "# this will automatically remove rows with missing data\n",
    "movie_full = movies.merge(movie_stats, on='movie_id', how='inner', suffixes=('_movies', '_stats'))\n",
    "\n",
    "movie_full['title'] = movie_full['title_movies'].combine_first(movie_full['title_stats'])\n",
    "movie_full['year_of_release'] = movie_full['year_of_release_movies'].combine_first(movie_full['year_of_release_stats'])\n",
    "\n",
    "movie_full = movie_full.drop(columns=['title_movies', 'title_stats', 'year_of_release_movies', 'year_of_release_stats'])\n",
    "\n",
    "dfs['movies'] = movies\n",
    "dfs['movie_statistics'] = movie_stats\n",
    "dfs['viewer_ratings'] = viewer_ratings\n",
    "add_new_df('movie_full', 'movie_full', movie_full)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d1d3b2a1691c50e",
   "metadata": {},
   "source": [
    "print(\"movies dtypes:\")\n",
    "print(movies.dtypes)\n",
    "\n",
    "print(\"\\nmovie_stats dtypes:\")\n",
    "print(movie_stats.dtypes)\n",
    "\n",
    "print(\"\\nmovie_full dtypes:\")\n",
    "print(movie_full.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4bec12db",
   "metadata": {},
   "source": [
    "- Converts the date parameter in viewer_ratings to datetime.\n",
    "- Merges viewer_ratings, movies, movie_statistics and user_statistics into one dataset as merged_data."
   ]
  },
  {
   "cell_type": "code",
   "id": "bc40bfb82470b10a",
   "metadata": {},
   "source": [
    "movie_full = dfs['movie_full'].copy()\n",
    "user_stats = dfs['user_statistics'].copy()\n",
    "viewer_ratings = dfs['viewer_ratings'].copy()\n",
    "\n",
    "\n",
    "viewer_ratings['date'] = pd.to_datetime(viewer_ratings['date'], errors = 'coerce')\n",
    "user_stats['first_rating_date'] = pd.to_datetime(user_stats['first_rating_date'], errors = 'coerce')\n",
    "user_stats['last_rating_date'] = pd.to_datetime(user_stats['last_rating_date'], errors = 'coerce')\n",
    "\n",
    "user_stats['total_ratings'] = (pd.to_numeric(user_stats['total_ratings'], errors='coerce').astype('Int64'))\n",
    "user_stats['unique_movies'] = (pd.to_numeric(user_stats['unique_movies'], errors='coerce').astype('Int64'))\n",
    "user_stats['activity_days'] = (pd.to_numeric(user_stats['activity_days'], errors='coerce').astype('Int64'))\n",
    "\n",
    "\n",
    "# create user and movie specific columns\n",
    "movie_full = movie_full.rename(columns={\n",
    "    'total_ratings':     'movie_total_ratings',\n",
    "    'avg_rating':        'movie_avg_rating',\n",
    "    'std_rating':        'movie_std_rating',\n",
    "    'min_rating':        'movie_min_rating',\n",
    "    'max_rating':        'movie_max_rating',\n",
    "    'first_rating_date': 'movie_first_rating_date',\n",
    "    'last_rating_date':  'movie_last_rating_date'\n",
    "})\n",
    "\n",
    "\n",
    "user_stats = user_stats.rename(columns={\n",
    "    'total_ratings':     'user_total_ratings',\n",
    "    'avg_rating':        'user_avg_rating',\n",
    "    'std_rating':        'user_std_rating',\n",
    "    'min_rating':        'user_min_rating',\n",
    "    'max_rating':        'user_max_rating',\n",
    "    'first_rating_date': 'user_first_rating_date',\n",
    "    'last_rating_date':  'user_last_rating_date'\n",
    "})\n",
    "\n",
    "# drop anomalous date for user stats\n",
    "if 'anomalous_date' in viewer_ratings.columns:\n",
    "    viewer_ratings = viewer_ratings.drop(columns=['anomalous_date'])\n",
    "\n",
    "# merge user stats and viewer ratings\n",
    "viewer_ratings_with_stats = viewer_ratings.merge(user_stats, on = 'customer_id', how = 'inner')\n",
    "\n",
    "merged_data = viewer_ratings_with_stats.merge(movie_full, on = 'movie_id', how = 'inner')\n",
    "\n",
    "# this will drop all rows with any null\n",
    "merged_data = merged_data.dropna(how = 'any')\n",
    "\n",
    "add_new_df('merged_data', 'merged_data', merged_data)\n",
    "dfs['movie_full'] = movie_full\n",
    "dfs['user_statistics'] = user_stats\n",
    "dfs['viewer_ratings'] = viewer_ratings\n",
    "\n",
    "print(merged_data.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Features for clustering\n",
    "movie_features = [\n",
    "    \"movie_total_ratings\",\n",
    "    \"movie_avg_rating\",\n",
    "    \"movie_std_rating\",\n",
    "    \"movie_min_rating\",\n",
    "    \"movie_max_rating\",\n",
    "]\n",
    "\n",
    "user_features_full = [\n",
    "    'user_total_ratings',\n",
    "    'user_avg_rating',\n",
    "    'user_std_rating',\n",
    "    'user_min_rating',\n",
    "    'user_max_rating',\n",
    "    'unique_movies',\n",
    "    'activity_days'\n",
    "]\n",
    "\n",
    "user_features = [\n",
    "    'user_total_ratings',\n",
    "    'activity_days',\n",
    "]\n",
    "\n",
    "movies_for_clustering = (\n",
    "    merged_data\n",
    "    .groupby(\"movie_id\")[movie_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")"
   ],
   "id": "2a03e0bdd0ee078c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Preprocessing Diagnostics",
   "id": "8f739a9d121fa80d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "user_features_full = [\n",
    "    'user_total_ratings',\n",
    "    'user_avg_rating',\n",
    "    'user_std_rating',\n",
    "    'user_min_rating',\n",
    "    'user_max_rating',\n",
    "    'unique_movies',\n",
    "    'activity_days'\n",
    "]\n",
    "user_stats[user_features_full].describe().T[[\"std\"]]\n",
    "#user_stats[user_features].corr()"
   ],
   "id": "ab47f2869e6f8d95",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Data normal distribution test</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ],
   "id": "e651d902b3886eea"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Purpose:**<br>\n",
    "Registers a dataframe under a global variable name while keeping a main dfs dictionary as the source.\n",
    "\n",
    "**Inputs:**<br>\n",
    "Takes global_name (new global variable name), dfs_key (dictionary key in dfs), and optional dataframe (only used if dfs_key is not already present) as inputs.\n",
    "\n",
    "**Behaviour:**<br>\n",
    "If dfs_key already exists in dfs, it simply maps that existing dataframe to the requested global name and records the mapping in DF_MAP. Otherwise, it raises an error when no dataframe is provided, or it stores the new dataframe in both dfs and the global namespace, updating DF_MAP.\n",
    "\n",
    "**Output:**<br>\n",
    "Doesn't return anything, prints whether an existing or new dataframe was mapped."
   ],
   "id": "37e46e8e68ea1645"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def plot_data_distribution(data):\n",
    "    stats.probplot(data, dist=\"norm\", plot=plt)\n",
    "    return plt.show()"
   ],
   "id": "897021858fc502bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for ud in user_features_full:\n",
    "    print(ud)\n",
    "    plot_data_distribution(merged_data[ud])\n",
    "\n",
    "for md in movie_features:\n",
    "    print(md)\n",
    "    plot_data_distribution(merged_data[md])"
   ],
   "id": "6d787e9c36e16b56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Testing DBSCAN Viabiliy</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ],
   "id": "203fd4852ee1ea2e"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Kernel Density Estimation Test\n",
    "\n",
    "**Purpose:**<br>\n",
    "\n",
    "**Inputs:**<br>\n",
    "\n",
    "**Behaviour:**<br>\n",
    "\n",
    "**Output:**<br>"
   ],
   "id": "8b8dca058a895bb5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "X = movies_for_clustering[movie_features].astype(float).dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(\n",
    "    x=pca_data[:, 0],\n",
    "    y=pca_data[:, 1],\n",
    "    fill=True,\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.7,\n",
    "    thresh=0.05,\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    pca_data[:, 0],\n",
    "    pca_data[:, 1],\n",
    "    s=8,\n",
    "    color=\"white\",\n",
    "    alpha=0.3,\n",
    "    edgecolor=None\n",
    ")\n",
    "\n",
    "plt.title(\"KDE + Scatter (PCA projection)\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.show()"
   ],
   "id": "ad867bcbfefe44cc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### KNN stats\n",
    "**Purpose:**<br>\n",
    "\n",
    "**Inputs:**<br>\n",
    "\n",
    "**Behaviour:**<br>\n",
    "\n",
    "**Output:**<br>"
   ],
   "id": "8b9aacd546b8933a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "nn = NearestNeighbors(n_neighbors=10)\n",
    "nn.fit(X_scaled)\n",
    "\n",
    "distances, _ = nn.kneighbors(X_scaled)\n",
    "mean_d = distances.mean(axis=1)\n",
    "\n",
    "print(\"Std of mean distances:\", np.std(mean_d))\n",
    "print(\"Mean of mean distances:\", np.mean(mean_d))"
   ],
   "id": "25ebce1e917fae06",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "users_for_clustering = (\n",
    "    merged_data\n",
    "    .groupby(\"customer_id\")[user_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "Y = users_for_clustering[user_features].astype(float).dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Y_scaled = scaler.fit_transform(Y)\n",
    "\n",
    "nn = NearestNeighbors(n_neighbors=10)\n",
    "nn.fit(Y_scaled)\n",
    "\n",
    "distances, _ = nn.kneighbors(Y_scaled)\n",
    "mean_d = distances.mean(axis=1)\n",
    "\n",
    "print(\"Std of mean distances:\", np.std(mean_d))\n",
    "print(\"Mean of mean distances:\", np.mean(mean_d))"
   ],
   "id": "bb64d383cedd3c4c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "#### Distance distribution test\n",
    "**Purpose:**<br>\n",
    "\n",
    "**Inputs:**<br>\n",
    "\n",
    "**Behaviour:**<br>\n",
    "\n",
    "**Output:**<br>"
   ],
   "id": "9fbad74ca637bf86"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "dists = pairwise_distances(X_scaled)\n",
    "flat = dists.flatten()\n",
    "\n",
    "plt.hist(flat, bins=100)\n",
    "plt.title(\"Distribution of Pairwise Distances\")\n",
    "plt.show()"
   ],
   "id": "ae18af5e23815649",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def hopkins(X, sampling=0.1, random_state=42):\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    n_samples = X.shape[0]\n",
    "    d = X.shape[1]\n",
    "    m = int(np.ceil(sampling * n_samples))\n",
    "\n",
    "    idx = np.random.choice(n_samples, m, replace=False)\n",
    "    X_sample = X[idx]\n",
    "\n",
    "    X_min = X.min(axis=0)\n",
    "    X_max = X.max(axis=0)\n",
    "    X_uniform = np.random.uniform(X_min, X_max, size=(m, d))\n",
    "\n",
    "    nn = NearestNeighbors(n_neighbors=1).fit(X)\n",
    "\n",
    "    u_dist, _ = nn.kneighbors(X_uniform)\n",
    "    w_dist, _ = nn.kneighbors(X_sample)\n",
    "\n",
    "    U = u_dist.sum()\n",
    "    W = w_dist.sum()\n",
    "\n",
    "    return U / (U + W)\n",
    "\n",
    "H = hopkins(X_scaled, sampling=0.1)\n",
    "print(\"Hopkins statistic:\", H)"
   ],
   "id": "8d7f5bb87c68a832",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "explained = pca.explained_variance_ratio_\n",
    "cum_explained = np.cumsum(explained)\n",
    "\n",
    "print(\"Explained variance ratio:\", np.round(explained, 3))\n",
    "print(\"Cumulative:\", np.round(cum_explained, 3))\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(range(1, len(explained) + 1), cum_explained, marker=\"o\")\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"PCA – cumulative explained variance\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "a7ca170b6abb3414",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Plotting Statistics and Overall findings",
   "id": "cc227b9c0a65cecd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Ryder",
   "id": "33b070947774f219"
  },
  {
   "cell_type": "code",
   "id": "2df1e09f",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Markdown cell above\n",
    "\"\"\"\n",
    "### Global Rating Distribution\n",
    "\n",
    "There is a clear over positive bias. This can be due to the fact that this was done on a streaming platform where users are more likely to rate higher\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(11, 6)) # made it this size for a better fit in the read me\n",
    "ax = sns.histplot(\n",
    "    data=dfs['viewer_ratings'],\n",
    "    x='rating',\n",
    "    bins=5,\n",
    "    discrete=True, #tells seaborn to treat x as integer values\n",
    "    color=\"#0062ff\",\n",
    "    edgecolor='white',\n",
    "    alpha=0.85,  #transparency\n",
    "    linewidth=1.5\n",
    ")\n",
    "\n",
    "# Add exact counts on top of each bar\n",
    "for rect in ax.patches:\n",
    "    height = rect.get_height()\n",
    "    ax.text(\n",
    "        rect.get_x() + rect.get_width()/2., \n",
    "        height + 200_000,                    # a bit above the bar\n",
    "        f'{int(height):,}',                  # adds commas: 12,345,678\n",
    "        ha='center', va='bottom', fontsize=12, fontweight='bold'\n",
    "    )\n",
    "\n",
    "# Clean y-axis\n",
    "plt.ylabel('Number of Ratings (millions)', fontsize=12)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))\n",
    "\n",
    "plt.title('Distribution of Viewer Ratings\\n(Strong Positive Bias)', \n",
    "          fontsize=15, pad=20)\n",
    "plt.xlabel('Rating (1–5)', fontsize=12)\n",
    "plt.xticks(range(0, 7))\n",
    "plt.ylim(0, 2_000_000)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2c540e1a",
   "metadata": {},
   "source": [
    "# \"\"\"\n",
    "# ### User Average Rating vs. Rating Variability\n",
    "\n",
    "# Scatter from user_statistics: High avg with low std = consistent positive raters. \n",
    "# Valuable for grouping users (e.g., strict critics vs. easy fans). Alpha for density.\n",
    "# \"\"\"\n",
    "\n",
    "# sync_dataframe(user_stats)\n",
    "\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.scatterplot(data=user_stats, x='avg_rating', y='std_rating', alpha=0.4, color='#0062ff', edgecolor=None)\n",
    "# plt.title('User Average Rating vs. Standard Deviation\\n(Rating Consistency Patterns)', fontsize=15, pad=20)\n",
    "# plt.xlabel('Average Rating Given (1–5)', fontsize=12)\n",
    "# plt.ylabel('Std Deviation of Ratings', fontsize=12)\n",
    "# plt.xlim(0, 5)\n",
    "# plt.ylim(0, 4)  # Std typically low due to bias\n",
    "# plt.grid(True, linestyle='--', alpha=0.7)\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a292f8066fef5d2",
   "metadata": {},
   "source": [
    "# # PERFECT PENTAGON — FINAL WORKING VERSION (Dec 2025)\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.path import Path\n",
    "# from matplotlib.patches import PathPatch\n",
    "\n",
    "# # CHANGE ONLY THESE TWO VALUES\n",
    "# avg_rating_value = user_stats['avg_rating'].mean()   # ← your real global avg from user_statistics\n",
    "# std_rating_value = user_stats['std_rating'].mean()   # ← your real global std from user_statistics\n",
    "\n",
    "# # Convert to 0–1 scale\n",
    "# pull_to_five = (avg_rating_value - 1.0) / 4.0\n",
    "# consistency  = np.clip(1.0 - (std_rating_value / 1.8), 0.1, 1.0)\n",
    "\n",
    "# # Pentagon vertices (5 corners + first repeated to close)\n",
    "# center = np.array([0.5, 0.5])\n",
    "# radius = 0.45\n",
    "# angles = np.linspace(0, 2*np.pi, 5, endpoint=False)\n",
    "# verts = center + radius * np.column_stack([np.cos(angles), np.sin(angles)])\n",
    "# verts = np.vstack([verts, verts[0]])  # close the loop → shape (6, 2)\n",
    "\n",
    "# # Blue pentagon (pulled toward Rating 5 = bottom-right)\n",
    "# blue_scale = 0.85 * pull_to_five\n",
    "# blue_verts = center + blue_scale * (verts - center)\n",
    "\n",
    "# # Red pentagon (size = consistency)\n",
    "# red_scale = 0.92 * consistency\n",
    "# red_verts = center + red_scale * (verts - center)\n",
    "\n",
    "# # Correct Path codes: 6 vertices → 6 codes\n",
    "# codes = [Path.MOVETO] + [Path.LINETO]*4 + [Path.CLOSEPOLY]\n",
    "\n",
    "# # Plot\n",
    "# fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# # Outer frame\n",
    "# frame_path = Path(verts, codes)\n",
    "# ax.add_patch(PathPatch(frame_path, facecolor='none', edgecolor='black', linewidth=3))\n",
    "\n",
    "# # Red pentagon (consistency)\n",
    "# red_path = Path(red_verts, codes)\n",
    "# ax.add_patch(PathPatch(red_path, facecolor=\"#000000\", alpha=0.7, edgecolor='#aa0000', linewidth=4))\n",
    "\n",
    "# # Blue pentagon (average rating)\n",
    "# blue_path = Path(blue_verts, codes)\n",
    "# ax.add_patch(PathPatch(blue_path, facecolor=\"#00ffdd\", alpha=0.75, edgecolor='#003380', linewidth=4))\n",
    "\n",
    "# # Corner labels\n",
    "# labels = ['1', '2', '3', '4', '5']\n",
    "# for (x, y), label in zip(verts[:-1], labels):\n",
    "#     ax.text(x, y, label, fontsize=16, fontweight='bold', ha='center', va='center',\n",
    "#             bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"white\", alpha=0.95))\n",
    "\n",
    "# ax.set_xlim(0, 1)\n",
    "# ax.set_ylim(0, 1)\n",
    "# ax.set_aspect('equal')\n",
    "# ax.axis('off')\n",
    "\n",
    "# ax.set_title(f\"Global User Rating Personality Pentagon\\n\"\n",
    "#              f\"Avg Rating = {avg_rating_value:.2f} | Std Dev = {std_rating_value:.2f}\",\n",
    "#              fontsize=20, pad=40, fontweight='bold')\n",
    "\n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d15d29082834d37d",
   "metadata": {},
   "source": [
    "'Plotting a timeline of ratings over time'\n",
    "\n",
    "sync_dataframe(movies_stats)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(data=movies_stats, x='year_of_release', y='avg_rating', alpha=0.4, color='#0062ff', edgecolor=None, s=35)\n",
    "plt.title('Movie Rating VS. Year of Release\\n(Trends Over Time)', fontsize=15, pad=20)\n",
    "plt.xlabel('Year of Release', fontsize=12)\n",
    "plt.ylabel('Average Rating Given (1–5)', fontsize=12)\n",
    "plt.xlim(1890, 2010) #this included the data for all the movies in the data set as it goes from 1896 to 2005\n",
    "plt.ylim(0, 7)  \n",
    "#plt.scatter(x,y, s)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "925d56e8f5632188",
   "metadata": {},
   "source": [
    "def get_top_rated_movies(min_ratings=100, top_n=10):\n",
    "    \"\"\"\n",
    "    Returns the top N best-rated movies with at least min_ratings.\n",
    "    Merges movie_statistics and movies tables to get titles and years.\n",
    "    \"\"\"\n",
    "    # Get movie stats and filter\n",
    "    sync_dataframe(movies_stats)\n",
    "    filtered = movies_stats[movies_stats['total_ratings'] >= min_ratings]\n",
    "    \n",
    "    # Sort by avg_rating descending\n",
    "    top_movies = filtered.sort_values('avg_rating', ascending=False).head(top_n)\n",
    "    \n",
    "    # Select and reorder columns nicely\n",
    "    result = top_movies[['movie_id', 'avg_rating', 'total_ratings']].copy()\n",
    "    \n",
    "    result = result.reset_index(drop=True)\n",
    "    result.index = result.index + 1  # Start ranking at 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "# === USE IT LIKE THIS ===\n",
    "print(\"TOP 10 BEST-RATED MOVIES (min 100 ratings):\")\n",
    "display(get_top_rated_movies(min_ratings=200, top_n=10))\n",
    "\n",
    "# sync_dataframe(movies_stats)\n",
    "\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# sns.scatterplot(data=get_top_rated_movies(min_ratings=100, top_n=10), x='movie_id', y='avg_rating', alpha=0.4, color='#0062ff', edgecolor=None, s=35)\n",
    "# plt.title('top 10 best-rated movies (min 100 ratings)', fontsize=15, pad=20)\n",
    "# plt.xlabel('Movies', fontsize=12)\n",
    "# plt.ylabel('Average Rating Given (1–5)', fontsize=12)\n",
    "# plt.xlim(str(search_by_parameter(\"movies\", \"movie_id\", get_top_rated_movies(min_ratings=100, top_n=10)['movie_id'].min())[\"title\"]), str(search_by_parameter(\"movies\", \"movie_id\", get_top_rated_movies(min_ratings=100, top_n=10)['movie_id'].max())[\"title\"])) #this included the data for all the movies in the data set as it goes from 1896 to 2005\n",
    "# plt.ylim(0, 5)  \n",
    "# #plt.scatter(x,y, s)\n",
    "# plt.grid(True, linestyle='--', alpha=0.7)\n",
    "# plt.show()\n",
    "\n",
    "print(str(search_by_parameter(\"movies\", \"movie_id\", get_top_rated_movies(min_ratings=200, top_n=10)['movie_id'].min())[\"title\"]))\n",
    "\n",
    "# ploting = []\n",
    "# for ploting.len() in range(10):\n",
    "#     ploting.append([search_by_parameter(\"movies\", \"movie_id\", get_top_rated_movies(min_ratings=100, top_n=10)['movie_id'])])\n",
    "\n",
    "# print(\"here\"+ploting(1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a8765e1370223402",
   "metadata": {},
   "source": [
    "def get_top_rated_movies(min_ratings=200, top_n=10):\n",
    "    sync_dataframe(movies_stats)\n",
    "    \n",
    "    # Filter and sort — pure movie_statistics\n",
    "    filtered = movies_stats[movies_stats['total_ratings'] >= min_ratings]\n",
    "    top_movies = filtered.sort_values('avg_rating', ascending=False).head(top_n).copy()\n",
    "    \n",
    "    # Reset index to get ranking 1 to 10\n",
    "    top_movies = top_movies.reset_index(drop=True)\n",
    "    top_movies.index = top_movies.index + 1\n",
    "    \n",
    "    # Use your existing search function to get titles\n",
    "    titles = []\n",
    "    for movie_id in top_movies['movie_id']:\n",
    "        result = search_by_parameter(\"movies\", \"movie_id\", movie_id)\n",
    "        title = result['title'].iloc[0] if not result.empty else f\"Unknown ({movie_id})\"\n",
    "        year = int(result['year_of_release'].iloc[0]) if not result.empty and pd.notna(result['year_of_release'].iloc[0]) else \"\"\n",
    "        titles.append(f\"{title} ({year})\" if year else title)\n",
    "    \n",
    "    top_movies['title_display'] = titles\n",
    "    return top_movies\n",
    "\n",
    "# === GET DATA ===\n",
    "top10 = get_top_rated_movies(min_ratings=1000, top_n=10)\n",
    "\n",
    "# === PLOT — TITLES ON X-AXIS (FIXED & WORKING) ===\n",
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "# Scatter plot with titles\n",
    "sns.scatterplot(\n",
    "    data=top10,\n",
    "    x='title_display',\n",
    "    y='avg_rating',\n",
    "    s=150,\n",
    "    color='#0062ff',\n",
    "    edgecolor='navy',\n",
    "    alpha=0.95,\n",
    "    linewidth=3\n",
    ")\n",
    "\n",
    "# Add text above each point\n",
    "for i, row in top10.iterrows():\n",
    "    plt.text(\n",
    "        x=i-1,  # categorical position\n",
    "        y=row['avg_rating'] + 0.015,\n",
    "        s=f\"{row['avg_rating']:.3f}\\n({row['total_ratings']:,} ratings)\",\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=10,\n",
    "        fontweight='bold',\n",
    "        color='green'\n",
    "    )\n",
    "\n",
    "plt.title('Top 10 Highest-Rated Movies (min 1000 ratings)', \n",
    "          fontsize=20, pad=40, fontweight='bold')\n",
    "plt.xlabel('Movie Title', fontsize=14)\n",
    "plt.ylabel('Average Rating (1–5)', fontsize=14)\n",
    "plt.ylim(4.0, 5.0)\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=50, ha='right', fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display table\n",
    "print(\"Top 10 Movies (no merge):\")\n",
    "display(top10[['title_display', 'avg_rating', 'total_ratings', 'movie_id']])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0548ebf42abcac1",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(14, 9))\n",
    "\n",
    "top10_Reversed = top10.sort_values('avg_rating', ascending=True)\n",
    "\n",
    "bars = plt.barh(\n",
    "    y=top10_Reversed['title_display'],\n",
    "    width=top10_Reversed['avg_rating'],\n",
    "    color='#0062ff',\n",
    "    edgecolor='navy',\n",
    "    height=0.7,\n",
    "    alpha=0.95\n",
    ")\n",
    "\n",
    "# Add the exact rating + number of ratings on each bar\n",
    "for bar, row in zip(bars, top10_Reversed.itertuples()):\n",
    "    plt.text(\n",
    "        x=bar.get_width() + 0.005,                    # slightly right of bar\n",
    "        y=bar.get_y() + bar.get_height()/2,           # center vertically\n",
    "        s=f\"{row.avg_rating:.3f}  ({row.total_ratings:,} ratings)\",\n",
    "        va='center',\n",
    "        ha='left',\n",
    "        fontsize=11,\n",
    "        fontweight='bold',\n",
    "        color='darkblue'\n",
    "    )\n",
    "\n",
    "# Styling — same as your scatterplot\n",
    "plt.title('Top 10 Highest-Rated Movies (min 1000 ratings)', \n",
    "          fontsize=20, pad=40, fontweight='bold')\n",
    "plt.xlabel('Average Rating (1–5)', fontsize=14)\n",
    "plt.ylabel('Movie Title', fontsize=14)\n",
    "plt.xlim(4.0, 5.0)\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Clean y-axis labels (movie titles)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b91a5400",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e3da7",
   "metadata": {},
   "source": [
    "K-means clustering algorithm for movies"
   ]
  },
  {
   "cell_type": "code",
   "id": "2a82b6c4",
   "metadata": {},
   "source": [
    "sync_dataframe(merged_data)\n",
    "merged = dfs[\"merged_data\"]\n",
    "\n",
    "# making a table 1 row - 1 movie\n",
    "movies_for_clustering = (\n",
    "    merged\n",
    "    .groupby(\"movie_id\")[movie_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# getting all number variables except for movie_id\n",
    "X = movies_for_clustering[movie_features].astype(float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# number of clusters\n",
    "n_clusters = 4\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    random_state=42,\n",
    "    n_init=10\n",
    ")\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# adding cluster ids to the table\n",
    "movies_for_clustering[\"cluster_id\"] = cluster_labels\n",
    "\n",
    "# saving\n",
    "add_new_df('movie_clusters', 'movie_clusters', movies_for_clustering)\n",
    "\n",
    "elbow_df = elbow_method(X_scaled, 1, 20)\n",
    "print(elbow_df)\n",
    "\n",
    "# printing how many movies in each cluster, only for debugging\n",
    "cluster_movies_amount_str = movies_for_clustering[\"cluster_id\"].value_counts().sort_index()\n",
    "print(f\"---movies in each cluster--- \\n{cluster_movies_amount_str}\")\n",
    "# Silhouette score\n",
    "print(compute_silhouette_score(X_scaled, cluster_labels))\n",
    "\n",
    "inspect_movie_cluster(movies_for_clustering, merged, 0)\n",
    "plot_clusters_pca_2d(X_scaled, cluster_labels, max_points_per_cluster=800)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4537ce683798714d",
   "metadata": {},
   "source": [
    "K-means clustering algorithm for movies without the outliers (cluster 4)"
   ]
  },
  {
   "cell_type": "code",
   "id": "d32340e9be7d2ae7",
   "metadata": {},
   "source": [
    "# --- KMeans *without* movies from old cluster 3 ---\n",
    "\n",
    "# 1. Filter out movies that were in cluster 3 in the *original* clustering\n",
    "mask_no3 = movies_for_clustering[\"cluster_id\"] != 2\n",
    "movies_no3 = movies_for_clustering[mask_no3].copy()\n",
    "\n",
    "# 2. Take the corresponding scaled features (so we keep the same scaler as before)\n",
    "X_scaled_no3 = X_scaled[mask_no3.values]\n",
    "\n",
    "# 3. Re-run KMeans on the reduced dataset\n",
    "n_clusters_new = 3  # keep 4 to see how cluster structure changes after removing old cluster 3\n",
    "\n",
    "kmeans_no3 = KMeans(\n",
    "    n_clusters=n_clusters_new,\n",
    "    random_state=42,\n",
    "    n_init=10\n",
    ")\n",
    "\n",
    "cluster_labels_no3 = kmeans_no3.fit_predict(X_scaled_no3)\n",
    "\n",
    "# 4. Attach the *new* cluster ids\n",
    "# (original movies_for_clustering is untouched, we work only on movies_no3)\n",
    "movies_no3[\"cluster_id\"] = cluster_labels_no3\n",
    "\n",
    "# 5. Save as a separate df in dfs\n",
    "add_new_df(\"movie_clusters_no3\", \"movie_clusters_no3\", movies_no3)\n",
    "\n",
    "# 6. Elbow on the reduced dataset (optional, to see if k changed)\n",
    "elbow_df_no3 = elbow_method(X_scaled_no3, 1, 20)\n",
    "print(\"--- Elbow after removing old cluster 3 ---\")\n",
    "print(elbow_df_no3)\n",
    "\n",
    "# 7. Cluster sizes for the new run\n",
    "cluster_movies_amount_str_no3 = movies_no3[\"cluster_id\"].value_counts().sort_index()\n",
    "print(\"--- movies in each NEW cluster (old cluster 3 removed) ---\")\n",
    "print(cluster_movies_amount_str_no3)\n",
    "\n",
    "# 8. Silhouette score for the new clustering\n",
    "print(\"--- Silhouette score (no old cluster 3) ---\")\n",
    "print(compute_silhouette_score(X_scaled_no3, cluster_labels_no3))\n",
    "\n",
    "# 9. Quick inspection of one of the new clusters (e.g., cluster 0)\n",
    "inspect_movie_cluster(movies_no3, merged, 0)\n",
    "\n",
    "# 10. PCA 2D visualization for the new clustering\n",
    "plot_clusters_pca_2d(X_scaled_no3, cluster_labels_no3, max_points_per_cluster=800)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ffd7a4fd5c8f4bc0",
   "metadata": {},
   "source": [
    "Agglomerative clustering algorithm for movies"
   ]
  },
  {
   "cell_type": "code",
   "id": "14b898e91d33f897",
   "metadata": {},
   "source": [
    "sync_dataframe(merged_data)\n",
    "merged = dfs[\"merged_data\"]\n",
    "\n",
    "# One row per movie\n",
    "movies_for_h_clustering = (\n",
    "    merged\n",
    "    .groupby(\"movie_id\")[movie_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(movies_for_h_clustering.head())\n",
    "\n",
    "# Matrix of fs\n",
    "X = movies_for_h_clustering[movie_features].astype(float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "Z = linkage(X_scaled, method=\"ward\")\n",
    "\n",
    "# Building a dendogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode=\"lastp\",\n",
    "    p=50,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=8.,\n",
    "    show_contracted=True\n",
    ")\n",
    "plt.title(\"Movie clustering dendrogram (truncated)\")\n",
    "plt.xlabel(\"Cluster index or movie groups\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "n_clusters = 4  # number of clusters\n",
    "\n",
    "h_cluster_labels = fcluster(Z, t=n_clusters, criterion=\"maxclust\")\n",
    "\n",
    "movies_for_h_clustering[\"cluster_id\"] = h_cluster_labels - 1\n",
    "\n",
    "# printing how many movies in each cluster, only for debugging\n",
    "h_cluster_movies_amount_str = movies_for_h_clustering[\"cluster_id\"].value_counts().sort_index()\n",
    "print(f\"---movies in each cluster--- \\n{h_cluster_movies_amount_str}\")\n",
    "# Silhouette score\n",
    "hier_labels = movies_for_h_clustering[\"cluster_id\"].to_numpy()\n",
    "print(compute_silhouette_score(X_scaled, hier_labels))\n",
    "\n",
    "# saving\n",
    "add_new_df(\"movie_clusters_h\", \"movie_clusters_h\", movies_for_h_clustering)\n",
    "inspect_movie_cluster(movie_clusters_h, merged, 0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "76fe9959c9b4a8a1",
   "metadata": {},
   "source": [
    "K-means on users"
   ]
  },
  {
   "cell_type": "code",
   "id": "3af32cd51209096c",
   "metadata": {},
   "source": [
    "sync_dataframe(merged_data)\n",
    "data = merged_data.copy()\n",
    "\n",
    "# Taking first row per user\n",
    "users_for_clustering = (\n",
    "    data.groupby(\"customer_id\")[user_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "data = users_for_clustering[user_features].astype(float)\n",
    "data = data[user_features].dropna()\n",
    "\n",
    "scalar = StandardScaler()\n",
    "scaled_data = scalar.fit_transform(data)\n",
    "\n",
    "print(elbow_method(data))\n",
    "\n",
    "# k-means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto')\n",
    "cluster_labels = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "users_for_clustering[\"cluster_id\"] = cluster_labels\n",
    "print(users_for_clustering[\"cluster_id\"].value_counts().sort_index())\n",
    "print(compute_silhouette_score(scaled_data, cluster_labels, 20000))\n",
    "\n",
    "plot_clusters_pca_2d(scaled_data, cluster_labels)\n",
    "\n",
    "add_new_df(\"user_clusters\", \"user_clusters\", users_for_clustering)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "125eb1d544b412c5",
   "metadata": {},
   "source": [
    "Markdown"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "eps = 0.7\n",
    "min_samples = 15\n",
    "\n",
    "dbscan = DBSCAN(\n",
    "    eps=eps,\n",
    "    min_samples=min_samples,\n",
    "    metric=\"euclidean\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "movies_for_clustering_db = (\n",
    "    merged\n",
    "    .groupby(\"movie_id\")[movie_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "movies_for_clustering_db[\"dbscan_cluster_id\"] = labels\n",
    "\n",
    "cluster_counts = (\n",
    "    movies_for_clustering_db[\"dbscan_cluster_id\"]\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "print(cluster_counts)\n",
    "\n",
    "unique_labels = set(labels)\n",
    "n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "noise = (labels == -1).sum()\n",
    "\n",
    "print(f\"Clusters: {n_clusters}\")\n",
    "print(f\"Noise: {noise}\")\n",
    "\n",
    "mask = labels != -1\n",
    "if n_clusters > 1 and mask.sum() > 0:\n",
    "    sil = compute_silhouette_score(X_scaled[mask], labels[mask])\n",
    "    print(\"Silhouette:\", sil)"
   ],
   "id": "7beef9d5c5a125ee",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "eps_list = [.3, .4, 0.50, 0.60, 0.70, 0.75, 0.85, 1.0]\n",
    "\n",
    "def test_eps_values(X_scaled, eps_list, min_samples=15):\n",
    "    print(\"eps  | clusters | noise | silhouette\")\n",
    "    print(\"-------------------------------------\")\n",
    "\n",
    "    for eps in eps_list:\n",
    "        model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = model.fit_predict(X_scaled)\n",
    "\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        noise = (labels == -1).sum()\n",
    "\n",
    "        if n_clusters > 1:\n",
    "            mask = labels != -1\n",
    "            sil = compute_silhouette_score(X_scaled[mask], labels[mask])\n",
    "            sil = round(sil, 3)\n",
    "        else:\n",
    "            sil = None\n",
    "\n",
    "        print(f\"{eps:<4} | {n_clusters:<8} | {noise:<5} | {sil}\")\n",
    "\n",
    "test_eps_values(X_scaled, eps_list)"
   ],
   "id": "eab44a3ab5106803",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sync_dataframe(merged_data)\n",
    "\n",
    "movies_for_gmm = (\n",
    "    merged_data\n",
    "    .groupby(\"movie_id\")[movie_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X = movies_for_gmm[movie_features].astype(float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "k_values = range(1, 11)\n",
    "bic_values = []\n",
    "aic_values = []\n",
    "\n",
    "for k in k_values:\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=k,\n",
    "        covariance_type=\"full\",\n",
    "        random_state=42,\n",
    "        n_init=5\n",
    "    )\n",
    "    gmm.fit(X_scaled)\n",
    "\n",
    "    bic_values.append(gmm.bic(X_scaled))\n",
    "    aic_values.append(gmm.aic(X_scaled))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_values, bic_values, marker=\"o\", label=\"BIC\")\n",
    "plt.plot(k_values, aic_values, marker=\"o\", label=\"AIC\")\n",
    "plt.title(\"BIC/AIC for different n_components\")\n",
    "plt.xlabel(\"Number of (k)\")\n",
    "plt.ylabel(\"BIC/AIC\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "c206b546a2f90533",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "sync_dataframe(merged_data)\n",
    "\n",
    "movies_for_clustering_gmm = (\n",
    "    merged_data\n",
    "    .groupby(\"movie_id\")[movie_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X = movies_for_clustering_gmm[movie_features].astype(float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "gmm = GaussianMixture(\n",
    "    n_components=4,\n",
    "    covariance_type=\"full\",\n",
    "    random_state=42,\n",
    "    n_init=10\n",
    ")\n",
    "\n",
    "gmm_labels = gmm.fit_predict(X_scaled)\n",
    "\n",
    "movies_for_clustering_gmm[\"gmm_cluster_id\"] = gmm_labels\n",
    "\n",
    "add_new_df(\"movie_clusters_gmm\",\"movie_clusters_gmm\",movies_for_clustering_gmm)\n",
    "\n",
    "gmm_s_score = compute_silhouette_score(\n",
    "    X_scaled,\n",
    "    gmm_labels,\n",
    ")\n",
    "\n",
    "print(f\"GMM silhouette score: {gmm_s_score:.4f}\")\n",
    "plot_clusters_pca_2d(X_scaled, gmm_labels)"
   ],
   "id": "aae8534030705f9a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
