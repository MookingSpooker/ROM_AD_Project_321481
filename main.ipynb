{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546ff8e7",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0bd42",
   "metadata": {},
   "source": [
    "### Project initialization and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be51f045",
   "metadata": {},
   "source": [
    "Importing all of the libraries that will be used. In the project."
   ]
  },
  {
   "cell_type": "code",
   "id": "f3a1060f",
   "metadata": {},
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3bb93472f940c938",
   "metadata": {},
   "source": [
    "Connecting the DB to SQL"
   ]
  },
  {
   "cell_type": "code",
   "id": "58e0738c73228680",
   "metadata": {},
   "source": [
    "DB_PATH = \"viewer_interactions.db\"\n",
    "\n",
    "try:\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    print(\"Connected successfully!\")\n",
    "except sqlite3.Error as e:\n",
    "    print(\"Connection failed:\", e)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dcb72a1f7b4374a3",
   "metadata": {},
   "source": [
    "# Display options\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 120)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27aeae317f3195dc",
   "metadata": {},
   "source": [
    "Listing all the tables"
   ]
  },
  {
   "cell_type": "code",
   "id": "cf5dc5f8997e3c70",
   "metadata": {},
   "source": [
    "tables_query = \"\"\"\n",
    "               SELECT name\n",
    "               FROM sqlite_master\n",
    "               WHERE type='table'\n",
    "               ORDER BY name; \\\n",
    "               \"\"\"\n",
    "\n",
    "tables_df = pd.read_sql_query(tables_query, conn)\n",
    "print(\"Tables in the database:\")\n",
    "display(tables_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c46cee5dd503efb6",
   "metadata": {},
   "source": [
    "Creating a dictionary of type table_name -> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "id": "74de9cc40660ddfd",
   "metadata": {},
   "source": [
    "table_names = tables_df[\"name\"].tolist()\n",
    "\n",
    "schemas = {}\n",
    "\n",
    "for table in table_names:\n",
    "    pragma_query = f\"PRAGMA table_info({table});\"\n",
    "    schema_df = pd.read_sql_query(pragma_query, conn)\n",
    "    schemas[table] = schema_df\n",
    "    print(f\"\\nSchema for table '{table}':\")\n",
    "    display(schema_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "40468af8",
   "metadata": {},
   "source": [
    "# creating a dictionary to map dataframes\n",
    "DF_MAP = {}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "989e7b2b",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65560f7e",
   "metadata": {},
   "source": [
    "Add new data frame function"
   ]
  },
  {
   "cell_type": "code",
   "id": "8701b129",
   "metadata": {},
   "source": [
    "def add_new_df(global_name: str, dfs_key: str, dataframe=None):\n",
    "\n",
    "    # if dataframe already exists in dfs → ignore \"dataframe\" input\n",
    "    if dfs_key in dfs:\n",
    "        globals()[global_name] = dfs[dfs_key]\n",
    "        DF_MAP[global_name] = dfs_key\n",
    "        print(f\"Mapped existing dfs['{dfs_key}'] to global '{global_name}'\")\n",
    "        return\n",
    "\n",
    "    # new dataframe\n",
    "    if dataframe is None:\n",
    "        raise ValueError(\n",
    "            f\"dfs_key '{dfs_key}' does not exist in dfs and no dataframe was provided.\"\n",
    "        )\n",
    "\n",
    "    dfs[dfs_key] = dataframe\n",
    "    globals()[global_name] = dataframe\n",
    "    DF_MAP[global_name] = dfs_key\n",
    "\n",
    "    print(f\"Added NEW df: global '{global_name}' → dfs['{dfs_key}']\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e1ae02fc",
   "metadata": {},
   "source": [
    "Syn data frame function"
   ]
  },
  {
   "cell_type": "code",
   "id": "f1731d00b21cbbf",
   "metadata": {},
   "source": [
    "def sync_dataframe(*dfs_to_refresh):\n",
    "    globals_dict = globals()\n",
    "\n",
    "    for df_obj in dfs_to_refresh:\n",
    "        updated = False\n",
    "\n",
    "        for global_name, dfs_key in DF_MAP.items():\n",
    "            if df_obj is globals_dict[global_name]:\n",
    "                globals_dict[global_name] = dfs[dfs_key]\n",
    "                updated = True\n",
    "                break\n",
    "\n",
    "        if not updated:\n",
    "            raise ValueError(\"Unknown dataframe passed!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1303d04b68150eb6",
   "metadata": {},
   "source": [
    "Searching function"
   ]
  },
  {
   "cell_type": "code",
   "id": "5d963a2b260481b4",
   "metadata": {},
   "source": [
    "# takes the name of the table, the name of the target key, and the value as inputs. Returns singular or multiple dataframes based on the request.\n",
    "\n",
    "# can also search for null values if value is set to None.\n",
    "\n",
    "def search_by_parameter(table_name, key, value):\n",
    "    df = dfs[table_name]\n",
    "\n",
    "    if value is None:\n",
    "        return df[df[key].isna()]\n",
    "\n",
    "    return df[df[key] == value]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5b55fbbe328695f4",
   "metadata": {},
   "source": [
    "# calculates std deviation\n",
    "# make sure no nulls are included in the calculation!\n",
    "\n",
    "def manual_std(values):\n",
    "    arr = np.array(values, dtype=float)\n",
    "\n",
    "    n = len(arr)\n",
    "\n",
    "    if n <= 1:\n",
    "        return 0.0\n",
    "\n",
    "    mean = arr.mean()\n",
    "    return np.sqrt(((arr - mean) ** 2).mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c3e541a4",
   "metadata": {},
   "source": [
    "def covert_string_to_date(date_str):\n",
    "    if date_str is None or pd.isna(date_str):\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    except:\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d76218baeef6680",
   "metadata": {},
   "source": [
    "def elbow_method(X, k_min=1, k_max=10, random_state=42, plot=True):\n",
    "    \"\"\"\n",
    "    Считает значения inertia для разных k (кол-во кластеров)\n",
    "    и, при желании, рисует elbow-график.\n",
    "\n",
    "    X      - матрица признаков (например, X_scaled)\n",
    "    k_min  - минимальное число кластеров\n",
    "    k_max  - максимальное число кластеров\n",
    "    plot   - рисовать ли график\n",
    "    \"\"\"\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        kmeans = KMeans(\n",
    "            n_clusters=k,\n",
    "            random_state=random_state,\n",
    "            n_init=10\n",
    "        )\n",
    "        kmeans.fit(X)\n",
    "        results.append({\"k\": k, \"inertia\": kmeans.inertia_})\n",
    "\n",
    "    elbow_df = pd.DataFrame(results)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.plot(elbow_df[\"k\"], elbow_df[\"inertia\"], marker=\"o\")\n",
    "        plt.xlabel(\"Number of clusters (k)\")\n",
    "        plt.ylabel(\"Inertia\")\n",
    "        plt.title(\"Elbow method\")\n",
    "        plt.xticks(elbow_df[\"k\"])\n",
    "        plt.show()\n",
    "\n",
    "    return elbow_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def compute_silhouette_score(X, labels, sample_size=None, random_state=42, metric=\"euclidean\"):\n",
    "    \"\"\"\n",
    "    X           : feature matrix (preferably after StandardScaler)\n",
    "    labels      : cluster labels (k-means, agglomerative, etc.)\n",
    "    sample_size: if None, we consider all objects, otherwise we take a random subsample of the specified size\n",
    "    random_state: random state fixation for reproducibility\n",
    "    metric: distance metric (Euclidean by default)\n",
    "    \"\"\"\n",
    "    X = np.asarray(X)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    if X.shape[0] != labels.shape[0]:\n",
    "        raise ValueError(f\"X has {X.shape[0]} rows, but labels has {labels.shape[0]} elements\")\n",
    "\n",
    "    if sample_size is not None and sample_size < len(X):\n",
    "        rng = np.random.RandomState(random_state)\n",
    "        idx = rng.choice(len(X), size=sample_size, replace=False)\n",
    "        X_used = X[idx]\n",
    "        labels_used = labels[idx]\n",
    "    else:\n",
    "        X_used = X\n",
    "        labels_used = labels\n",
    "\n",
    "    score = silhouette_score(X_used, labels_used, metric=metric)\n",
    "    return score"
   ],
   "id": "6c10d4ecd680e8df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def inspect_cluster(movies_df, merged_df, cluster_id):\n",
    "    \"\"\"\n",
    "    movies_df — table with movie_id and cluster_id \n",
    "    merged_df — merged table with movie_id + title\n",
    "    cluster_id — cluster index to inspect\n",
    "    \"\"\"\n",
    "    cluster = movies_df[movies_df[\"cluster_id\"] == cluster_id]\n",
    "\n",
    "    display(cluster.describe())\n",
    "\n",
    "    detailed = cluster.merge(\n",
    "        merged_df[[\"movie_id\", \"title\"]].drop_duplicates(),\n",
    "        on=\"movie_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    print(f\"\\nФильмы в кластере {cluster_id}:\")\n",
    "    return detailed"
   ],
   "id": "191c9313fa28c536",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c2f9cc01",
   "metadata": {},
   "source": [
    "### Database initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f77086",
   "metadata": {},
   "source": [
    "implementing dfs and importing data"
   ]
  },
  {
   "cell_type": "code",
   "id": "f9fff70c",
   "metadata": {},
   "source": [
    "dfs = {} # dictionary that maps the name of the table to the related data frame\n",
    "# isnt it simpler if we just use universal dataframes for each dataset instead of having to worry about dictionaries\n",
    "\n",
    "for t in table_names:\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {t};\", conn)\n",
    "    dfs[t] = df\n",
    "    print(f\"\\nLoaded table '{t}' with shape {df.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2aebc136",
   "metadata": {},
   "source": [
    "# adding our base data frames\n",
    "add_new_df(\"movies_stats\", \"movie_statistics\", dfs[\"movie_statistics\"])\n",
    "add_new_df(\"movies\",       \"movies\",           dfs[\"movies\"])\n",
    "add_new_df(\"user_stats\",   \"user_statistics\",  dfs[\"user_statistics\"])\n",
    "add_new_df(\"viewer_ratings\", \"viewer_ratings\", dfs[\"viewer_ratings\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f1e8ddfc",
   "metadata": {},
   "source": [
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5d6109d206cea",
   "metadata": {},
   "source": [
    "Counting all missing values, diagnostics purposes only"
   ]
  },
  {
   "cell_type": "code",
   "id": "def25638f8eb473b",
   "metadata": {},
   "source": [
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name} missing values (%):\")\n",
    "    missing_pct = df.isna().mean() * 100\n",
    "    display(missing_pct.to_frame(\"missing_%\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "3ce2621ae7448d16",
   "metadata": {},
   "source": [
    "missing_after = user_stats[user_stats[\"std_rating\"].isna()].copy()\n",
    "\n",
    "print(\"Всего пользователей с NaN std:\", len(missing_after))\n",
    "\n",
    "# 1) Есть ли они вообще в viewer_ratings?\n",
    "print(\n",
    "    \"Сколько из них встречаются в viewer_ratings:\",\n",
    "    missing_after[\"customer_id\"].isin(viewer_ratings[\"customer_id\"]).sum()\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "95d469c6",
   "metadata": {},
   "source": [
    "### Movie Statistics Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78394717c371ac27",
   "metadata": {},
   "source": [
    "A function to calculate missing std. ratings of films"
   ]
  },
  {
   "cell_type": "code",
   "id": "1395d0b962d46cb5",
   "metadata": {},
   "source": [
    "# Синхронизируем dataframes\n",
    "sync_dataframe(movies_stats, viewer_ratings)\n",
    "\n",
    "# Убираем NaN-рейтинги и считаем std для фильмов\n",
    "viewer_ratings_clean = viewer_ratings[~viewer_ratings[\"rating\"].isna()]\n",
    "\n",
    "movie_std = (\n",
    "    viewer_ratings[~viewer_ratings[\"rating\"].isna()]\n",
    "    .groupby(\"movie_id\")[\"rating\"]\n",
    "    .apply(list)\n",
    "    .reset_index(name=\"ratings\")\n",
    ")\n",
    "\n",
    "# manual_std уже объявлена выше и игнорирует NaN (или можно явно указать ignore_nan=True)\n",
    "movie_std[\"std_rating\"] = movie_std[\"ratings\"].apply(manual_std)\n",
    "movie_std = movie_std[[\"movie_id\", \"std_rating\"]]\n",
    "\n",
    "# Старый процент пропусков\n",
    "old_null_pct = dfs[\"movie_statistics\"][\"std_rating\"].isna().mean() * 100\n",
    "\n",
    "# Мержим новые значения std по movie_id\n",
    "movies_stats = movies_stats.merge(\n",
    "    movie_std,\n",
    "    on=\"movie_id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_new\")\n",
    ")\n",
    "\n",
    "# Обновляем std_rating\n",
    "movies_stats[\"std_rating\"] = movies_stats[\"std_rating_new\"]\n",
    "movies_stats.drop(columns=[\"std_rating_new\"], inplace=True)\n",
    "\n",
    "# Сохраняем обратно\n",
    "dfs[\"movie_statistics\"] = movies_stats\n",
    "\n",
    "# Новый процент пропусков\n",
    "new_null_pct = movies_stats[\"std_rating\"].isna().mean() * 100\n",
    "\n",
    "# Улучшения\n",
    "improvement_abs = old_null_pct - new_null_pct\n",
    "improvement_rel = (improvement_abs / old_null_pct) * 100 if old_null_pct > 0 else 0\n",
    "\n",
    "print(f\"Missing values reduced from {old_null_pct:.2f}% to {new_null_pct:.2f}%.\")\n",
    "print(f\"Absolute improvement: {improvement_abs:.2f}%\")\n",
    "print(f\"Relative improvement: {improvement_rel:.2f}% better than before.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "541a627d968fd125",
   "metadata": {},
   "source": [
    "#Figure out how to drop na values in general\n",
    "movies_stats = dfs['movie_statistics']\n",
    "\n",
    "print(f\"Before cleaning: {len(movies_stats)} movies\")\n",
    "#movies_stats = movies_stats.dropna(subset=['std_rating'])\n",
    "#dfs['movie_statistics'] = movies_stats\n",
    "print(f\"After removing single-rating movies: {len(movies_stats)} movies\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a5d4518d",
   "metadata": {},
   "source": [
    "Calculating the missing total_ratings of movies"
   ]
  },
  {
   "cell_type": "code",
   "id": "11b6c1bf",
   "metadata": {},
   "source": [
    "sync_dataframe(movies_stats)\n",
    "\n",
    "# Collecting all the movies with absent total_rating in a dictionary\n",
    "missing_dict = {}\n",
    "\n",
    "missing = search_by_parameter('movie_statistics', 'total_ratings', None)\n",
    "missing_dict = {row.movie_id: 0 for row in missing.itertuples(index=False)}\n",
    "\n",
    "# Iterating through viewer_ratings and manually counting the ratings for each film\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "    if movie_id in missing_dict:\n",
    "        missing_dict[movie_id] += 1\n",
    "\n",
    "# Update movie_stats\n",
    "for row in movies_stats.itertuples(index=True):\n",
    "    if row.movie_id in missing_dict:\n",
    "        movies_stats.at[row.Index, \"total_ratings\"] = missing_dict[row.movie_id]\n",
    "\n",
    "dfs[\"movie_statistics\"] = movies_stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "13f9afb8",
   "metadata": {},
   "source": [
    "Calculating Missing Averages"
   ]
  },
  {
   "cell_type": "code",
   "id": "63d4756a",
   "metadata": {},
   "source": [
    "sync_dataframe(movies_stats, viewer_ratings)\n",
    "\n",
    "# Finding movies with null avg_rating\n",
    "missing_avg = search_by_parameter('movie_statistics', 'avg_rating', None)\n",
    "\n",
    "# Creating a dict of type { movie_id : avg_rating }\n",
    "# set 0 as base value for now, might change it later\n",
    "missing_avg_dict = {row.movie_id: 0 for row in missing_avg.itertuples(index=False)}\n",
    "\n",
    "# Storing the sum of all ratings for each movie\n",
    "rating_sums = {movie_id: 0 for movie_id in missing_avg_dict}\n",
    "\n",
    "# Iterating through viewer ratings and adding to sum if movie_id matches\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if movie_id in missing_avg_dict:\n",
    "        rating_sums[movie_id] += rating\n",
    "\n",
    "for row in movies_stats.itertuples():\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    if movie_id in rating_sums:\n",
    "        total = row.total_ratings # I'm assuming that my calculations of total_ratings per movie is correct ang i got rid of                            all null values\n",
    "\n",
    "        # IF FORE SOME MAGICAL REASON THERE IS STILL A NULL THEN IGNORE\n",
    "        if pd.isna(total) or total == 0:\n",
    "            avg = 0\n",
    "        else:\n",
    "            avg = rating_sums[movie_id] / total\n",
    "\n",
    "        movies_stats.at[row.Index, \"avg_rating\"] = avg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21090df8",
   "metadata": {},
   "source": [
    "Calculating the missing min and max ratings for movies"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9bb36c2",
   "metadata": {},
   "source": [
    "sync_dataframe(movies_stats)\n",
    "\n",
    "# Find movies with missing min_rating and max_rating using your function\n",
    "missing_min = search_by_parameter('movie_statistics', 'min_rating', None)\n",
    "missing_max = search_by_parameter('movie_statistics', 'max_rating', None)\n",
    "\n",
    "# Combine them as some movies may be in both\n",
    "missing_ids = set(missing_min[\"movie_id\"]) | set(missing_max[\"movie_id\"])\n",
    "\n",
    "# Take only ratings for the movies we care\n",
    "relevant_ratings = viewer_ratings[viewer_ratings[\"movie_id\"].isin(missing_ids)]\n",
    "\n",
    "# Building a nested dict {movie_id : {\"min\": ..., \"max\": ...}}\n",
    "min_max_dict = {}\n",
    "\n",
    "for row in relevant_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if movie_id not in min_max_dict:\n",
    "        min_max_dict[movie_id] = {\"min\": rating, \"max\": rating}\n",
    "    else:\n",
    "        if rating < min_max_dict[movie_id][\"min\"]:\n",
    "            min_max_dict[movie_id][\"min\"] = rating\n",
    "        if rating > min_max_dict[movie_id][\"max\"]:\n",
    "            min_max_dict[movie_id][\"max\"] = rating\n",
    "\n",
    "# Update movie_statistics\n",
    "for row in movies_stats.itertuples(index=True):\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    if movie_id in min_max_dict:\n",
    "        if pd.isna(row.min_rating):\n",
    "            movies_stats.at[row.Index, \"min_rating\"] = min_max_dict[movie_id][\"min\"]\n",
    "        if pd.isna(row.max_rating):\n",
    "            movies_stats.at[row.Index, \"max_rating\"] = min_max_dict[movie_id][\"max\"]\n",
    "\n",
    "dfs[\"movie_statistics\"] = movies_stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "a83870a25990a35e",
   "metadata": {},
   "source": [
    "Finding missing unique users"
   ]
  },
  {
   "cell_type": "code",
   "id": "65c9dd911b77bbd1",
   "metadata": {},
   "source": [
    "sync_dataframe(movies_stats)\n",
    "\n",
    "missing_unique = search_by_parameter('movie_statistics', 'unique_users', None)\n",
    "\n",
    "# creating my favorite movie set\n",
    "missing_movie_ids = {row.movie_id for row in missing_unique.itertuples(index=False)}\n",
    "\n",
    "# creating a dict movie_id: customer_id\n",
    "unique_users_dict = {movie_id: set() for movie_id in missing_movie_ids}\n",
    "\n",
    "# Gathering unique users\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    # as always im getting only those movies which have null for unique users\n",
    "    if movie_id in unique_users_dict:\n",
    "        unique_users_dict[movie_id].add(row.customer_id)\n",
    "\n",
    "# Counting unique users\n",
    "updated = 0\n",
    "for movie_id, users in unique_users_dict.items():\n",
    "    count = len(users)  # unique users count\n",
    "\n",
    "    movies_stats.loc[\n",
    "        movies_stats[\"movie_id\"] == movie_id,\n",
    "        \"unique_users\"\n",
    "    ] = count\n",
    "\n",
    "    updated += 1\n",
    "\n",
    "# just in case if a movie has 0 ratings im setting unique users to 0\n",
    "movies_stats[\"unique_users\"] = movies_stats[\"unique_users\"].fillna(0).astype(int)\n",
    "\n",
    "# Updating\n",
    "dfs[\"movie_statistics\"] = movies_stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8ee021deca5034ee",
   "metadata": {},
   "source": [
    "Checking for any duplicate movie ids and removing them from all datasets to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "id": "d8681d7bb16a5d50",
   "metadata": {},
   "source": [
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3f9981c970eddd04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "c18107ca34206a01",
   "metadata": {},
   "source": [
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e4807162c092ff91",
   "metadata": {},
   "source": [
    "### User_statistics calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0dc4f80d5c7f16",
   "metadata": {},
   "source": [
    "Calculating user's std rating"
   ]
  },
  {
   "cell_type": "code",
   "id": "9820aaa2583197",
   "metadata": {},
   "source": [
    "sync_dataframe(user_stats)\n",
    "\n",
    "# grouping by user id\n",
    "user_std = (\n",
    "    viewer_ratings[~viewer_ratings[\"rating\"].isna()]\n",
    "    .groupby(\"customer_id\")[\"rating\"]\n",
    "    .apply(list)\n",
    "    .reset_index(name=\"ratings\")\n",
    ")\n",
    "\n",
    "user_std[\"std_rating\"] = user_std[\"ratings\"].apply(manual_std)\n",
    "user_std = user_std[[\"customer_id\", \"std_rating\"]]\n",
    "\n",
    "# old percentage\n",
    "old_null_pct = user_stats[\"std_rating\"].isna().mean() * 100\n",
    "\n",
    "# merging\n",
    "user_stats = user_stats.merge(\n",
    "    user_std,\n",
    "    on=\"customer_id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_new\")\n",
    ")\n",
    "\n",
    "# replacing old std with new\n",
    "user_stats[\"std_rating\"] = user_stats[\"std_rating_new\"]\n",
    "user_stats.drop(columns=[\"std_rating_new\"], inplace=True)\n",
    "\n",
    "# updating\n",
    "dfs[\"user_statistics\"] = user_stats\n",
    "\n",
    "# calculating new percantages\n",
    "new_null_pct = user_stats[\"std_rating\"].isna().mean() * 100\n",
    "\n",
    "# calculating improvements\n",
    "improvement_abs = old_null_pct - new_null_pct\n",
    "improvement_rel = (improvement_abs / old_null_pct) * 100 if old_null_pct > 0 else 0\n",
    "\n",
    "print(f\"Missing values reduced from {old_null_pct:.2f}% to {new_null_pct:.2f}%.\")\n",
    "print(f\"Absolute improvement: {improvement_abs:.2f}%\")\n",
    "print(f\"Relative improvement: {improvement_rel:.2f}% better than before.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ed22d075b53743f9",
   "metadata": {},
   "source": [
    "Calculating user's total ratings"
   ]
  },
  {
   "cell_type": "code",
   "id": "3e6f6477f9abcc7b",
   "metadata": {},
   "source": [
    "sync_dataframe(user_stats)\n",
    "\n",
    "# Collecting all the users with absent total_ratings\n",
    "missing_dict = {}\n",
    "\n",
    "missing = search_by_parameter('user_statistics', 'total_ratings', None)\n",
    "missing_dict = {row.customer_id: 0 for row in missing.itertuples(index=False)}\n",
    "\n",
    "# Iterating through viewer_ratings and manually counting the ratings for each user\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    if pd.isna(row.rating):\n",
    "        continue\n",
    "\n",
    "    customer_id = row.customer_id\n",
    "    if customer_id in missing_dict:\n",
    "        missing_dict[customer_id] += 1\n",
    "\n",
    "# Update user_stats\n",
    "for row in user_stats.itertuples(index=True):\n",
    "    if row.customer_id in missing_dict:\n",
    "        user_stats.at[row.Index, \"total_ratings\"] = missing_dict[row.customer_id]\n",
    "\n",
    "dfs[\"user_statistics\"] = user_stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8b7f2348b331e259",
   "metadata": {},
   "source": [
    "Calculating user's avg rating"
   ]
  },
  {
   "cell_type": "code",
   "id": "4641d086358af2ec",
   "metadata": {},
   "source": [
    "sync_dataframe(user_stats)\n",
    "\n",
    "# Finding all users with null avg_rating\n",
    "missing_avg = search_by_parameter('user_statistics', 'avg_rating', None)\n",
    "\n",
    "# Creating a dict of type { customer_id : avg_rating }\n",
    "# set 0 as base value\n",
    "missing_avg_dict = {row.customer_id: 0 for row in missing_avg.itertuples(index=False)}\n",
    "\n",
    "# Storing the sum of all ratings for each movie\n",
    "rating_sums = {customer_id: 0 for customer_id in missing_avg_dict}\n",
    "\n",
    "# Iterating through viewer ratings and adding to sum if movie_id matches\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    if pd.isna(row.rating):\n",
    "        continue\n",
    "\n",
    "    customer_id = row.customer_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if customer_id in missing_avg_dict:\n",
    "        rating_sums[customer_id] += rating\n",
    "\n",
    "for row in user_stats.itertuples():\n",
    "    customer_id = row.customer_id\n",
    "\n",
    "    if customer_id in rating_sums:\n",
    "        total = row.total_ratings\n",
    "\n",
    "        if pd.isna(total) or total == 0:\n",
    "            avg = 0\n",
    "        else:\n",
    "            avg = rating_sums[customer_id] / total\n",
    "\n",
    "        user_stats.at[row.Index, \"avg_rating\"] = avg"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "34cdefa630a021fd",
   "metadata": {},
   "source": [
    "#activity days\n",
    "sync_dataframe(user_stats)\n",
    "\n",
    "missing = search_by_parameter('user_statistics', 'activity_days', None)\n",
    "\n",
    "# creating a dict\n",
    "missing_dict = {row.customer_id: 0 for row in missing.itertuples(index=False)}\n",
    "\n",
    "for row in user_stats.itertuples():\n",
    "    customer_id = row.customer_id\n",
    "\n",
    "    if customer_id not in missing_dict: # wroking only with users with missing activity_days\n",
    "        continue\n",
    "\n",
    "    first_date = covert_string_to_date(row.first_rating_date)\n",
    "    last_date = covert_string_to_date(row.last_rating_date)\n",
    "\n",
    "    # null exception\n",
    "    if first_date is None or last_date is None:\n",
    "        continue\n",
    "\n",
    "    # calcing the difference\n",
    "    difference = (last_date - last_date).days\n",
    "\n",
    "    # updating activity_days\n",
    "    user_stats.at[row.Index, 'activity_days'] = difference\n",
    "\n",
    "null_percentage = user_stats['activity_days'].isna().mean() * 100\n",
    "print(f\"Missing activity_days: {null_percentage}%\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "fbb5f2e6dfe2b465",
   "metadata": {},
   "source": [
    "#minmax\n",
    "sync_dataframe(user_stats)\n",
    "\n",
    "# Find users with missing min_rating and max_rating using your function\n",
    "missing_min = search_by_parameter('user_statistics', 'min_rating', None)\n",
    "missing_max = search_by_parameter('user_statistics', 'max_rating', None)\n",
    "\n",
    "# Combine them as some users may be in both\n",
    "missing_ids = set(missing_min[\"customer_id\"]) | set(missing_max[\"customer_id\"])\n",
    "\n",
    "# Init dictionary for users we need to compute\n",
    "missing_users = {\n",
    "    customer_id: {\"min\": -1, \"max\": -1} #setting base to -1\n",
    "    for customer_id in missing_ids\n",
    "}\n",
    "\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    customer_id = row.customer_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if customer_id in missing_users:\n",
    "        # updating min\n",
    "        if missing_users[customer_id][\"min\"] == -1 or rating < missing_users[customer_id][\"min\"]:\n",
    "            missing_users[customer_id][\"min\"] = rating\n",
    "\n",
    "        # updating max\n",
    "        if missing_users[customer_id][\"max\"] == -1 or rating > missing_users[customer_id][\"max\"]:\n",
    "            missing_users[customer_id][\"max\"] = rating\n",
    "\n",
    "# Updating user stats\n",
    "for row in user_stats.itertuples():\n",
    "    customer_id = row.customer_id\n",
    "\n",
    "    if customer_id in missing_users:\n",
    "        min_val = missing_users[customer_id][\"min\"]\n",
    "        max_val = missing_users[customer_id][\"max\"]\n",
    "\n",
    "        # if the guy hasn't rated anything im making sure we set min or max to none so we get rid of them in the future\n",
    "        if min_val == -1 and max_val == -1:\n",
    "            min_val = None\n",
    "            max_val = None\n",
    "\n",
    "        user_stats.at[row.Index, \"min_rating\"] = min_val\n",
    "        user_stats.at[row.Index, \"max_rating\"] = max_val\n",
    "\n",
    "dfs[\"user_statistics\"] = user_stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "de42b238",
   "metadata": {},
   "source": [
    "sync_dataframe(user_stats)\n",
    "missing_unique = search_by_parameter('user_statistics', 'unique_movies', None)\n",
    "\n",
    "missing_ids = set(missing_unique[\"customer_id\"])\n",
    "\n",
    "unique_movies_dict = {customer_id: set() for customer_id in missing_ids}\n",
    "\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    user_id = row.customer_id\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    if user_id in unique_movies_dict:\n",
    "        unique_movies_dict[user_id].add(movie_id)\n",
    "\n",
    "for row in user_stats.itertuples():\n",
    "    user_id = row.customer_id\n",
    "\n",
    "    if user_id in unique_movies_dict:\n",
    "        count_unique = len(unique_movies_dict[user_id])\n",
    "\n",
    "        if count_unique == 0:\n",
    "            value = None\n",
    "        else:\n",
    "            value = count_unique\n",
    "\n",
    "        user_stats.at[row.Index, \"unique_movies\"] = value\n",
    "\n",
    "# updating\n",
    "dfs[\"user_statistics\"] = user_stats"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d12e52c5",
   "metadata": {},
   "source": [
    "### Merging datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c36b6",
   "metadata": {},
   "source": [
    "Merging the Movies and movie statis filling in missing values on either dataset and converting all of the dates to type DateTime as well as all counts and years to integers.\n",
    "This is in order to clean our movie data before merging it with our user data to fill in any recoverable missing values.\n",
    "Rows with missing values will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "id": "46319c89",
   "metadata": {},
   "source": [
    "movies = dfs['movies'].copy() # does this overwrite the movies we have right now? maybe call it something else.\n",
    "movie_stats = dfs['movie_statistics'].copy()\n",
    "viewer_ratings = dfs['viewer_ratings'].copy()\n",
    "\n",
    "bad_ids_movies = movies.loc[movies['movie_id'].duplicated(keep=False), 'movie_id'].unique().tolist()\n",
    "bad_ids_stats = movie_stats.loc[movie_stats['movie_id'].duplicated(keep=False), 'movie_id'].unique().tolist()\n",
    "bad_ids = list(set(bad_ids_movies + bad_ids_stats))\n",
    "print('bad movie ids to remove: ', bad_ids)\n",
    "\n",
    "viewer_ratings = viewer_ratings[~viewer_ratings['movie_id'].isin(bad_ids)]\n",
    "movies = movies[~movies['movie_id'].isin(bad_ids)]\n",
    "movie_stats = movie_stats[~movie_stats['movie_id'].isin(bad_ids)]\n",
    "\n",
    "movies['year_of_release'] = (pd.to_numeric(movies['year_of_release'], errors='coerce').astype('Int64'))\n",
    "movie_stats['total_ratings'] = (pd.to_numeric(movie_stats['total_ratings'], errors='coerce').astype('Int64'))\n",
    "movie_stats['unique_users'] = (pd.to_numeric(movie_stats['unique_users'], errors='coerce').astype('Int64'))\n",
    "movie_stats['year_of_release'] = (pd.to_numeric(movie_stats['year_of_release'], errors='coerce').astype('Int64'))\n",
    "\n",
    "movie_stats['first_rating_date'] = pd.to_datetime(movie_stats['first_rating_date'], errors='coerce')\n",
    "movie_stats['last_rating_date'] = pd.to_datetime(movie_stats['last_rating_date'], errors='coerce')\n",
    "\n",
    "# this will automatically remove rows with missing data\n",
    "movie_full = movies.merge(movie_stats, on='movie_id', how='inner', suffixes=('_movies', '_stats'))\n",
    "\n",
    "movie_full['title'] = movie_full['title_movies'].combine_first(movie_full['title_stats'])\n",
    "movie_full['year_of_release'] = movie_full['year_of_release_movies'].combine_first(movie_full['year_of_release_stats'])\n",
    "\n",
    "movie_full = movie_full.drop(columns=['title_movies', 'title_stats', 'year_of_release_movies', 'year_of_release_stats'])\n",
    "\n",
    "dfs['movies'] = movies\n",
    "dfs['movie_statistics'] = movie_stats\n",
    "dfs['viewer_ratings'] = viewer_ratings\n",
    "add_new_df('movie_full', 'movie_full', movie_full)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d1d3b2a1691c50e",
   "metadata": {},
   "source": [
    "print(\"movies dtypes:\")\n",
    "print(movies.dtypes)\n",
    "\n",
    "print(\"\\nmovie_stats dtypes:\")\n",
    "print(movie_stats.dtypes)\n",
    "\n",
    "print(\"\\nmovie_full dtypes:\")\n",
    "print(movie_full.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4bec12db",
   "metadata": {},
   "source": [
    "- Converts the date parameter in viewer_ratings to datetime.\n",
    "- Merges viewer_ratings, movies, movie_statistics and user_statistics into one dataset as merged_data."
   ]
  },
  {
   "cell_type": "code",
   "id": "bc40bfb82470b10a",
   "metadata": {},
   "source": [
    "movie_full = dfs['movie_full'].copy()\n",
    "user_stats = dfs['user_statistics'].copy()\n",
    "viewer_ratings = dfs['viewer_ratings'].copy()\n",
    "\n",
    "\n",
    "viewer_ratings['date'] = pd.to_datetime(viewer_ratings['date'], errors = 'coerce')\n",
    "user_stats['first_rating_date'] = pd.to_datetime(user_stats['first_rating_date'], errors = 'coerce')\n",
    "user_stats['last_rating_date'] = pd.to_datetime(user_stats['last_rating_date'], errors = 'coerce')\n",
    "\n",
    "user_stats['total_ratings'] = (pd.to_numeric(user_stats['total_ratings'], errors='coerce').astype('Int64'))\n",
    "user_stats['unique_movies'] = (pd.to_numeric(user_stats['unique_movies'], errors='coerce').astype('Int64'))\n",
    "user_stats['activity_days'] = (pd.to_numeric(user_stats['activity_days'], errors='coerce').astype('Int64'))\n",
    "\n",
    "\n",
    "# create user and movie specific columns\n",
    "movie_full = movie_full.rename(columns={\n",
    "    'total_ratings':     'movie_total_ratings',\n",
    "    'avg_rating':        'movie_avg_rating',\n",
    "    'std_rating':        'movie_std_rating',\n",
    "    'min_rating':        'movie_min_rating',\n",
    "    'max_rating':        'movie_max_rating',\n",
    "    'first_rating_date': 'movie_first_rating_date',\n",
    "    'last_rating_date':  'movie_last_rating_date'\n",
    "})\n",
    "\n",
    "\n",
    "user_stats = user_stats.rename(columns={\n",
    "    'total_ratings':     'user_total_ratings',\n",
    "    'avg_rating':        'user_avg_rating',\n",
    "    'std_rating':        'user_std_rating',\n",
    "    'min_rating':        'user_min_rating',\n",
    "    'max_rating':        'user_max_rating',\n",
    "    'first_rating_date': 'user_first_rating_date',\n",
    "    'last_rating_date':  'user_last_rating_date'\n",
    "})\n",
    "\n",
    "# drop anomalous date for user stats\n",
    "if 'anomalous_date' in viewer_ratings.columns:\n",
    "    viewer_ratings = viewer_ratings.drop(columns=['anomalous_date'])\n",
    "\n",
    "# merge user stats and viewer ratings\n",
    "viewer_ratings_with_stats = viewer_ratings.merge(user_stats, on = 'customer_id', how = 'inner')\n",
    "\n",
    "merged_data = viewer_ratings_with_stats.merge(movie_full, on = 'movie_id', how = 'inner')\n",
    "\n",
    "# this will drop all rows with any null\n",
    "merged_data = merged_data.dropna(how = 'any')\n",
    "\n",
    "add_new_df('merged_data', 'merged_data', merged_data)\n",
    "dfs['movie_full'] = movie_full\n",
    "dfs['user_statistics'] = user_stats\n",
    "dfs['viewer_ratings'] = viewer_ratings\n",
    "\n",
    "print(merged_data.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7908f360",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "id": "6706f664",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "12ac23fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5bba06e",
   "metadata": {},
   "source": [
    "### Plotting Statistics and Overall findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d80a8",
   "metadata": {},
   "source": [
    "Ryder"
   ]
  },
  {
   "cell_type": "code",
   "id": "2df1e09f",
   "metadata": {},
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Markdown cell above\n",
    "\"\"\"\n",
    "### Global Rating Distribution\n",
    "\n",
    "There is a clear over positive bias. This can be due to the fact that this was done on a streaming platform where users are more likely to rate higher\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(11, 6)) # made it this size for a better fit in the read me\n",
    "ax = sns.histplot(\n",
    "    data=dfs['viewer_ratings'],\n",
    "    x='rating',\n",
    "    bins=5,\n",
    "    discrete=True, #tells seaborn to treat x as integer values\n",
    "    color=\"#0062ff\",\n",
    "    edgecolor='white',\n",
    "    alpha=0.85,  #transparency\n",
    "    linewidth=1.5\n",
    ")\n",
    "\n",
    "# Add exact counts on top of each bar\n",
    "for rect in ax.patches:\n",
    "    height = rect.get_height()\n",
    "    ax.text(\n",
    "        rect.get_x() + rect.get_width()/2., \n",
    "        height + 200_000,                    # a bit above the bar\n",
    "        f'{int(height):,}',                  # adds commas: 12,345,678\n",
    "        ha='center', va='bottom', fontsize=12, fontweight='bold'\n",
    "    )\n",
    "\n",
    "# Clean y-axis\n",
    "plt.ylabel('Number of Ratings (millions)', fontsize=12)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))\n",
    "\n",
    "plt.title('Distribution of Viewer Ratings\\n(Strong Positive Bias)', \n",
    "          fontsize=15, pad=20)\n",
    "plt.xlabel('Rating (1–5)', fontsize=12)\n",
    "plt.xticks(range(0, 7))\n",
    "plt.ylim(0, 2_000_000)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2c540e1a",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "### User Average Rating vs. Rating Variability\n",
    "\n",
    "Scatter from user_statistics: High avg with low std = consistent positive raters. \n",
    "Valuable for grouping users (e.g., strict critics vs. easy fans). Alpha for density.\n",
    "\"\"\"\n",
    "\n",
    "sync_dataframe(user_stats)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=user_stats, x='avg_rating', y='std_rating', alpha=0.4, color='#0062ff', edgecolor=None)\n",
    "plt.title('User Average Rating vs. Standard Deviation\\n(Rating Consistency Patterns)', fontsize=15, pad=20)\n",
    "plt.xlabel('Average Rating Given (1–5)', fontsize=12)\n",
    "plt.ylabel('Std Deviation of Ratings', fontsize=12)\n",
    "plt.xlim(0, 5)\n",
    "plt.ylim(0, 4)  # Std typically low due to bias\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6a292f8066fef5d2",
   "metadata": {},
   "source": [
    "# PERFECT PENTAGON — FINAL WORKING VERSION (Dec 2025)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.patches import PathPatch\n",
    "\n",
    "# CHANGE ONLY THESE TWO VALUES\n",
    "avg_rating_value = user_stats['avg_rating'].mean()   # ← your real global avg from user_statistics\n",
    "std_rating_value = user_stats['std_rating'].mean()   # ← your real global std from user_statistics\n",
    "\n",
    "# Convert to 0–1 scale\n",
    "pull_to_five = (avg_rating_value - 1.0) / 4.0\n",
    "consistency  = np.clip(1.0 - (std_rating_value / 1.8), 0.1, 1.0)\n",
    "\n",
    "# Pentagon vertices (5 corners + first repeated to close)\n",
    "center = np.array([0.5, 0.5])\n",
    "radius = 0.45\n",
    "angles = np.linspace(0, 2*np.pi, 5, endpoint=False)\n",
    "verts = center + radius * np.column_stack([np.cos(angles), np.sin(angles)])\n",
    "verts = np.vstack([verts, verts[0]])  # close the loop → shape (6, 2)\n",
    "\n",
    "# Blue pentagon (pulled toward Rating 5 = bottom-right)\n",
    "blue_scale = 0.85 * pull_to_five\n",
    "blue_verts = center + blue_scale * (verts - center)\n",
    "\n",
    "# Red pentagon (size = consistency)\n",
    "red_scale = 0.92 * consistency\n",
    "red_verts = center + red_scale * (verts - center)\n",
    "\n",
    "# Correct Path codes: 6 vertices → 6 codes\n",
    "codes = [Path.MOVETO] + [Path.LINETO]*4 + [Path.CLOSEPOLY]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Outer frame\n",
    "frame_path = Path(verts, codes)\n",
    "ax.add_patch(PathPatch(frame_path, facecolor='none', edgecolor='black', linewidth=3))\n",
    "\n",
    "# Red pentagon (consistency)\n",
    "red_path = Path(red_verts, codes)\n",
    "ax.add_patch(PathPatch(red_path, facecolor=\"#000000\", alpha=0.7, edgecolor='#aa0000', linewidth=4))\n",
    "\n",
    "# Blue pentagon (average rating)\n",
    "blue_path = Path(blue_verts, codes)\n",
    "ax.add_patch(PathPatch(blue_path, facecolor=\"#00ffdd\", alpha=0.75, edgecolor='#003380', linewidth=4))\n",
    "\n",
    "# Corner labels\n",
    "labels = ['1', '2', '3', '4', '5']\n",
    "for (x, y), label in zip(verts[:-1], labels):\n",
    "    ax.text(x, y, label, fontsize=16, fontweight='bold', ha='center', va='center',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"white\", alpha=0.95))\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.set_title(f\"Global User Rating Personality Pentagon\\n\"\n",
    "             f\"Avg Rating = {avg_rating_value:.2f} | Std Dev = {std_rating_value:.2f}\",\n",
    "             fontsize=20, pad=40, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "d15d29082834d37d",
   "metadata": {},
   "source": [
    "'Plotting a timeline of ratings over time'\n",
    "\n",
    "sync_dataframe(movies_stats)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(data=movies_stats, x='year_of_release', y='avg_rating', alpha=0.4, color='#0062ff', edgecolor=None, s=35)\n",
    "plt.title('Movie Rating VS. Year of Release\\n(Trends Over Time)', fontsize=15, pad=20)\n",
    "plt.xlabel('Year of Release', fontsize=12)\n",
    "plt.ylabel('Average Rating Given (1–5)', fontsize=12)\n",
    "plt.xlim(1890, 2010) #this included the data for all the movies in the data set as it goes from 1896 to 2005\n",
    "plt.ylim(0, 7)  \n",
    "#plt.scatter(x,y, s)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "925d56e8f5632188",
   "metadata": {},
   "source": [
    "def get_top_rated_movies(min_ratings=100, top_n=10):\n",
    "    \"\"\"\n",
    "    Returns the top N best-rated movies with at least min_ratings.\n",
    "    Merges movie_statistics and movies tables to get titles and years.\n",
    "    \"\"\"\n",
    "    # Get movie stats and filter\n",
    "    sync_dataframe(movies_stats)\n",
    "    filtered = movies_stats[movies_stats['total_ratings'] >= min_ratings]\n",
    "    \n",
    "    # Sort by avg_rating descending\n",
    "    top_movies = filtered.sort_values('avg_rating', ascending=False).head(top_n)\n",
    "    \n",
    "    # Select and reorder columns nicely\n",
    "    result = top_movies[['movie_id', 'avg_rating', 'total_ratings']].copy()\n",
    "    \n",
    "    result = result.reset_index(drop=True)\n",
    "    result.index = result.index + 1  # Start ranking at 1\n",
    "    \n",
    "    return result\n",
    "\n",
    "# === USE IT LIKE THIS ===\n",
    "print(\"TOP 10 BEST-RATED MOVIES (min 100 ratings):\")\n",
    "display(get_top_rated_movies(min_ratings=200, top_n=10))\n",
    "\n",
    "# sync_dataframe(movies_stats)\n",
    "\n",
    "# plt.figure(figsize=(15, 10))\n",
    "# sns.scatterplot(data=get_top_rated_movies(min_ratings=100, top_n=10), x='movie_id', y='avg_rating', alpha=0.4, color='#0062ff', edgecolor=None, s=35)\n",
    "# plt.title('top 10 best-rated movies (min 100 ratings)', fontsize=15, pad=20)\n",
    "# plt.xlabel('Movies', fontsize=12)\n",
    "# plt.ylabel('Average Rating Given (1–5)', fontsize=12)\n",
    "# plt.xlim(str(search_by_parameter(\"movies\", \"movie_id\", get_top_rated_movies(min_ratings=100, top_n=10)['movie_id'].min())[\"title\"]), str(search_by_parameter(\"movies\", \"movie_id\", get_top_rated_movies(min_ratings=100, top_n=10)['movie_id'].max())[\"title\"])) #this included the data for all the movies in the data set as it goes from 1896 to 2005\n",
    "# plt.ylim(0, 5)  \n",
    "# #plt.scatter(x,y, s)\n",
    "# plt.grid(True, linestyle='--', alpha=0.7)\n",
    "# plt.show()\n",
    "\n",
    "print(str(search_by_parameter(\"movies\", \"movie_id\", get_top_rated_movies(min_ratings=200, top_n=10)['movie_id'].min())[\"title\"]))\n",
    "\n",
    "# ploting = []\n",
    "# for ploting.len() in range(10):\n",
    "#     ploting.append([search_by_parameter(\"movies\", \"movie_id\", get_top_rated_movies(min_ratings=100, top_n=10)['movie_id'])])\n",
    "\n",
    "# print(\"here\"+ploting(1))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "a8765e1370223402",
   "metadata": {},
   "source": [
    "def get_top_rated_movies(min_ratings=200, top_n=10):\n",
    "    sync_dataframe(movies_stats)\n",
    "    \n",
    "    # Filter and sort — pure movie_statistics\n",
    "    filtered = movies_stats[movies_stats['total_ratings'] >= min_ratings]\n",
    "    top_movies = filtered.sort_values('avg_rating', ascending=False).head(top_n).copy()\n",
    "    \n",
    "    # Reset index to get ranking 1 to 10\n",
    "    top_movies = top_movies.reset_index(drop=True)\n",
    "    top_movies.index = top_movies.index + 1\n",
    "    \n",
    "    # Use your existing search function to get titles\n",
    "    titles = []\n",
    "    for movie_id in top_movies['movie_id']:\n",
    "        result = search_by_parameter(\"movies\", \"movie_id\", movie_id)\n",
    "        title = result['title'].iloc[0] if not result.empty else f\"Unknown ({movie_id})\"\n",
    "        year = int(result['year_of_release'].iloc[0]) if not result.empty and pd.notna(result['year_of_release'].iloc[0]) else \"\"\n",
    "        titles.append(f\"{title} ({year})\" if year else title)\n",
    "    \n",
    "    top_movies['title_display'] = titles\n",
    "    return top_movies\n",
    "\n",
    "# === GET DATA ===\n",
    "top10 = get_top_rated_movies(min_ratings=1000, top_n=10)\n",
    "\n",
    "# === PLOT — TITLES ON X-AXIS (FIXED & WORKING) ===\n",
    "plt.figure(figsize=(16, 9))\n",
    "\n",
    "# Scatter plot with titles\n",
    "sns.scatterplot(\n",
    "    data=top10,\n",
    "    x='title_display',\n",
    "    y='avg_rating',\n",
    "    s=150,\n",
    "    color='#0062ff',\n",
    "    edgecolor='navy',\n",
    "    alpha=0.95,\n",
    "    linewidth=3\n",
    ")\n",
    "\n",
    "# Add text above each point\n",
    "for i, row in top10.iterrows():\n",
    "    plt.text(\n",
    "        x=i-1,  # categorical position\n",
    "        y=row['avg_rating'] + 0.015,\n",
    "        s=f\"{row['avg_rating']:.3f}\\n({row['total_ratings']:,} ratings)\",\n",
    "        ha='center',\n",
    "        va='bottom',\n",
    "        fontsize=10,\n",
    "        fontweight='bold',\n",
    "        color='green'\n",
    "    )\n",
    "\n",
    "plt.title('Top 10 Highest-Rated Movies (min 1000 ratings)', \n",
    "          fontsize=20, pad=40, fontweight='bold')\n",
    "plt.xlabel('Movie Title', fontsize=14)\n",
    "plt.ylabel('Average Rating (1–5)', fontsize=14)\n",
    "plt.ylim(4.0, 5.0)\n",
    "plt.grid(True, axis='y', linestyle='--', alpha=0.7)\n",
    "plt.xticks(rotation=50, ha='right', fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Display table\n",
    "print(\"Top 10 Movies (no merge):\")\n",
    "display(top10[['title_display', 'avg_rating', 'total_ratings', 'movie_id']])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f0548ebf42abcac1",
   "metadata": {},
   "source": [
    "plt.figure(figsize=(14, 9))\n",
    "\n",
    "top10_Reversed = top10.sort_values('avg_rating', ascending=True)\n",
    "\n",
    "bars = plt.barh(\n",
    "    y=top10_Reversed['title_display'],\n",
    "    width=top10_Reversed['avg_rating'],\n",
    "    color='#0062ff',\n",
    "    edgecolor='navy',\n",
    "    height=0.7,\n",
    "    alpha=0.95\n",
    ")\n",
    "\n",
    "# Add the exact rating + number of ratings on each bar\n",
    "for bar, row in zip(bars, top10_Reversed.itertuples()):\n",
    "    plt.text(\n",
    "        x=bar.get_width() + 0.005,                    # slightly right of bar\n",
    "        y=bar.get_y() + bar.get_height()/2,           # center vertically\n",
    "        s=f\"{row.avg_rating:.3f}  ({row.total_ratings:,} ratings)\",\n",
    "        va='center',\n",
    "        ha='left',\n",
    "        fontsize=11,\n",
    "        fontweight='bold',\n",
    "        color='darkblue'\n",
    "    )\n",
    "\n",
    "# Styling — same as your scatterplot\n",
    "plt.title('Top 10 Highest-Rated Movies (min 1000 ratings)', \n",
    "          fontsize=20, pad=40, fontweight='bold')\n",
    "plt.xlabel('Average Rating (1–5)', fontsize=14)\n",
    "plt.ylabel('Movie Title', fontsize=14)\n",
    "plt.xlim(4.0, 5.0)\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Clean y-axis labels (movie titles)\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ecb44782fb2e6450",
   "metadata": {},
   "source": [],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f67eaa88",
   "metadata": {},
   "source": [
    " ### Work in progress/Miscellaneous"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdebc4d6",
   "metadata": {},
   "source": [
    "Giacomo's Ting"
   ]
  },
  {
   "cell_type": "code",
   "id": "fdf3e068",
   "metadata": {},
   "source": [
    "tables = pd.read_sql(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\",\n",
    "    conn\n",
    "    )['name'].tolist()\n",
    "\n",
    "print(\"=== DATA DICTIONARY ===\\n\")\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"Table: {table}\")\n",
    "    print(\"-\" * (7 + len(table)))\n",
    "\n",
    "    # Get actual column info from PRAGMA but filter to nice output\n",
    "    schema = pd.read_sql(f\"PRAGMA table_info('{table}')\", conn)\n",
    "\n",
    "    # Keep only real schema fields you want (remove cid, default, pk if desired)\n",
    "    clean_schema = schema[['name', 'type', ]]\n",
    "\n",
    "    print(clean_schema.to_string(index=False))\n",
    "    print(\"\\n\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b91a5400",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e3da7",
   "metadata": {},
   "source": "K-means clustering algorithm for movies"
  },
  {
   "cell_type": "code",
   "id": "2a82b6c4",
   "metadata": {},
   "source": [
    "sync_dataframe(merged_data)\n",
    "#marco\n",
    "merged = dfs[\"merged_data\"]\n",
    "\n",
    "# feautures im basing my clusters on\n",
    "movie_features = [\n",
    "    \"movie_total_ratings\",\n",
    "    \"movie_avg_rating\",\n",
    "    \"movie_std_rating\",\n",
    "    \"movie_min_rating\",\n",
    "    \"movie_max_rating\",\n",
    "    \"unique_users\",\n",
    "    \"year_of_release\",\n",
    "]\n",
    "\n",
    "# making a table 1 row - 1 movie\n",
    "movies_for_clustering = (\n",
    "    merged\n",
    "    .groupby(\"movie_id\")[movie_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# getting all number variables except for movie_id\n",
    "X = movies_for_clustering[movie_features].astype(float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# number of clusters\n",
    "n_clusters = 4\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    random_state=42,\n",
    "    n_init=10\n",
    ")\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# adding cluster ids to the table\n",
    "movies_for_clustering[\"cluster_id\"] = cluster_labels\n",
    "\n",
    "# saving\n",
    "add_new_df('movie_clusters', 'movie_clusters', movies_for_clustering)\n",
    "\n",
    "# X_scaled из предыдущего шага (после StandardScaler)\n",
    "elbow_df = elbow_method(X_scaled, 1, 20)\n",
    "print(elbow_df)\n",
    "\n",
    "# printing how many movies in each cluster, only for debugging\n",
    "cluster_movies_amount_str = movies_for_clustering[\"cluster_id\"].value_counts().sort_index()\n",
    "print(f\"---movies in each cluster--- \\n{cluster_movies_amount_str}\")\n",
    "# Silhouette score\n",
    "print(compute_silhouette_score(X_scaled, cluster_labels))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4537ce683798714d",
   "metadata": {},
   "source": "K-means clustering algorithm for movies without the outliers (cluster 4)"
  },
  {
   "cell_type": "code",
   "id": "d32340e9be7d2ae7",
   "metadata": {},
   "source": [
    "# фильмы, которые НЕ входят в маленький кластер\n",
    "valid_movie_ids = movies_for_clustering[movies_for_clustering[\"cluster_id\"] != 3][\"movie_id\"]\n",
    "\n",
    "# новый merged, очищенный от кластера 3\n",
    "merged_clean = merged[merged[\"movie_id\"].isin(valid_movie_ids)].copy()\n",
    "\n",
    "movie_features = [\n",
    "    \"movie_total_ratings\",\n",
    "    \"movie_avg_rating\",\n",
    "    \"movie_std_rating\",\n",
    "    \"movie_min_rating\",\n",
    "    \"movie_max_rating\",\n",
    "    \"unique_users\",\n",
    "    \"year_of_release\",\n",
    "]\n",
    "\n",
    "movies_clean = (\n",
    "    merged_clean\n",
    "    .groupby(\"movie_id\")[movie_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "X_clean = movies_clean[movie_features].astype(float)\n",
    "\n",
    "scaler_clean = StandardScaler()\n",
    "X_clean_scaled = scaler_clean.fit_transform(X_clean)\n",
    "\n",
    "clean_cluster_labels = kmeans.fit_predict(X_clean_scaled)\n",
    "\n",
    "kmeans_clean = KMeans(\n",
    "    n_clusters=4,     # оставляем то же k\n",
    "    random_state=42,\n",
    "    n_init=10\n",
    ")\n",
    "\n",
    "labels_clean = kmeans_clean.fit_predict(X_clean_scaled)\n",
    "\n",
    "movies_clean[\"cluster_id\"] = labels_clean\n",
    "\n",
    "elbow_df = elbow_method(X_clean_scaled, 1, 20)\n",
    "print(elbow_df)\n",
    "\n",
    "# printing how many movies in each cluster, only for debugging\n",
    "clean_cluster_movies_amount_str = movies_clean[\"cluster_id\"].value_counts().sort_index()\n",
    "print(f\"---movies in each cluster--- \\n{clean_cluster_movies_amount_str}\")\n",
    "# Silhouette score\n",
    "print(compute_silhouette_score(X_clean_scaled, clean_cluster_labels))\n",
    "\n",
    "inspect_cluster(movies_clean, merged_clean, 3)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "21a9ce4fe9b75493",
   "metadata": {},
   "source": [
    "#r"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c10d4fcb5b326b46",
   "metadata": {},
   "source": [
    "#r"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Agglomerative clustering algorithm for movies",
   "id": "ffd7a4fd5c8f4bc0"
  },
  {
   "cell_type": "code",
   "id": "14b898e91d33f897",
   "metadata": {},
   "source": [
    "from scipy.cluster.hierarchy import fcluster\n",
    "\n",
    "sync_dataframe(merged_data)\n",
    "merged = dfs[\"merged_data\"]\n",
    "\n",
    "# Features for clustering\n",
    "movie_features = [\n",
    "    \"movie_total_ratings\",\n",
    "    \"movie_avg_rating\",\n",
    "    \"movie_std_rating\",\n",
    "    \"movie_min_rating\",\n",
    "    \"movie_max_rating\",\n",
    "    \"unique_users\",\n",
    "    \"year_of_release\",\n",
    "]\n",
    "\n",
    "# One row per movie\n",
    "movies_for_h_clustering = (\n",
    "    merged\n",
    "    .groupby(\"movie_id\")[movie_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(movies_for_h_clustering.head())\n",
    "\n",
    "# Matrix of fs\n",
    "X = movies_for_h_clustering[movie_features].astype(float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "Z = linkage(X_scaled, method=\"ward\")\n",
    "\n",
    "# Building a dendogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode=\"lastp\",\n",
    "    p=50,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=8.,\n",
    "    show_contracted=True\n",
    ")\n",
    "plt.title(\"Movie clustering dendrogram (truncated)\")\n",
    "plt.xlabel(\"Cluster index or movie groups\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "n_clusters = 4  # number of clusters\n",
    "\n",
    "h_cluster_labels = fcluster(Z, t=n_clusters, criterion=\"maxclust\")\n",
    "\n",
    "movies_for_h_clustering[\"cluster_id\"] = h_cluster_labels - 1\n",
    "\n",
    "# printing how many movies in each cluster, only for debugging\n",
    "h_cluster_movies_amount_str = movies_for_h_clustering[\"cluster_id\"].value_counts().sort_index()\n",
    "print(f\"---movies in each cluster--- \\n{h_cluster_movies_amount_str}\")\n",
    "# Silhouette score\n",
    "hier_labels = h_clustering_movies[\"cluster_id\"].to_numpy()\n",
    "print(compute_silhouette_score(X_clean_scaled, hier_labels))\n",
    "\n",
    "# saving\n",
    "add_new_df(\"movie_clusters_h\", \"movie_clusters_h\", movies_for_h_clustering)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c8785b9b87081a54",
   "metadata": {},
   "source": [
    "#m"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "76fe9959c9b4a8a1",
   "metadata": {},
   "source": [
    "K-means on users"
   ]
  },
  {
   "cell_type": "code",
   "id": "3af32cd51209096c",
   "metadata": {},
   "source": [
    "sync_dataframe(merged_data)\n",
    "data = merged_data.copy()\n",
    "\n",
    "user_features = [\n",
    "    'user_total_ratings',\n",
    "    'unique_movies',\n",
    "    'activity_days',\n",
    "    'user_avg_rating',\n",
    "    'user_std_rating',\n",
    "]\n",
    "\n",
    "# Taking first row per user\n",
    "users_for_clustering = (\n",
    "    data.groupby(\"customer_id\")[user_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "data = users_for_clustering[user_features].astype(float)\n",
    "data = data[user_features].dropna()\n",
    "\n",
    "scalar = StandardScaler()\n",
    "scaled_data = scalar.fit_transform(data)\n",
    "\n",
    "print(elbow_method(data))\n",
    "\n",
    "# k-means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto')\n",
    "cluster_labels = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "users_for_clustering[\"cluster_id\"] = cluster_labels\n",
    "print(users_for_clustering[\"cluster_id\"].value_counts().sort_index())\n",
    "\n",
    "add_new_df(\"user_clusters\", \"user_clusters\", users_for_clustering)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "125eb1d544b412c5",
   "metadata": {},
   "source": [
    "Markdown"
   ]
  },
  {
   "cell_type": "code",
   "id": "65a99c1321a12df1",
   "metadata": {},
   "source": [
    "X = user_clusters[user_features].to_numpy()\n",
    "labels = user_clusters[\"cluster_id\"].to_numpy()\n",
    "\n",
    "\n",
    "sample_size = 20000   \n",
    "idx = np.random.choice(len(X), size=sample_size, replace=False)\n",
    "\n",
    "X_sample = X[idx]\n",
    "labels_sample = labels[idx]\n",
    "\n",
    "score = silhouette_score(X_sample, labels_sample)\n",
    "\n",
    "print(\"Sampled silhouette score:\", score)\n",
    "for cid in [0, 1, 2]:\n",
    "    print(f\"\\nCLUSTER {cid}\\n\")\n",
    "\n",
    "    cluster = users_for_clustering[users_for_clustering[\"cluster_id\"] == cid]\n",
    "\n",
    "    display(cluster[user_features].describe())\n",
    "    display(cluster.head(20))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "e4cb5136ef390ae8",
   "metadata": {},
   "source": [
    "#g"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f9911c305a902027",
   "metadata": {},
   "source": [
    "#g"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
