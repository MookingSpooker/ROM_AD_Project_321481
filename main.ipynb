{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546ff8e7",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0bd42",
   "metadata": {},
   "source": [
    "### Project initialization and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be51f045",
   "metadata": {},
   "source": [
    "Importing all of the libraries that will be used. In the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a1060f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T16:16:51.249677Z",
     "start_time": "2025-12-04T16:16:51.247109Z"
    }
   },
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb93472f940c938",
   "metadata": {},
   "source": [
    "Connecting the DB to SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e0738c73228680",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T16:16:52.621431Z",
     "start_time": "2025-12-04T16:16:52.618262Z"
    }
   },
   "outputs": [],
   "source": [
    "DB_PATH = \"viewer_interactions.db\"\n",
    "\n",
    "try:\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    print(\"Connected successfully!\")\n",
    "except sqlite3.Error as e:\n",
    "    print(\"Connection failed:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb72a1f7b4374a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T16:16:54.737653Z",
     "start_time": "2025-12-04T16:16:54.734576Z"
    }
   },
   "outputs": [],
   "source": [
    "# Display options\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 120)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27aeae317f3195dc",
   "metadata": {},
   "source": [
    "Listing all the tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf5dc5f8997e3c70",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T16:16:56.257890Z",
     "start_time": "2025-12-04T16:16:56.239770Z"
    }
   },
   "outputs": [],
   "source": [
    "tables_query = \"\"\"\n",
    "               SELECT name\n",
    "               FROM sqlite_master\n",
    "               WHERE type='table'\n",
    "               ORDER BY name; \\\n",
    "               \"\"\"\n",
    "\n",
    "tables_df = pd.read_sql_query(tables_query, conn)\n",
    "print(\"Tables in the database:\")\n",
    "display(tables_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c46cee5dd503efb6",
   "metadata": {},
   "source": [
    "Creating a dictionary of type table_name -> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74de9cc40660ddfd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T16:16:58.185778Z",
     "start_time": "2025-12-04T16:16:58.160547Z"
    }
   },
   "outputs": [],
   "source": [
    "table_names = tables_df[\"name\"].tolist()\n",
    "\n",
    "schemas = {}\n",
    "\n",
    "for table in table_names:\n",
    "    pragma_query = f\"PRAGMA table_info({table});\"\n",
    "    schema_df = pd.read_sql_query(pragma_query, conn)\n",
    "    schemas[table] = schema_df\n",
    "    print(f\"\\nSchema for table '{table}':\")\n",
    "    display(schema_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6b66fb7",
   "metadata": {},
   "source": [
    "implementing dfs and importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34c75763",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T17:23:54.664725Z",
     "start_time": "2025-12-04T17:23:46.485846Z"
    }
   },
   "outputs": [],
   "source": [
    "dfs = {} # dictionary that maps the name of the table to the related data frame\n",
    "# isnt it simpler if we just use universal dataframes for each dataset instead of having to worry about dictionaries\n",
    "\n",
    "for t in table_names:\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {t};\", conn)\n",
    "    dfs[t] = df\n",
    "    print(f\"\\nLoaded table '{t}' with shape {df.shape}\")\n",
    "\n",
    "# initializing the dataframes\n",
    "movies_stats = dfs['movie_statistics']\n",
    "movies = dfs['movies']\n",
    "user_stats = dfs['user_statistics']\n",
    "viewer_ratings = dfs['viewer_ratings']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989e7b2b",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1731d00b21cbbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T17:29:38.936723Z",
     "start_time": "2025-12-04T17:29:38.922657Z"
    }
   },
   "outputs": [],
   "source": [
    "# Refreshes one or more global dataframes by passing the actual df objects.\n",
    "def sync_dataframe(*dfs_to_refresh):\n",
    "    global movies_stats, movies, user_stats, viewer_ratings\n",
    "\n",
    "    for dataframe in dfs_to_refresh:\n",
    "        if dataframe is movies_stats:\n",
    "            movies_stats = dfs[\"movie_statistics\"]\n",
    "        elif dataframe is movies:\n",
    "            movies = dfs[\"movies\"]\n",
    "        elif dataframe is user_stats:\n",
    "            user_stats = dfs[\"user_statistics\"]\n",
    "        elif dataframe is viewer_ratings:\n",
    "            viewer_ratings = dfs[\"viewer_ratings\"]\n",
    "        else:\n",
    "            raise ValueError(\"Unknown dataframe passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303d04b68150eb6",
   "metadata": {},
   "source": [
    "Searching function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d963a2b260481b4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T17:24:00.813813Z",
     "start_time": "2025-12-04T17:24:00.805561Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes the name of the table, the name of the target key, and the value as inputs. Returns singular or multiple dataframes based on the request.\n",
    "\n",
    "# can also search for null values if value is set to None\n",
    "\n",
    "def search_by_parameter(table_name, key, value):\n",
    "    df = dfs[table_name]\n",
    "\n",
    "    if value is None:\n",
    "        return df[df[key].isna()]\n",
    "\n",
    "    return df[df[key] == value]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1e8ddfc",
   "metadata": {},
   "source": [
    "### Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5d6109d206cea",
   "metadata": {},
   "source": [
    "Counting all missing values, diagnostics purposes only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def25638f8eb473b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T17:25:56.239672Z",
     "start_time": "2025-12-04T17:25:55.815932Z"
    }
   },
   "outputs": [],
   "source": [
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name} missing values (%):\")\n",
    "    missing_pct = df.isna().mean() * 100\n",
    "    display(missing_pct.to_frame(\"missing_%\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d469c6",
   "metadata": {},
   "source": [
    "### Movie Statistics Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78394717c371ac27",
   "metadata": {},
   "source": [
    "A function to calculate missing std. ratings of films"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1395d0b962d46cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T17:24:17.934719Z",
     "start_time": "2025-12-04T17:24:17.252773Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_film_std(df):\n",
    "    sync_dataframe(df)\n",
    "\n",
    "    film_stats = (\n",
    "        df.groupby('movie_id')['rating']\n",
    "        .apply(list)\n",
    "        .reset_index(name='ratings')\n",
    "    )\n",
    "\n",
    "    def manual_std(ratings):\n",
    "        ratings = np.array(ratings)\n",
    "        n = len(ratings)\n",
    "        if n <= 1:\n",
    "            return 0.0\n",
    "        mean = ratings.mean()\n",
    "        return np.sqrt(((ratings - mean) ** 2).mean())\n",
    "\n",
    "    film_stats['std_rating'] = film_stats['ratings'].apply(manual_std)\n",
    "\n",
    "    return film_stats[['movie_id', 'std_rating']]\n",
    "\n",
    "# Compute std for all films\n",
    "viewer_ratings = dfs['viewer_ratings']\n",
    "film_std = compute_film_std(viewer_ratings)\n",
    "\n",
    "# Load movies_statistics\n",
    "movies_stats = dfs[\"movie_statistics\"]\n",
    "\n",
    "# Compute the old percentage before merging\n",
    "old_null_pct = dfs[\"movie_statistics\"][\"std_rating\"].isna().mean() * 100\n",
    "\n",
    "# Merge new std values\n",
    "movies_stats = movies_stats.merge(\n",
    "    film_std,\n",
    "    on=\"movie_id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_new\")\n",
    ")\n",
    "\n",
    "# Replace old std_rating with the new one\n",
    "movies_stats[\"std_rating\"] = movies_stats[\"std_rating_new\"]\n",
    "movies_stats.drop(columns=[\"std_rating_new\"], inplace=True)\n",
    "\n",
    "# Save updated table\n",
    "dfs[\"movie_statistics\"] = movies_stats\n",
    "\n",
    "# Compute new percentage ---\n",
    "new_null_pct = movies_stats[\"std_rating\"].isna().mean() * 100\n",
    "\n",
    "# absolute improvement (percentage points)\n",
    "improvement_abs = old_null_pct - new_null_pct\n",
    "\n",
    "# relative improvement (how many percent of the original NaNs we removed)\n",
    "improvement_rel = (improvement_abs / old_null_pct) * 100 if old_null_pct > 0 else 0\n",
    "\n",
    "print(f\"Missing values reduced from {old_null_pct:.2f}% to {new_null_pct:.2f}%.\")\n",
    "print(f\"Absolute improvement: {improvement_abs:.2f}%\")\n",
    "print(f\"Relative improvement: {improvement_rel:.2f}% better than before.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541a627d968fd125",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T16:18:05.889043Z",
     "start_time": "2025-12-04T16:18:05.883227Z"
    }
   },
   "outputs": [],
   "source": [
    "#Figure out how to drop na values in general\n",
    "movies_stats = dfs['movie_statistics']\n",
    "\n",
    "print(f\"Before cleaning: {len(movies_stats)} movies\")\n",
    "movies_stats = movies_stats.dropna(subset=['std_rating'])\n",
    "dfs['movie_statistics'] = movies_stats\n",
    "print(f\"After removing single-rating movies: {len(movies_stats)} movies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4518d",
   "metadata": {},
   "source": [
    "Calculating the missing total_ratings of movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6c1bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T17:24:24.411375Z",
     "start_time": "2025-12-04T17:24:22.554317Z"
    }
   },
   "outputs": [],
   "source": [
    "sync_dataframe(movies_stats)\n",
    "\n",
    "# Collecting all the movies with absent total_rating in a dictionary\n",
    "missing_dict = {}\n",
    "\n",
    "missing = search_by_parameter('movie_statistics', 'total_ratings', None)\n",
    "missing_dict = {row.movie_id: 0 for row in missing.itertuples(index=False)}\n",
    "\n",
    "# Iterating through viewer_ratings and manually counting the ratings for each film\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "    if movie_id in missing_dict:\n",
    "        missing_dict[movie_id] += 1\n",
    "\n",
    "# Update movie_stats\n",
    "for row in movies_stats.itertuples(index=True):\n",
    "    if row.movie_id in missing_dict:\n",
    "        movies_stats.at[row.Index, \"total_ratings\"] = missing_dict[row.movie_id]\n",
    "\n",
    "dfs[\"movie_statistics\"] = movies_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9afb8",
   "metadata": {},
   "source": [
    "Calculating Missing Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d4756a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T17:30:00.747234Z",
     "start_time": "2025-12-04T17:29:58.724298Z"
    }
   },
   "outputs": [],
   "source": [
    "sync_dataframe(movies_stats, viewer_ratings)\n",
    "\n",
    "# Finding movies with null avg_rating\n",
    "missing_avg = search_by_parameter('movie_statistics', 'avg_rating', None)\n",
    "\n",
    "# Creating a dict of type { movie_id : avg_rating }\n",
    "# set 0 as base value for now, might change it later\n",
    "missing_avg_dict = {row.movie_id: 0 for row in missing_avg.itertuples(index=False)}\n",
    "\n",
    "# Storing the sum of all ratings for each movie\n",
    "rating_sums = {movie_id: 0 for movie_id in missing_avg_dict}\n",
    "\n",
    "# Iterating through viewer ratings and adding to sum if movie_id matches\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if movie_id in missing_avg_dict:\n",
    "        rating_sums[movie_id] += rating\n",
    "\n",
    "for row in movies_stats.itertuples():\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    if movie_id in rating_sums:\n",
    "        total = row.total_ratings # I'm assuming that my calculations of total_ratings per movie is correct ang i got rid of                            all null values\n",
    "\n",
    "        # IF FORE SOME MAGICAL REASON THERE IS STILL A NULL THEN IGNORE\n",
    "        if pd.isna(total) or total == 0:\n",
    "            avg = 0\n",
    "        else:\n",
    "            avg = rating_sums[movie_id] / total\n",
    "\n",
    "        movies_stats.at[row.Index, \"avg_rating\"] = avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21090df8",
   "metadata": {},
   "source": [
    "Calculating the missing min and max ratings for movies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9bb36c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T17:24:45.780163Z",
     "start_time": "2025-12-04T17:24:45.519609Z"
    }
   },
   "outputs": [],
   "source": [
    "sync_dataframe(movies_stats)\n",
    "\n",
    "# Find movies with missing min_rating and max_rating using your function\n",
    "missing_min = search_by_parameter('movie_statistics', 'min_rating', None)\n",
    "missing_max = search_by_parameter('movie_statistics', 'max_rating', None)\n",
    "\n",
    "# Combine them as some movies may be in both\n",
    "missing_ids = set(missing_min[\"movie_id\"]) | set(missing_max[\"movie_id\"])\n",
    "\n",
    "# Take only ratings for the movies we care\n",
    "relevant_ratings = viewer_ratings[viewer_ratings[\"movie_id\"].isin(missing_ids)]\n",
    "\n",
    "# Building a nested dict {movie_id : {\"min\": ..., \"max\": ...}}\n",
    "min_max_dict = {}\n",
    "\n",
    "for row in relevant_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if movie_id not in min_max_dict:\n",
    "        min_max_dict[movie_id] = {\"min\": rating, \"max\": rating}\n",
    "    else:\n",
    "        if rating < min_max_dict[movie_id][\"min\"]:\n",
    "            min_max_dict[movie_id][\"min\"] = rating\n",
    "        if rating > min_max_dict[movie_id][\"max\"]:\n",
    "            min_max_dict[movie_id][\"max\"] = rating\n",
    "\n",
    "# Update movie_statistics\n",
    "for row in movies_stats.itertuples(index=True):\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    if movie_id in min_max_dict:\n",
    "        if pd.isna(row.min_rating):\n",
    "            movies_stats.at[row.Index, \"min_rating\"] = min_max_dict[movie_id][\"min\"]\n",
    "        if pd.isna(row.max_rating):\n",
    "            movies_stats.at[row.Index, \"max_rating\"] = min_max_dict[movie_id][\"max\"]\n",
    "\n",
    "dfs[\"movie_statistics\"] = movies_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83870a25990a35e",
   "metadata": {},
   "source": [
    "Finding missing unique users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9dd911b77bbd1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T17:25:01.401056Z",
     "start_time": "2025-12-04T17:24:59.401268Z"
    }
   },
   "outputs": [],
   "source": [
    "sync_dataframe(movies_stats)\n",
    "\n",
    "missing_unique = search_by_parameter('movie_statistics', 'unique_users', None)\n",
    "\n",
    "# creating my favorite movie set\n",
    "missing_movie_ids = {row.movie_id for row in missing_unique.itertuples(index=False)}\n",
    "\n",
    "# creating a dict movie_id: customer_id\n",
    "unique_users_dict = {movie_id: set() for movie_id in missing_movie_ids}\n",
    "\n",
    "# Gathering unique users\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    # as always im getting only those movies which have null for unique users\n",
    "    if movie_id in unique_users_dict:\n",
    "        unique_users_dict[movie_id].add(row.customer_id)\n",
    "\n",
    "# Counting unique users\n",
    "updated = 0\n",
    "for movie_id, users in unique_users_dict.items():\n",
    "    count = len(users)  # unique users count\n",
    "\n",
    "    movies_stats.loc[\n",
    "        movies_stats[\"movie_id\"] == movie_id,\n",
    "        \"unique_users\"\n",
    "    ] = count\n",
    "\n",
    "    updated += 1\n",
    "\n",
    "# just in case if a movie has 0 ratings im setting unique users to 0\n",
    "movies_stats[\"unique_users\"] = movies_stats[\"unique_users\"].fillna(0).astype(int)\n",
    "\n",
    "# Updating\n",
    "dfs[\"movie_statistics\"] = movies_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ee021deca5034ee",
   "metadata": {},
   "source": [
    "Checking for any duplicate movie ids and removing them from all datasets to clean the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8681d7bb16a5d50",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f9981c970eddd04",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18107ca34206a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4807162c092ff91",
   "metadata": {},
   "source": [
    "### User_statistics calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9820aaa2583197",
   "metadata": {},
   "outputs": [],
   "source": [
    "#std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e6f6477f9abcc7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4641d086358af2ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12e52c5",
   "metadata": {},
   "source": [
    "### Merging datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c36b6",
   "metadata": {},
   "source": [
    "Merging the Movies and movie statis filling in missing values on either dataset and converting all of the dates to type DateTime as well as all counts and years to integers.\n",
    "This is in order to clean our movie data before merging it with our user data to fill in any recoverable missing values.\n",
    "Rows with missing values will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46319c89",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T11:29:27.267775Z",
     "start_time": "2025-12-03T11:29:26.861951Z"
    }
   },
   "outputs": [],
   "source": [
    "movies = dfs['movies'].copy() # does this overwrite the movies we have right now? maybe call it something else.\n",
    "movie_stats = dfs['movie_statistics'].copy()\n",
    "viewer_ratings = dfs['viewer_ratings'].copy()\n",
    "\n",
    "bad_ids_movies = movies.loc[movies['movie_id'].duplicated(keep=False), 'movie_id'].unique().tolist()\n",
    "bad_ids_stats = movie_stats.loc[movie_stats['movie_id'].duplicated(keep=False), 'movie_id'].unique().tolist()\n",
    "bad_ids = list(set(bad_ids_movies + bad_ids_stats))\n",
    "print('bad movie ids to remove: ', bad_ids)\n",
    "\n",
    "viewer_ratings = viewer_ratings[~viewer_ratings['movie_id'].isin(bad_ids)]\n",
    "movies = movies[~movies['movie_id'].isin(bad_ids)]\n",
    "movie_stats = movie_stats[~movie_stats['movie_id'].isin(bad_ids)]\n",
    "\n",
    "movies['year_of_release'] = (pd.to_numeric(movies['year_of_release'], errors='coerce').astype('Int64'))\n",
    "movie_stats['total_ratings'] = (pd.to_numeric(movie_stats['total_ratings'], errors='coerce').astype('Int64'))\n",
    "movie_stats['unique_users'] = (pd.to_numeric(movie_stats['unique_users'], errors='coerce').astype('Int64'))\n",
    "movie_stats['year_of_release'] = (pd.to_numeric(movie_stats['year_of_release'], errors='coerce').astype('Int64'))\n",
    "\n",
    "movie_stats['first_rating_date'] = pd.to_datetime(movie_stats['first_rating_date'], errors='coerce')\n",
    "movie_stats['last_rating_date'] = pd.to_datetime(movie_stats['last_rating_date'], errors='coerce')\n",
    "\n",
    "# this will automatically remove rows with missing data\n",
    "movie_full = movies.merge(movie_stats, on='movie_id', how='inner', suffixes=('_movies', '_stats'))\n",
    "\n",
    "movie_full['title'] = movie_full['title_movies'].combine_first(movie_full['title_stats'])\n",
    "movie_full['year_of_release'] = movie_full['year_of_release_movies'].combine_first(movie_full['year_of_release_stats'])\n",
    "\n",
    "movie_full = movie_full.drop(columns=['title_movies', 'title_stats', 'year_of_release_movies', 'year_of_release_stats'])\n",
    "\n",
    "dfs['movies'] = movies\n",
    "dfs['movie_statistics'] = movie_stats\n",
    "dfs['viewer_ratings'] = viewer_ratings\n",
    "dfs['movie_full'] = movie_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1d3b2a1691c50e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T11:29:30.485027Z",
     "start_time": "2025-12-03T11:29:30.474103Z"
    }
   },
   "outputs": [],
   "source": [
    "print(\"movies dtypes:\")\n",
    "print(movies.dtypes)\n",
    "\n",
    "print(\"\\nmovie_stats dtypes:\")\n",
    "print(movie_stats.dtypes)\n",
    "\n",
    "print(\"\\nmovie_full dtypes:\")\n",
    "print(movie_full.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bec12db",
   "metadata": {},
   "source": [
    "- Converts the date parameter in viewer_ratings to datetime.\n",
    "- Merges viewer_ratings, movies, movie_statistics and user_statistics into one dataset as merged_data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc40bfb82470b10a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-03T11:29:40.514810Z",
     "start_time": "2025-12-03T11:29:36.537390Z"
    }
   },
   "outputs": [],
   "source": [
    "user_stats = dfs['user_statistics'].copy()\n",
    "viewer_ratings = dfs['viewer_ratings'].copy()\n",
    "movie_full = dfs['movie_full'].copy()\n",
    "\n",
    "viewer_ratings['date'] = pd.to_datetime(viewer_ratings['date'], errors = 'coerce')\n",
    "user_stats['first_rating_date'] = pd.to_datetime(user_stats['first_rating_date'], errors = 'coerce')\n",
    "user_stats['last_rating_date'] = pd.to_datetime(user_stats['last_rating_date'], errors = 'coerce')\n",
    "\n",
    "user_stats['total_ratings'] = (pd.to_numeric(user_stats['total_ratings'], errors='coerce').astype('Int64'))\n",
    "user_stats['unique_movies'] = (pd.to_numeric(user_stats['unique_movies'], errors='coerce').astype('Int64'))\n",
    "user_stats['activity_days'] = (pd.to_numeric(user_stats['activity_days'], errors='coerce').astype('Int64'))\n",
    "\n",
    "\n",
    "# create user and movie specific columns\n",
    "movie_full = movie_full.rename(columns={\n",
    "    'total_ratings':     'movie_total_ratings',\n",
    "    'avg_rating':        'movie_avg_rating',\n",
    "    'std_rating':        'movie_std_rating',\n",
    "    'min_rating':        'movie_min_rating',\n",
    "    'max_rating':        'movie_max_rating',\n",
    "    'first_rating_date': 'movie_first_rating_date',\n",
    "    'last_rating_date':  'movie_last_rating_date'\n",
    "})\n",
    "\n",
    "\n",
    "user_stats = user_stats.rename(columns={\n",
    "    'total_ratings':     'user_total_ratings',\n",
    "    'avg_rating':        'user_avg_rating',\n",
    "    'std_rating':        'user_std_rating',\n",
    "    'min_rating':        'user_min_rating',\n",
    "    'max_rating':        'user_max_rating',\n",
    "    'first_rating_date': 'user_first_rating_date',\n",
    "    'last_rating_date':  'user_last_rating_date'\n",
    "})\n",
    "\n",
    "# drop anomalous date for user stats\n",
    "if 'anomalous_date' in viewer_ratings.columns:\n",
    "    viewer_ratings = viewer_ratings.drop(columns=['anomalous_date'])\n",
    "\n",
    "viewer_ratings.dtypes\n",
    "\n",
    "merged_data = viewer_ratings.merge(movies, on = 'movie_id', how = 'left')\n",
    "merged_data = merged_data.merge(user_stats, on = 'customer_id', how = 'left')\n",
    "merged_data = merged_data.merge(movie_stats, on = 'movie_id', how = 'left')\n",
    "# avg rating standard rating mean rating\n",
    "'''\n",
    "want to merge\n",
    "- title\n",
    "- year of release\n",
    "want to keep independent\n",
    "- avg rating\n",
    "- std rating\n",
    "- min rating\n",
    "- max rating\n",
    "- first rating date\n",
    "- last rating date\n",
    "'''\n",
    "\n",
    "print(\"Total columns:\", len(merged_data.columns))\n",
    "list(merged_data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7908f360",
   "metadata": {},
   "source": [
    "Giacomo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ac23fc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c5bba06e",
   "metadata": {},
   "source": [
    "### Plotting Statistics and Overall findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d80a8",
   "metadata": {},
   "source": [
    "Ryder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2df1e09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Markdown cell above\n",
    "\"\"\"\n",
    "### Global Rating Distribution\n",
    "\n",
    "There is a clear over positive bias. This can be due to the fact that this was done on a streaming platform where users are more likely to rate higher\n",
    "\"\"\"\n",
    "\n",
    "plt.figure(figsize=(11, 6)) # made it this size for a better fit in the read me\n",
    "ax = sns.histplot(\n",
    "    data=dfs['viewer_ratings'],\n",
    "    x='rating',\n",
    "    bins=5,\n",
    "    discrete=True, #tells seaborn to treat x as integer values\n",
    "    color=\"#0062ff\",\n",
    "    edgecolor='white',\n",
    "    alpha=0.85,  #transparency\n",
    "    linewidth=1.5\n",
    ")\n",
    "\n",
    "# Add exact counts on top of each bar\n",
    "for rect in ax.patches:\n",
    "    height = rect.get_height()\n",
    "    ax.text(\n",
    "        rect.get_x() + rect.get_width()/2., \n",
    "        height + 200_000,                    # a bit above the bar\n",
    "        f'{int(height):,}',                  # adds commas: 12,345,678\n",
    "        ha='center', va='bottom', fontsize=12, fontweight='bold'\n",
    "    )\n",
    "\n",
    "# Clean y-axis\n",
    "plt.ylabel('Number of Ratings (millions)', fontsize=12)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M'))\n",
    "\n",
    "plt.title('Distribution of Viewer Ratings\\n(Strong Positive Bias)', \n",
    "          fontsize=15, pad=20)\n",
    "plt.xlabel('Rating (1–5)', fontsize=12)\n",
    "plt.xticks(range(0, 7))\n",
    "plt.ylim(0, 2_000_000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c540e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "### User Average Rating vs. Rating Variability\n",
    "\n",
    "Scatter from user_statistics: High avg with low std = consistent positive raters. \n",
    "Valuable for grouping users (e.g., strict critics vs. easy fans). Alpha for density.\n",
    "\"\"\"\n",
    "\n",
    "sync_dataframe(user_stats)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(data=user_stats, x='avg_rating', y='std_rating', alpha=0.4, color='#0062ff', edgecolor=None)\n",
    "plt.title('User Average Rating vs. Standard Deviation\\n(Rating Consistency Patterns)', fontsize=15, pad=20)\n",
    "plt.xlabel('Average Rating Given (1–5)', fontsize=12)\n",
    "plt.ylabel('Std Deviation of Ratings', fontsize=12)\n",
    "plt.xlim(0, 5)\n",
    "plt.ylim(0, 4)  # Std typically low due to bias\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a292f8066fef5d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PERFECT PENTAGON — FINAL WORKING VERSION (Dec 2025)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.path import Path\n",
    "from matplotlib.patches import PathPatch\n",
    "\n",
    "# CHANGE ONLY THESE TWO VALUES\n",
    "avg_rating_value = user_stats['avg_rating'].mean()   # ← your real global avg from user_statistics\n",
    "std_rating_value = user_stats['std_rating'].mean()   # ← your real global std from user_statistics\n",
    "\n",
    "# Convert to 0–1 scale\n",
    "pull_to_five = (avg_rating_value - 1.0) / 4.0\n",
    "consistency  = np.clip(1.0 - (std_rating_value / 1.8), 0.1, 1.0)\n",
    "\n",
    "# Pentagon vertices (5 corners + first repeated to close)\n",
    "center = np.array([0.5, 0.5])\n",
    "radius = 0.45\n",
    "angles = np.linspace(0, 2*np.pi, 5, endpoint=False)\n",
    "verts = center + radius * np.column_stack([np.cos(angles), np.sin(angles)])\n",
    "verts = np.vstack([verts, verts[0]])  # close the loop → shape (6, 2)\n",
    "\n",
    "# Blue pentagon (pulled toward Rating 5 = bottom-right)\n",
    "blue_scale = 0.85 * pull_to_five\n",
    "blue_verts = center + blue_scale * (verts - center)\n",
    "\n",
    "# Red pentagon (size = consistency)\n",
    "red_scale = 0.92 * consistency\n",
    "red_verts = center + red_scale * (verts - center)\n",
    "\n",
    "# Correct Path codes: 6 vertices → 6 codes\n",
    "codes = [Path.MOVETO] + [Path.LINETO]*4 + [Path.CLOSEPOLY]\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "\n",
    "# Outer frame\n",
    "frame_path = Path(verts, codes)\n",
    "ax.add_patch(PathPatch(frame_path, facecolor='none', edgecolor='black', linewidth=3))\n",
    "\n",
    "# Red pentagon (consistency)\n",
    "red_path = Path(red_verts, codes)\n",
    "ax.add_patch(PathPatch(red_path, facecolor=\"#000000\", alpha=0.7, edgecolor='#aa0000', linewidth=4))\n",
    "\n",
    "# Blue pentagon (average rating)\n",
    "blue_path = Path(blue_verts, codes)\n",
    "ax.add_patch(PathPatch(blue_path, facecolor=\"#00ffdd\", alpha=0.75, edgecolor='#003380', linewidth=4))\n",
    "\n",
    "# Corner labels\n",
    "labels = ['1', '2', '3', '4', '5']\n",
    "for (x, y), label in zip(verts[:-1], labels):\n",
    "    ax.text(x, y, label, fontsize=16, fontweight='bold', ha='center', va='center',\n",
    "            bbox=dict(boxstyle=\"round,pad=0.5\", facecolor=\"white\", alpha=0.95))\n",
    "\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_aspect('equal')\n",
    "ax.axis('off')\n",
    "\n",
    "ax.set_title(f\"Global User Rating Personality Pentagon\\n\"\n",
    "             f\"Avg Rating = {avg_rating_value:.2f} | Std Dev = {std_rating_value:.2f}\",\n",
    "             fontsize=20, pad=40, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15d29082834d37d",
   "metadata": {},
   "outputs": [],
   "source": [
    "'Plotting a timeline of ratings over time'\n",
    "\n",
    "sync_dataframe(movies_stats)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(data=movies_stats, x='year_of_release', y='avg_rating', alpha=0.4, color='#0062ff', edgecolor=None, s=35)\n",
    "plt.title('Movie Rating VS. Year of Release\\n(Trends Over Time)', fontsize=15, pad=20)\n",
    "plt.xlabel('Year of Release', fontsize=12)\n",
    "plt.ylabel('Average Rating Given (1–5)', fontsize=12)\n",
    "plt.xlim(1890, 2010) #this included the data for all the movies in the data set as it goes from 1896 to 2005\n",
    "plt.ylim(0, 7)  \n",
    "#plt.scatter(x,y, s)\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67eaa88",
   "metadata": {},
   "source": [
    "### Work in progress/Miscellaneous "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdebc4d6",
   "metadata": {},
   "source": [
    "Giacomo's Ting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf3e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = pd.read_sql(\n",
    "    \"SELECT name FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%';\",\n",
    "    conn\n",
    "    )['name'].tolist()\n",
    "\n",
    "print(\"=== DATA DICTIONARY ===\\n\")\n",
    "\n",
    "for table in tables:\n",
    "    print(f\"Table: {table}\")\n",
    "    print(\"-\" * (7 + len(table)))\n",
    "\n",
    "    # Get actual column info from PRAGMA but filter to nice output\n",
    "    schema = pd.read_sql(f\"PRAGMA table_info('{table}')\", conn)\n",
    "\n",
    "    # Keep only real schema fields you want (remove cid, default, pk if desired)\n",
    "    clean_schema = schema[['name', 'type', ]]\n",
    "\n",
    "    print(clean_schema.to_string(index=False))\n",
    "    print(\"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
