{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "546ff8e7",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0bd42",
   "metadata": {},
   "source": [
    "### Project initialization and setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be51f045",
   "metadata": {},
   "source": [
    "Importing all of the libraries that will be used. In the project."
   ]
  },
  {
   "cell_type": "code",
   "id": "f3a1060f",
   "metadata": {},
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.stats as stats\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.mixture import GaussianMixture\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "3bb93472f940c938",
   "metadata": {},
   "source": [
    "Connecting the DB to SQL"
   ]
  },
  {
   "cell_type": "code",
   "id": "58e0738c73228680",
   "metadata": {},
   "source": [
    "DB_PATH = \"viewer_interactions.db\"\n",
    "\n",
    "try:\n",
    "    conn = sqlite3.connect(DB_PATH)\n",
    "    print(\"Connected successfully!\")\n",
    "except sqlite3.Error as e:\n",
    "    print(\"Connection failed:\", e)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "dcb72a1f7b4374a3",
   "metadata": {},
   "source": [
    "# Display options\n",
    "pd.set_option(\"display.max_columns\", 100)\n",
    "pd.set_option(\"display.width\", 120)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "27aeae317f3195dc",
   "metadata": {},
   "source": [
    "Listing all the tables"
   ]
  },
  {
   "cell_type": "code",
   "id": "cf5dc5f8997e3c70",
   "metadata": {},
   "source": [
    "tables_query = \"\"\"\n",
    "               SELECT name\n",
    "               FROM sqlite_master\n",
    "               WHERE type='table'\n",
    "               ORDER BY name; \\\n",
    "               \"\"\"\n",
    "\n",
    "tables_df = pd.read_sql_query(tables_query, conn)\n",
    "print(\"Tables in the database:\")\n",
    "display(tables_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c46cee5dd503efb6",
   "metadata": {},
   "source": [
    "Creating a dictionary of type table_name -> DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "id": "74de9cc40660ddfd",
   "metadata": {},
   "source": [
    "table_names = tables_df[\"name\"].tolist()\n",
    "\n",
    "schemas = {}\n",
    "\n",
    "for table in table_names:\n",
    "    pragma_query = f\"PRAGMA table_info({table});\"\n",
    "    schema_df = pd.read_sql_query(pragma_query, conn)\n",
    "    schemas[table] = schema_df\n",
    "    print(f\"\\nSchema for table '{table}':\")\n",
    "    display(schema_df)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "eb246e3a2c2abbb7",
   "metadata": {},
   "source": [
    "DF_MAP is the lookup table that links each global DataFrame name (for instance movies_stats) to its actual entry inside the dfs dictionary. It is filled by add_new_df function whenever a new dataframe is registered.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "40468af8",
   "metadata": {},
   "source": [
    "# creating a dictionary to map dataframes\n",
    "DF_MAP = {}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e3a9b52d9d7cfc5d",
   "metadata": {},
   "source": [
    "### Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed380c3f39390a13",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Add new data frame function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65560f7e",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Registers a dataframe under a global variable name while keeping a main dfs dictionary as the source.\n",
    "\n",
    "**Inputs:**<br>\n",
    "Takes global_name (new global variable name), dfs_key (dictionary key in dfs), and optional dataframe (only used if dfs_key is not already present) as inputs.\n",
    "\n",
    "**Behaviour:**<br>\n",
    "If dfs_key already exists in dfs, it simply maps that existing dataframe to the requested global name and records the mapping in DF_MAP. Otherwise, it raises an error when no dataframe is provided, or it stores the new dataframe in both dfs and the global namespace, updating DF_MAP.\n",
    "\n",
    "**Output:**<br>\n",
    "Doesn't return anything, prints whether an existing or new dataframe was mapped.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "8701b129",
   "metadata": {},
   "source": [
    "def add_new_df(global_name: str, dfs_key: str, dataframe=None):\n",
    "\n",
    "    # if dataframe already exists in dfs then ignore \"dataframe\" input\n",
    "    if dfs_key in dfs:\n",
    "        globals()[global_name] = dfs[dfs_key]\n",
    "        DF_MAP[global_name] = dfs_key\n",
    "        print(f\"Mapped existing dfs['{dfs_key}'] to global '{global_name}'\")\n",
    "        return\n",
    "\n",
    "    # new dataframe\n",
    "    if dataframe is None:\n",
    "        raise ValueError(\n",
    "            f\"dfs_key '{dfs_key}' does not exist in dfs and no dataframe was provided.\"\n",
    "        )\n",
    "\n",
    "    dfs[dfs_key] = dataframe\n",
    "    globals()[global_name] = dataframe\n",
    "    DF_MAP[global_name] = dfs_key\n",
    "\n",
    "    print(f\"Added NEW df: global '{global_name}' -> dfs['{dfs_key}']\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "52e00e810b2aa439",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Sync data frame function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ae02fc",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Refreshing the global dataframe variables so they always match the newest versions stored inside the dfs dictionary.\n",
    "\n",
    "**Inputs:**<br>\n",
    "One or more dataframe objects that currently exist as global variables.\n",
    "\n",
    "**Behavior:**<br>\n",
    "For each passed dataframe, it looks up the corresponding DF_MAP entry and replaces the global variable with the latest version from dfs. If a dataframe isnâ€™t found in DF_MAP, it raises an error.\n",
    "\n",
    "**Output:**<br>\n",
    "Silently updates globals or raises on unknown inputs."
   ]
  },
  {
   "cell_type": "code",
   "id": "f1731d00b21cbbf",
   "metadata": {},
   "source": [
    "def sync_dataframe(*dfs_to_refresh):\n",
    "    globals_dict = globals()\n",
    "\n",
    "    for df_obj in dfs_to_refresh:\n",
    "        updated = False\n",
    "\n",
    "        for global_name, dfs_key in DF_MAP.items():\n",
    "            if df_obj is globals_dict[global_name]:\n",
    "                globals_dict[global_name] = dfs[dfs_key]\n",
    "                updated = True\n",
    "                break\n",
    "\n",
    "        if not updated:\n",
    "            raise ValueError(\"Unknown dataframe passed!\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4705ab12e35df441",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Search by parameter function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1303d04b68150eb6",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Searches for required parameters in dataframes\n",
    "\n",
    "**Inputs:**<br>\n",
    "table_name (dataframe key in dfs), key (column name), and value (target value or None to locate nulls).\n",
    "\n",
    "**Behavior:**<br>\n",
    "Retrieves the dataframe from dfs, then returns rows where the column matches value. If value is None, it returns rows with nulls in that column.\n",
    "\n",
    "**Output:** <br>\n",
    "Returns a filtered dataframe matching the search condition."
   ]
  },
  {
   "cell_type": "code",
   "id": "5d963a2b260481b4",
   "metadata": {},
   "source": [
    "def search_by_parameter(table_name, key, value):\n",
    "    df = dfs[table_name]\n",
    "\n",
    "    if value is None:\n",
    "        return df[df[key].isna()]\n",
    "\n",
    "    return df[df[key] == value]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "73af4bb57e56160a",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Manual std values calculation function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be82224302a0ef8",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Computes the standard deviation, see the used formula in README\n",
    "\n",
    "**Inputs:**<br>\n",
    "Values iterable convertible to a NumPy array.\n",
    "\n",
    "**Behavior:**<br>\n",
    " Converts to float array, returns 0.0 for length is lees or equal to 1, otherwise calculates mean and square-root of mean squared deviation.\n",
    "\n",
    "**Output:**<br>\n",
    "A float representing the standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "id": "5b55fbbe328695f4",
   "metadata": {},
   "source": [
    "# make sure no nulls are included in the calculation!\n",
    "\n",
    "def manual_std(values):\n",
    "    arr = np.array(values, dtype=float)\n",
    "\n",
    "    n = len(arr)\n",
    "\n",
    "    if n <= 1:\n",
    "        return 0.0\n",
    "\n",
    "    mean = arr.mean()\n",
    "    return np.sqrt(((arr - mean) ** 2).mean())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Merging function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ],
   "id": "f51fb33669250918"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Purpose:**<br>\n",
    "\n",
    "**Inputs:**<br>\n",
    "\n",
    "**Behavior:**<br>\n",
    "\n",
    "**Output:**<br>"
   ],
   "id": "6740aea3f59de058"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def merge_and_replace_column(base_df,update_df,id_col: str,value_col: str):\n",
    "\n",
    "    merged = base_df.merge(\n",
    "        update_df[[id_col, value_col]],\n",
    "        on=id_col,\n",
    "        how=\"left\",\n",
    "        suffixes=(\"\", \"_new\")\n",
    "    )\n",
    "\n",
    "    new_col = f\"{value_col}_new\"\n",
    "    merged[value_col] = merged[new_col]\n",
    "    merged.drop(columns=[new_col], inplace=True)\n",
    "\n",
    "    return merged"
   ],
   "id": "3e5096c7ec67b8f8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Calcukate missing values % function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ],
   "id": "f919956fa73898f8"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "**Purpose:**<br>\n",
    "\n",
    "**Inputs:**<br>\n",
    "\n",
    "**Behavior:**<br>\n",
    "\n",
    "**Output:**<br>"
   ],
   "id": "b7692697c49b69f0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def calculate_missing_values_improvement(old_df, new_df,col_name: str):\n",
    "\n",
    "    old_null_pct = old_df[col_name].isna().mean() * 100\n",
    "    new_null_pct = new_df[col_name].isna().mean() * 100\n",
    "\n",
    "    improvement_abs = old_null_pct - new_null_pct\n",
    "    improvement_rel = (improvement_abs / old_null_pct * 100) if old_null_pct > 0 else 0\n",
    "\n",
    "    print(f\"Missing reduced: {old_null_pct:.2f}% -> {new_null_pct:.2f}%.\")\n",
    "    print(f\"Absolute improvement: {improvement_abs:.2f}%.\")\n",
    "    print(f\"Relative improvement: {improvement_rel:.2f}% better.\\n\")"
   ],
   "id": "125f2c5752e7df64",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "80b63fe0fc34e27c",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Safe string to datetime convertion function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8516a3c40143e36",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Parsing a \"YYYY-MM-DD\" formatted string into a datetime.\n",
    "\n",
    "**Inputs:**<br>\n",
    "String in a \"YYYY-MM-DD\" format.\n",
    "\n",
    "**Behavior:**<br>\n",
    "Converting string to a datetime.strptime type or None for null/NaN\n",
    "\n",
    "**Output:**<br>\n",
    "Returns a datetime object or None when parsing fails or input is missing."
   ]
  },
  {
   "cell_type": "code",
   "id": "c3e541a4",
   "metadata": {},
   "source": [
    "def covert_string_to_date(date_str):\n",
    "    if date_str is None or pd.isna(date_str):\n",
    "        return None\n",
    "    try:\n",
    "        return datetime.strptime(date_str, \"%Y-%m-%d\")\n",
    "    except:\n",
    "        return None"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "394dcd26350a204a",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Elbow method</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d859cdffca32e6f7",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Evaluates KMeans inertia across a range of cluster counts to find an elbow point.\n",
    "\n",
    "**Inputs:**<br>\n",
    "Feature matrix X, cluster bounds k_min/k_max, random_state, and a \"plot\" bool to indicate necessity of plotting.\n",
    "\n",
    "**Behavior:**<br>\n",
    "Fits KMeans for each k, collects inertia values into a dataframe, and plots inertia vs k.\n",
    "\n",
    "**Output:**<br>\n",
    "A dataframe with columns k and inertia, optionally draws a plot for visual elbow inspection"
   ]
  },
  {
   "cell_type": "code",
   "id": "6d76218baeef6680",
   "metadata": {},
   "source": [
    "def elbow_method(X, k_min=1, k_max=10, random_state=42, plot=True):\n",
    "    results = []\n",
    "\n",
    "    for k in range(k_min, k_max + 1):\n",
    "        kmeans = KMeans(\n",
    "            n_clusters=k,\n",
    "            random_state=random_state,\n",
    "            n_init=10\n",
    "        )\n",
    "        kmeans.fit(X)\n",
    "        results.append({\"k\": k, \"inertia\": kmeans.inertia_})\n",
    "\n",
    "    elbow_df = pd.DataFrame(results)\n",
    "\n",
    "    if plot:\n",
    "        plt.figure()\n",
    "        plt.plot(elbow_df[\"k\"], elbow_df[\"inertia\"], marker=\"o\")\n",
    "        plt.xlabel(\"Number of clusters (k)\")\n",
    "        plt.ylabel(\"Inertia\")\n",
    "        plt.title(\"Elbow method\")\n",
    "        plt.xticks(elbow_df[\"k\"])\n",
    "        plt.show()\n",
    "\n",
    "    return elbow_df"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "40ede6253c9f2ddd",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Silhouette score computation function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29b4363a923af0be",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Calculates silhouette score for clustering results\n",
    "\n",
    "**Inputs:**<br>\n",
    "Feature matrix X, cluster label array labels, optional sample_size, random_state, and distance metric.\n",
    "\n",
    "**Behavior:**<br>\n",
    "Validates matching lengths, then computes silhouette_score with the chosen metric.\n",
    "\n",
    "**Output:**<br>\n",
    "Returns a single float silhouette score summarizing cluster separation and cohesion."
   ]
  },
  {
   "cell_type": "code",
   "id": "6c10d4ecd680e8df",
   "metadata": {},
   "source": [
    "def compute_silhouette_score(X, labels, sample_size=None, random_state=42, metric=\"euclidean\"):\n",
    "    X = np.asarray(X)\n",
    "    labels = np.asarray(labels)\n",
    "\n",
    "    if X.shape[0] != labels.shape[0]:\n",
    "        raise ValueError(f\"X has {X.shape[0]} rows, but labels has {labels.shape[0]} elements\")\n",
    "\n",
    "    if sample_size is not None and sample_size < len(X):\n",
    "        rng = np.random.RandomState(random_state)\n",
    "        idx = rng.choice(len(X), size=sample_size, replace=False)\n",
    "        X_used = X[idx]\n",
    "        labels_used = labels[idx]\n",
    "    else:\n",
    "        X_used = X\n",
    "        labels_used = labels\n",
    "\n",
    "    score = silhouette_score(X_used, labels_used, metric=metric)\n",
    "    return score"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7e0890850b14e212",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Movie cluster inspection function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5da9fa697e891c7",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Lists movies belonging to a specific cluster.\n",
    "\n",
    "**Inputs:**<br>\n",
    "movies_df containing movie_id and cluster_id, merged_df with movie_id and title, and a target cluster_id to inspect.\n",
    "\n",
    "**Behavior:**<br>\n",
    "Looking for rows matching the cluster, displays descriptive stats, then merges titles onto those rows for readability.\n",
    "\n",
    "**Output:**<br>\n",
    "Returns a dataframe of cluster members with movie titles, along with a displayed statistical summary"
   ]
  },
  {
   "cell_type": "code",
   "id": "191c9313fa28c536",
   "metadata": {},
   "source": [
    "def inspect_movie_cluster(movies_df, merged_df, cluster_id):\n",
    "\n",
    "    cluster = movies_df[movies_df[\"cluster_id\"] == cluster_id]\n",
    "\n",
    "    display(cluster.describe())\n",
    "\n",
    "    detailed = cluster.merge(\n",
    "        merged_df[[\"movie_id\", \"title\"]].drop_duplicates(),\n",
    "        on=\"movie_id\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "\n",
    "    return detailed"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "537073c7a8a02258",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Cluster visualization function</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e122ccb388f5f52",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Visualizes clusters in two dimensions using PCA\n",
    "\n",
    "**Inputs:**<br>\n",
    "Scaled feature matrix X_scaled, cluster_labels, and optional cap on points per cluster to reduce clutter (only for visual purposes).\n",
    "\n",
    "**Behavior:**<br>\n",
    "Projects data to two principal components, builds a plotting dataframe, samples large clusters for readability, and scatters each cluster with size/edge styling based on cluster size. Adds variance percentages to axes, legend counts, and grid.\n",
    "\n",
    "**Output:**<br>\n",
    "Displays a PCA scatter plot showing cluster separation; no return value beyond visualization"
   ]
  },
  {
   "cell_type": "code",
   "id": "303dbd1509c90958",
   "metadata": {},
   "source": [
    "def plot_clusters_pca_2d(X_scaled, cluster_labels, max_points_per_cluster=1000):\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    df_plot = pd.DataFrame({\n",
    "        \"pc1\": X_pca[:, 0],\n",
    "        \"pc2\": X_pca[:, 1],\n",
    "        \"cluster\": cluster_labels\n",
    "    })\n",
    "\n",
    "    unique_clusters = sorted(df_plot[\"cluster\"].unique())\n",
    "\n",
    "    plt.figure(figsize=(9, 7))\n",
    "\n",
    "    for cid in unique_clusters:\n",
    "        cluster_points = df_plot[df_plot[\"cluster\"] == cid]\n",
    "\n",
    "        if len(cluster_points) > max_points_per_cluster:\n",
    "            cluster_points = cluster_points.sample(\n",
    "                max_points_per_cluster,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "        if len(df_plot[df_plot[\"cluster\"] == cid]) <= 50:\n",
    "            s = 20\n",
    "            edgecolor = \"black\"\n",
    "            linewidth = 0.7\n",
    "        else:\n",
    "            s = 20\n",
    "            edgecolor = \"none\"\n",
    "            linewidth = 0.0\n",
    "\n",
    "        plt.scatter(\n",
    "            cluster_points[\"pc1\"],\n",
    "            cluster_points[\"pc2\"],\n",
    "            s=s,\n",
    "            alpha=0.6,\n",
    "            label=f\"Cluster {cid} (n={len(df_plot[df_plot['cluster'] == cid])})\",\n",
    "            edgecolors=edgecolor,\n",
    "            linewidths=linewidth\n",
    "        )\n",
    "\n",
    "    var1, var2 = pca.explained_variance_ratio_\n",
    "    plt.xlabel(f\"PC1 ({var1:.1%} variance)\")\n",
    "    plt.ylabel(f\"PC2 ({var2:.1%} variance)\")\n",
    "    plt.title(\"K-means movie clusters (PCA 2D)\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "350fb6bdca6963fc",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Spicialized cluster visualization function for DBSCAN</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208ff8f2dfdf32c1",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Visualizes DBSCAN clustering results in two dimensions using PCA, explicitly highlighting both clustered points and noise.\n",
    "\n",
    "**Inputs:**<br>\n",
    "Scaled feature matrix X_scaled and DBSCAN labels (cluster IDs plus -1 for noise), with an optional cap max_points_per_cluster to subsample large clusters for clearer visualization, and an optional title for the plot.\n",
    "\n",
    "**Behavior:**<br>\n",
    "Reduces the data to two principal components via PCA, builds a plotting dataframe, and iterates over each unique DBSCAN label. For each cluster, it optionally subsamples points if the cluster is large, then scatters them on a 2D plot. Noise points (-1) are rendered with a distinct style (e.g., different marker and transparency) to visually separate them from true clusters. Axis labels include explained variance of each principal component, and the legend reports the number of points per cluster including noise.\n",
    "\n",
    "**Output:**<br>\n",
    "Displays a 2D PCA scatter plot showing the spatial distribution of DBSCAN clusters and noise; does not return any value other than the visualization itself."
   ]
  },
  {
   "cell_type": "code",
   "id": "4f919f9c4319d5e1",
   "metadata": {},
   "source": [
    "def plot_dbscan_pca_2d(X_scaled,labels,max_points_per_cluster=1000,title=None):\n",
    "    pca = PCA(n_components=2, random_state=42)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "    df_plot = pd.DataFrame({\n",
    "        \"pc1\": X_pca[:, 0],\n",
    "        \"pc2\": X_pca[:, 1],\n",
    "        \"cluster\": labels\n",
    "    })\n",
    "\n",
    "    unique_labels = np.unique(labels)\n",
    "\n",
    "    plt.figure(figsize=(9, 7))\n",
    "\n",
    "    for cid in unique_labels:\n",
    "        mask = df_plot[\"cluster\"] == cid\n",
    "        cluster_points = df_plot[mask]\n",
    "        n_points = len(cluster_points)\n",
    "\n",
    "        # Subsample large clusters\n",
    "        if n_points > max_points_per_cluster:\n",
    "            cluster_points = cluster_points.sample(\n",
    "                max_points_per_cluster,\n",
    "                random_state=42\n",
    "            )\n",
    "\n",
    "        if cid == -1:\n",
    "            # NOISE\n",
    "            label = f\"Noise (n={n_points})\"\n",
    "            plt.scatter(\n",
    "                cluster_points[\"pc1\"],\n",
    "                cluster_points[\"pc2\"],\n",
    "                s=20,\n",
    "                alpha=0.3,\n",
    "                marker=\"x\",\n",
    "                color=\"red\",\n",
    "                label=label\n",
    "            )\n",
    "        else:\n",
    "            # NORMAL CLUSTERS\n",
    "            label = f\"Cluster {cid} (n={n_points})\"\n",
    "            plt.scatter(\n",
    "                cluster_points[\"pc1\"],\n",
    "                cluster_points[\"pc2\"],\n",
    "                s=20,\n",
    "                alpha=0.6,\n",
    "                marker=\"o\",\n",
    "                edgecolors=\"none\",\n",
    "                label=label\n",
    "            )\n",
    "\n",
    "    # Axis labels with explained variance\n",
    "    var1, var2 = pca.explained_variance_ratio_\n",
    "    plt.xlabel(f\"PC1 ({var1:.1%} variance)\")\n",
    "    plt.ylabel(f\"PC2 ({var2:.1%} variance)\")\n",
    "\n",
    "    plt.title(title or \"DBSCAN clusters (PCA 2D)\")\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.2)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c2f9cc01",
   "metadata": {},
   "source": [
    "### Database initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20634283493ba272",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>DFS</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f77086",
   "metadata": {},
   "source": [
    "**Definition:**<br>\n",
    "**dfs** is the central dictionary that caches every table from the SQLite database. During initialization each table name in table_names is read with pd.read_sql_query and stored under its table name key.\n",
    "\n",
    "**Behaviour:** <br>\n",
    "Takes a list of table names (table_names) and the SQL query results for each table as inputs. Then, pandas DataFrames is keyed by table name.\n",
    "\n",
    "**Usage:**<br>\n",
    "**dfs[key]** can be called to access tables, through registered global variables created with add_new_df function. Keeps globals in sync through sync_dataframe so analyses always reference the up-to-date dataframes"
   ]
  },
  {
   "cell_type": "code",
   "id": "f9fff70c",
   "metadata": {},
   "source": [
    "dfs = {} # dictionary that maps the name of the table to the related data frame\n",
    "\n",
    "for t in table_names:\n",
    "    df = pd.read_sql_query(f\"SELECT * FROM {t};\", conn)\n",
    "    dfs[t] = df\n",
    "    print(f\"\\nLoaded table '{t}' with shape {df.shape}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2aebc136",
   "metadata": {},
   "source": [
    "# adding our base data frames\n",
    "add_new_df(\"movies_stats\", \"movie_statistics\", dfs[\"movie_statistics\"])\n",
    "add_new_df(\"movies\",       \"movies\",           dfs[\"movies\"])\n",
    "add_new_df(\"user_stats\",   \"user_statistics\",  dfs[\"user_statistics\"])\n",
    "add_new_df(\"viewer_ratings\", \"viewer_ratings\", dfs[\"viewer_ratings\"])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f1e8ddfc",
   "metadata": {},
   "source": [
    "### Database Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a5d6109d206cea",
   "metadata": {},
   "source": [
    "Iterates over every loaded DataFrame in dfs, printing the table name and computing the percentage of missing values per column. The loop uses df.isna().mean() * 100 to calculate column-wise null rates and displays them as a single-column DataFrame so missing values can quickly spot data quality gaps across all tables"
   ]
  },
  {
   "cell_type": "code",
   "id": "def25638f8eb473b",
   "metadata": {},
   "source": [
    "for name, df in dfs.items():\n",
    "    print(f\"\\n{name} missing values (%):\")\n",
    "    missing_pct = df.isna().mean() * 100\n",
    "    display(missing_pct.to_frame(\"missing_%\"))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ca1bb4066703bf66",
   "metadata": {},
   "source": [
    "Filters user_stats to the subset of users with std_rating still missing, tells how many such users exist, and then checks how many of their customer_id values are present in viewer_ratings table. Then prints the count of the total missing-std_rating users and how many of them appear in the ratings table, confirming whether the missing entries have corresponding viewer activity"
   ]
  },
  {
   "cell_type": "code",
   "id": "3ce2621ae7448d16",
   "metadata": {},
   "source": [
    "missing_after = user_stats[user_stats[\"std_rating\"].isna()].copy()\n",
    "\n",
    "print(\"All users with NaN std:\", len(missing_after))\n",
    "\n",
    "# checking if they are in viewer_ratings\n",
    "print(missing_after[\"customer_id\"].isin(viewer_ratings[\"customer_id\"]).sum())"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "95d469c6",
   "metadata": {},
   "source": [
    "### Movie Statistics Calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2362bc",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Missing STD Calculations</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78394717c371ac27",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "A function to calculate missing std. ratings of films\n",
    "\n",
    "**Behaviour:**<br>\n",
    "Calculates the missing STD values by gathering all the information from the dataset provided\n",
    "\n",
    "**Output:**<br>\n",
    "Doesn't return anything, just updates the dataframe and prints the diagnostics for improvement checking\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "1395d0b962d46cb5",
   "metadata": {},
   "source": [
    "# syncing\n",
    "sync_dataframe(movies_stats, viewer_ratings)\n",
    "\n",
    "# removing nan values\n",
    "viewer_ratings_clean = viewer_ratings[~viewer_ratings[\"rating\"].isna()]\n",
    "\n",
    "movie_std = (viewer_ratings_clean.groupby(\"movie_id\")[\"rating\"].apply(list).reset_index(name=\"ratings\"))\n",
    "\n",
    "# calculating std\n",
    "movie_std[\"std_rating\"] = movie_std[\"ratings\"].apply(manual_std)\n",
    "movie_std = movie_std[[\"movie_id\", \"std_rating\"]]\n",
    "\n",
    "# registering pre-change df\n",
    "old_movies_stats = movies_stats.copy()\n",
    "\n",
    "# merging and updating\n",
    "movies_stats = merge_and_replace_column(movies_stats,movie_std,\"movie_id\",\"std_rating\")\n",
    "\n",
    "# updating\n",
    "dfs[\"movie_statistics\"] = movies_stats\n",
    "\n",
    "# calculating improvements (diagnostics purposes only)\n",
    "calculate_missing_values_improvement(old_movies_stats, movies_stats, \"std_rating\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "58f58fd5",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Missing Total Ratings Calculation</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d4518d",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Calculating the missing total_ratings of movies\n",
    "\n",
    "**Behaviour:**<br>\n",
    "Calculates the missing total ratings by gathering all the information from the dataset provided and manually counting the ratings for each film\n",
    "\n",
    "**Output:**<br>\n",
    "Doesn't return anything, just updates the dataframe with the newly calculated missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "11b6c1bf",
   "metadata": {},
   "source": [
    "sync_dataframe(movies_stats)\n",
    "\n",
    "# Collecting all the movies with absent total_rating in a dictionary\n",
    "missing_dict = {}\n",
    "\n",
    "missing = search_by_parameter('movie_statistics', 'total_ratings', None)\n",
    "missing_dict = {row.movie_id: 0 for row in missing.itertuples(index=False)}\n",
    "\n",
    "# registering pre-change df\n",
    "old_df = movies_stats.copy()\n",
    "\n",
    "# Iterating through viewer_ratings and manually counting the ratings for each film\n",
    "for row in viewer_ratings_clean.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "    if movie_id in missing_dict:\n",
    "        missing_dict[movie_id] += 1\n",
    "\n",
    "# Update movie_stats\n",
    "for row in movies_stats.itertuples(index=True):\n",
    "    if row.movie_id in missing_dict:\n",
    "        movies_stats.at[row.Index, \"total_ratings\"] = missing_dict[row.movie_id]\n",
    "\n",
    "dfs[\"movie_statistics\"] = movies_stats\n",
    "calculate_missing_values_improvement(old_df, movies_stats, \"total_ratings\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ef4c7d36",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Missing Average Ratings Calculation</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f9afb8",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Calculating Missing Averages\n",
    "\n",
    "**Behaviour:**<br>\n",
    "Calculates the missing average ratings by gathering all the information from the dataset provided, Iterating through viewer ratings and adding to sum if movie_id matches\n",
    "\n",
    "**Output:**<br>\n",
    "Doesn't return anything, just updates the dataframe with the newly calculated missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "63d4756a",
   "metadata": {},
   "source": [
    "sync_dataframe(movies_stats, viewer_ratings)\n",
    "\n",
    "# Finding movies with null avg_rating\n",
    "missing_avg = search_by_parameter('movie_statistics', 'avg_rating', None)\n",
    "\n",
    "# Creating a dict of type { movie_id : avg_rating }\n",
    "# set 0 as base value for now, might change it later\n",
    "missing_avg_dict = {row.movie_id: 0 for row in missing_avg.itertuples(index=False)}\n",
    "\n",
    "# Storing the sum of all ratings for each movie\n",
    "rating_sums = {movie_id: 0 for movie_id in missing_avg_dict}\n",
    "\n",
    "#registering the old df\n",
    "old_df = movies_stats.copy()\n",
    "\n",
    "# Iterating through viewer ratings and adding to sum if movie_id matches\n",
    "for row in viewer_ratings_clean.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if movie_id in missing_avg_dict:\n",
    "        rating_sums[movie_id] += rating\n",
    "\n",
    "for row in movies_stats.itertuples():\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    if movie_id in rating_sums:\n",
    "        total = row.total_ratings # I'm assuming that my calculations of total_ratings per movie is correct ang i got rid of                            all null values\n",
    "\n",
    "        # IF FORE SOME MAGICAL REASON THERE IS STILL A NULL THEN IGNORE\n",
    "        if pd.isna(total) or total == 0:\n",
    "            avg = 0\n",
    "        else:\n",
    "            avg = rating_sums[movie_id] / total\n",
    "\n",
    "        movies_stats.at[row.Index, \"avg_rating\"] = avg\n",
    "\n",
    "# calculating improvements\n",
    "calculate_missing_values_improvement(old_df, movies_stats, \"avg_rating\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "0fa1401f",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Missing Min and Max Retrieval</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21090df8",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Calculating the missing min and max ratings for movies\n",
    "\n",
    "**Behaviour:**<br>\n",
    "Calculates the min and max values by gathering all the information from the dataset provided, Retrieved all actual ratings for these movies from the viewer_ratings table\n",
    "\n",
    "**Output:**<br>\n",
    "Doesn't return anything, just updates the dataframe with the newly reterived values\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "b9bb36c2",
   "metadata": {},
   "source": [
    "sync_dataframe(movies_stats)\n",
    "\n",
    "# Find movies with missing min_rating and max_rating\n",
    "missing_min = search_by_parameter('movie_statistics', 'min_rating', None)\n",
    "missing_max = search_by_parameter('movie_statistics', 'max_rating', None)\n",
    "\n",
    "# Combine them as some movies may be in both\n",
    "missing_ids = set(missing_min[\"movie_id\"]) | set(missing_max[\"movie_id\"])\n",
    "\n",
    "# Init dictionary for movies we need to compute\n",
    "missing_movies = {\n",
    "    movie_id: {\"min\": -1, \"max\": -1}  # base = -1\n",
    "    for movie_id in missing_ids\n",
    "}\n",
    "\n",
    "# Go through all ratings and update min/max for relevant movies\n",
    "for row in viewer_ratings_clean.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if movie_id in missing_movies:\n",
    "        # updating min\n",
    "        if missing_movies[movie_id][\"min\"] == -1 or rating < missing_movies[movie_id][\"min\"]:\n",
    "            missing_movies[movie_id][\"min\"] = rating\n",
    "\n",
    "        # updating max\n",
    "        if missing_movies[movie_id][\"max\"] == -1 or rating > missing_movies[movie_id][\"max\"]:\n",
    "            missing_movies[movie_id][\"max\"] = rating\n",
    "\n",
    "# register old\n",
    "old_df = movies_stats.copy()\n",
    "\n",
    "# Update movie_statistics\n",
    "for row in movies_stats.itertuples():\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    if movie_id in missing_movies:\n",
    "        min_val = missing_movies[movie_id][\"min\"]\n",
    "        max_val = missing_movies[movie_id][\"max\"]\n",
    "\n",
    "        # if movie ended up with no ratings, keep them as None\n",
    "        if min_val == -1 and max_val == -1:\n",
    "            min_val = None\n",
    "            max_val = None\n",
    "\n",
    "        movies_stats.at[row.Index, \"min_rating\"] = min_val\n",
    "        movies_stats.at[row.Index, \"max_rating\"] = max_val\n",
    "\n",
    "#updating\n",
    "dfs[\"movie_statistics\"] = movies_stats\n",
    "\n",
    "#calculating improvements\n",
    "calculate_missing_values_improvement(old_df, movies_stats, \"min_rating\")\n",
    "calculate_missing_values_improvement(old_df, movies_stats, \"max_rating\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e830a145",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Missing Unique Users Calculation/Retrieval</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a83870a25990a35e",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Finding missing unique users\n",
    "\n",
    "**Behaviour:**<br>\n",
    "Calculates the missing unique user data by gathering all the information from the dataset provided, Gathers unique users and counts them.\n",
    "\n",
    "**Output:**<br>\n",
    "Doesn't return anything, just updates the dataframe with the newly calculated data\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "65c9dd911b77bbd1",
   "metadata": {},
   "source": [
    "sync_dataframe(movies_stats)\n",
    "\n",
    "missing_unique = search_by_parameter('movie_statistics', 'unique_users', None)\n",
    "\n",
    "# creating my favorite movie set\n",
    "missing_movie_ids = {row.movie_id for row in missing_unique.itertuples(index=False)}\n",
    "\n",
    "# creating a dict movie_id: customer_id\n",
    "unique_users_dict = {movie_id: set() for movie_id in missing_movie_ids}\n",
    "\n",
    "# Gathering unique users\n",
    "for row in viewer_ratings_clean.itertuples(index=False):\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    # as always im getting only those movies which have null for unique users\n",
    "    if movie_id in unique_users_dict:\n",
    "        unique_users_dict[movie_id].add(row.customer_id)\n",
    "\n",
    "old_df = movies_stats.copy()\n",
    "\n",
    "# Counting unique users\n",
    "updated = 0\n",
    "for movie_id, users in unique_users_dict.items():\n",
    "    count = len(users)  # unique users count\n",
    "\n",
    "    movies_stats.loc[\n",
    "        movies_stats[\"movie_id\"] == movie_id,\n",
    "        \"unique_users\"\n",
    "    ] = count\n",
    "\n",
    "    updated += 1\n",
    "\n",
    "# just in case if a movie has 0 ratings im setting unique users to 0\n",
    "movies_stats[\"unique_users\"] = movies_stats[\"unique_users\"].fillna(0).astype(int)\n",
    "\n",
    "# Updating\n",
    "dfs[\"movie_statistics\"] = movies_stats\n",
    "calculate_missing_values_improvement(old_df, movies_stats, \"unique_users\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e4807162c092ff91",
   "metadata": {},
   "source": [
    "### User_statistics calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae46b86c",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Missing STD Calculations</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b0dc4f80d5c7f16",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Calculating user's std rating\n",
    "\n",
    "**Behaviour:**<br>\n",
    "Calculates the missing STD values by gathering all the information from the dataset provided\n",
    "\n",
    "**Output:**<br>\n",
    "Doesn't return anything, just updates the dataframe and prints the diagnostics for improvement checking"
   ]
  },
  {
   "cell_type": "code",
   "id": "9820aaa2583197",
   "metadata": {},
   "source": [
    "sync_dataframe(user_stats)\n",
    "\n",
    "# grouping by user id\n",
    "user_std = (viewer_ratings_clean.groupby(\"customer_id\")[\"rating\"].apply(list).reset_index(name=\"ratings\"))\n",
    "\n",
    "# calculating std rating per user\n",
    "user_std[\"std_rating\"] = user_std[\"ratings\"].apply(manual_std)\n",
    "user_std = user_std[[\"customer_id\", \"std_rating\"]]\n",
    "\n",
    "# registering old\n",
    "old_df = user_stats.copy()\n",
    "\n",
    "# merging and updating\n",
    "user_stats = merge_and_replace_column(user_stats, user_std, \"customer_id\", \"std_rating\")\n",
    "\n",
    "# updating\n",
    "dfs[\"user_statistics\"] = user_stats\n",
    "\n",
    "# calculating improvements\n",
    "calculate_missing_values_improvement(old_df, user_stats, \"std_rating\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "371d117a",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Missing Total Ratings Calculation</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed22d075b53743f9",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Calculating the missing total_ratings of Users\n",
    "\n",
    "**Behaviour:**<br>\n",
    "Calculates the missing total ratings by gathering all the information from the dataset provided and manually counting the ratings for each user\n",
    "\n",
    "**Output:**<br>\n",
    "Doesn't return anything, just updates the dataframe with the newly calculated missing values\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "3e6f6477f9abcc7b",
   "metadata": {},
   "source": [
    "sync_dataframe(user_stats)\n",
    "\n",
    "# Collecting all the users with absent total_ratings\n",
    "missing_dict = {}\n",
    "\n",
    "missing = search_by_parameter('user_statistics', 'total_ratings', None)\n",
    "missing_dict = {row.customer_id: 0 for row in missing.itertuples(index=False)}\n",
    "\n",
    "# Iterating through viewer_ratings and manually counting the ratings for each user\n",
    "for row in viewer_ratings.itertuples(index=False):\n",
    "    if pd.isna(row.rating):\n",
    "        continue\n",
    "\n",
    "    customer_id = row.customer_id\n",
    "    if customer_id in missing_dict:\n",
    "        missing_dict[customer_id] += 1\n",
    "\n",
    "old_df = user_stats.copy()\n",
    "\n",
    "# Update user_stats\n",
    "for row in user_stats.itertuples(index=True):\n",
    "    if row.customer_id in missing_dict:\n",
    "        user_stats.at[row.Index, \"total_ratings\"] = missing_dict[row.customer_id]\n",
    "\n",
    "dfs[\"user_statistics\"] = user_stats\n",
    "calculate_missing_values_improvement(old_df, user_stats, \"total_ratings\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "05dbdc81",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Missing Average Ratings Calculation</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b7f2348b331e259",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Calculating Missing user avg ratings\n",
    "\n",
    "**Behaviour:**<br>\n",
    "Calculates the missing avg ratings by gathering all the information from the dataset provided, Iterating through viewer ratings and adding to sum if movie_id matches\n",
    "\n",
    "**Output:**<br>\n",
    "Doesn't return anything, just updates the dataframe with the newly calculated missing values"
   ]
  },
  {
   "cell_type": "code",
   "id": "4641d086358af2ec",
   "metadata": {},
   "source": [
    "sync_dataframe(user_stats)\n",
    "\n",
    "# Finding all users with null avg_rating\n",
    "missing_avg = search_by_parameter('user_statistics', 'avg_rating', None)\n",
    "\n",
    "# Creating a dict of type { customer_id : avg_rating }\n",
    "# set 0 as base value\n",
    "missing_avg_dict = {row.customer_id: 0 for row in missing_avg.itertuples(index=False)}\n",
    "\n",
    "# Storing the sum of all ratings for each movie\n",
    "rating_sums = {customer_id: 0 for customer_id in missing_avg_dict}\n",
    "\n",
    "old_df = user_stats.copy()\n",
    "\n",
    "# Iterating through viewer ratings and adding to sum if movie_id matches\n",
    "for row in viewer_ratings_clean.itertuples(index=False):\n",
    "\n",
    "    customer_id = row.customer_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if customer_id in missing_avg_dict:\n",
    "        rating_sums[customer_id] += rating\n",
    "\n",
    "for row in user_stats.itertuples():\n",
    "    customer_id = row.customer_id\n",
    "\n",
    "    if customer_id in rating_sums:\n",
    "        total = row.total_ratings\n",
    "\n",
    "        if pd.isna(total) or total == 0:\n",
    "            avg = 0\n",
    "        else:\n",
    "            avg = rating_sums[customer_id] / total\n",
    "\n",
    "        user_stats.at[row.Index, \"avg_rating\"] = avg\n",
    "\n",
    "calculate_missing_values_improvement(old_df, user_stats, \"avg_rating\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "6c8e0ec1",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Missing Activity Day Values Calculation</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f168904a",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Calculating Missing Activity Day Values\n",
    "\n",
    "**Behaviour:**<br>\n",
    "Calculates the missing Activity Day Values by gathering all the information from the dataset provided, it takes the first day and last day for each user to calculate there total active days\n",
    "\n",
    "**Output:**<br>\n",
    "Doesn't return anything, just updates the dataframe with the newly calculated missing values"
   ]
  },
  {
   "cell_type": "code",
   "id": "34cdefa630a021fd",
   "metadata": {},
   "source": [
    "#activity days\n",
    "sync_dataframe(user_stats)\n",
    "\n",
    "missing = search_by_parameter('user_statistics', 'activity_days', None)\n",
    "\n",
    "# creating a dict\n",
    "missing_dict = {row.customer_id: 0 for row in missing.itertuples(index=False)}\n",
    "\n",
    "old_df = user_stats.copy()\n",
    "\n",
    "for row in user_stats.itertuples():\n",
    "    customer_id = row.customer_id\n",
    "\n",
    "    if customer_id not in missing_dict: # wroking only with users with missing activity_days\n",
    "        continue\n",
    "\n",
    "    first_date = covert_string_to_date(row.first_rating_date)\n",
    "    last_date = covert_string_to_date(row.last_rating_date)\n",
    "\n",
    "    # null exception\n",
    "    if first_date is None or last_date is None:\n",
    "        continue\n",
    "\n",
    "    # calcing the difference\n",
    "    difference = (last_date - last_date).days\n",
    "\n",
    "    # updating activity_days\n",
    "    user_stats.at[row.Index, 'activity_days'] = difference\n",
    "\n",
    "calculate_missing_values_improvement(old_df, user_stats, \"activity_days\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c09aa1c7",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Missing Min Max Rating Retrieval</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7273337",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Calculating the missing min and max ratings for users\n",
    "\n",
    "**Behaviour:**<br>\n",
    "Calculates the min and max values by gathering all the information from the dataset provided, Retrieved all actual ratings for these users from the viewer_ratings table\n",
    "\n",
    "**Output:**<br>\n",
    "Doesn't return anything, just updates the dataframe with the newly reterived values"
   ]
  },
  {
   "cell_type": "code",
   "id": "fbb5f2e6dfe2b465",
   "metadata": {},
   "source": [
    "#minmax\n",
    "sync_dataframe(user_stats)\n",
    "\n",
    "# Find users with missing min_rating and max_rating using your function\n",
    "missing_min = search_by_parameter('user_statistics', 'min_rating', None)\n",
    "missing_max = search_by_parameter('user_statistics', 'max_rating', None)\n",
    "\n",
    "# Combine them as some users may be in both\n",
    "missing_ids = set(missing_min[\"customer_id\"]) | set(missing_max[\"customer_id\"])\n",
    "\n",
    "# Init dictionary for users we need to compute\n",
    "missing_users = {\n",
    "    customer_id: {\"min\": -1, \"max\": -1} #setting base to -1\n",
    "    for customer_id in missing_ids\n",
    "}\n",
    "\n",
    "for row in viewer_ratings_clean.itertuples(index=False):\n",
    "    customer_id = row.customer_id\n",
    "    rating = row.rating\n",
    "\n",
    "    if customer_id in missing_users:\n",
    "        # updating min\n",
    "        if missing_users[customer_id][\"min\"] == -1 or rating < missing_users[customer_id][\"min\"]:\n",
    "            missing_users[customer_id][\"min\"] = rating\n",
    "\n",
    "        # updating max\n",
    "        if missing_users[customer_id][\"max\"] == -1 or rating > missing_users[customer_id][\"max\"]:\n",
    "            missing_users[customer_id][\"max\"] = rating\n",
    "\n",
    "old_df = user_stats.copy()\n",
    "\n",
    "# Updating user stats\n",
    "for row in user_stats.itertuples():\n",
    "    customer_id = row.customer_id\n",
    "\n",
    "    if customer_id in missing_users:\n",
    "        min_val = missing_users[customer_id][\"min\"]\n",
    "        max_val = missing_users[customer_id][\"max\"]\n",
    "\n",
    "        # if the guy hasn't rated anything im making sure we set min or max to none so we get rid of them in the future\n",
    "        if min_val == -1 and max_val == -1:\n",
    "            min_val = None\n",
    "            max_val = None\n",
    "\n",
    "        user_stats.at[row.Index, \"min_rating\"] = min_val\n",
    "        user_stats.at[row.Index, \"max_rating\"] = max_val\n",
    "\n",
    "dfs[\"user_statistics\"] = user_stats\n",
    "calculate_missing_values_improvement(old_df, user_stats,\"min_rating\")\n",
    "calculate_missing_values_improvement(old_df, user_stats,\"max_rating\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "fd645586",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Missing Unique Movie Calculations</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6283a2c",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Finding missing unique movies\n",
    "\n",
    "**Behaviour:**<br>\n",
    "Calculates the missing unique movies data by gathering all the information from the dataset provided, Gathers unique movies the user has interacted with and counts them if the value is missing.\n",
    "\n",
    "**Output:**<br>\n",
    "Doesn't return anything, just updates the dataframe with the newly calculated data\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "de42b238",
   "metadata": {},
   "source": [
    "sync_dataframe(user_stats)\n",
    "missing_unique = search_by_parameter('user_statistics', 'unique_movies', None)\n",
    "\n",
    "missing_ids = set(missing_unique[\"customer_id\"])\n",
    "\n",
    "unique_movies_dict = {customer_id: set() for customer_id in missing_ids}\n",
    "\n",
    "for row in viewer_ratings_clean.itertuples(index=False):\n",
    "    user_id = row.customer_id\n",
    "    movie_id = row.movie_id\n",
    "\n",
    "    if user_id in unique_movies_dict:\n",
    "        unique_movies_dict[user_id].add(movie_id)\n",
    "\n",
    "old_df = user_stats.copy()\n",
    "\n",
    "for row in user_stats.itertuples():\n",
    "    user_id = row.customer_id\n",
    "\n",
    "    if user_id in unique_movies_dict:\n",
    "        count_unique = len(unique_movies_dict[user_id])\n",
    "\n",
    "        if count_unique == 0:\n",
    "            value = None\n",
    "        else:\n",
    "            value = count_unique\n",
    "\n",
    "        user_stats.at[row.Index, \"unique_movies\"] = value\n",
    "\n",
    "# updating\n",
    "dfs[\"user_statistics\"] = user_stats\n",
    "calculate_missing_values_improvement(old_df, user_stats, \"unique_movies\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d12e52c5",
   "metadata": {},
   "source": [
    "### Merging datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593c36b6",
   "metadata": {},
   "source": [
    "Merging the Movies and movie statis filling in missing values on either dataset and converting all of the dates to type DateTime as well as all counts and years to integers.\n",
    "This is in order to clean our movie data before merging it with our user data to fill in any recoverable missing values.\n",
    "Rows with missing values will be dropped."
   ]
  },
  {
   "cell_type": "code",
   "id": "46319c89",
   "metadata": {},
   "source": [
    "movies = dfs['movies'].copy()\n",
    "movie_stats = dfs['movie_statistics'].copy()\n",
    "viewer_ratings = dfs['viewer_ratings'].copy()\n",
    "\n",
    "bad_ids_movies = movies.loc[movies['movie_id'].duplicated(keep=False), 'movie_id'].unique().tolist()\n",
    "bad_ids_stats = movie_stats.loc[movie_stats['movie_id'].duplicated(keep=False), 'movie_id'].unique().tolist()\n",
    "bad_ids = list(set(bad_ids_movies + bad_ids_stats))\n",
    "print('bad movie ids to remove: ', bad_ids)\n",
    "\n",
    "viewer_ratings = viewer_ratings[~viewer_ratings['movie_id'].isin(bad_ids)]\n",
    "movies = movies[~movies['movie_id'].isin(bad_ids)]\n",
    "movie_stats = movie_stats[~movie_stats['movie_id'].isin(bad_ids)]\n",
    "\n",
    "movies['year_of_release'] = (pd.to_numeric(movies['year_of_release'], errors='coerce').astype('Int64'))\n",
    "movie_stats['total_ratings'] = (pd.to_numeric(movie_stats['total_ratings'], errors='coerce').astype('Int64'))\n",
    "movie_stats['unique_users'] = (pd.to_numeric(movie_stats['unique_users'], errors='coerce').astype('Int64'))\n",
    "movie_stats['year_of_release'] = (pd.to_numeric(movie_stats['year_of_release'], errors='coerce').astype('Int64'))\n",
    "\n",
    "movie_stats['first_rating_date'] = pd.to_datetime(movie_stats['first_rating_date'], errors='coerce')\n",
    "movie_stats['last_rating_date'] = pd.to_datetime(movie_stats['last_rating_date'], errors='coerce')\n",
    "\n",
    "# this will automatically remove rows with missing data\n",
    "movie_full = movies.merge(movie_stats, on='movie_id', how='inner', suffixes=('_movies', '_stats'))\n",
    "\n",
    "movie_full['title'] = movie_full['title_movies'].combine_first(movie_full['title_stats'])\n",
    "movie_full['year_of_release'] = movie_full['year_of_release_movies'].combine_first(movie_full['year_of_release_stats'])\n",
    "\n",
    "movie_full = movie_full.drop(columns=['title_movies', 'title_stats', 'year_of_release_movies', 'year_of_release_stats'])\n",
    "\n",
    "dfs['movies'] = movies\n",
    "dfs['movie_statistics'] = movie_stats\n",
    "dfs['viewer_ratings'] = viewer_ratings\n",
    "add_new_df('movie_full', 'movie_full', movie_full)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9d1d3b2a1691c50e",
   "metadata": {},
   "source": [
    "print(\"movies dtypes:\")\n",
    "print(movies.dtypes)\n",
    "\n",
    "print(\"\\nmovie_stats dtypes:\")\n",
    "print(movie_stats.dtypes)\n",
    "\n",
    "print(\"\\nmovie_full dtypes:\")\n",
    "print(movie_full.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4bec12db",
   "metadata": {},
   "source": [
    "- Converts the date parameter in viewer_ratings to datetime.\n",
    "- Merges viewer_ratings, movies, movie_statistics and user_statistics into one dataset as merged_data."
   ]
  },
  {
   "cell_type": "code",
   "id": "bc40bfb82470b10a",
   "metadata": {},
   "source": [
    "movie_full = dfs['movie_full'].copy()\n",
    "user_stats = dfs['user_statistics'].copy()\n",
    "viewer_ratings = dfs['viewer_ratings'].copy()\n",
    "\n",
    "\n",
    "viewer_ratings['date'] = pd.to_datetime(viewer_ratings['date'], errors = 'coerce')\n",
    "user_stats['first_rating_date'] = pd.to_datetime(user_stats['first_rating_date'], errors = 'coerce')\n",
    "user_stats['last_rating_date'] = pd.to_datetime(user_stats['last_rating_date'], errors = 'coerce')\n",
    "\n",
    "user_stats['total_ratings'] = (pd.to_numeric(user_stats['total_ratings'], errors='coerce').astype('Int64'))\n",
    "user_stats['unique_movies'] = (pd.to_numeric(user_stats['unique_movies'], errors='coerce').astype('Int64'))\n",
    "user_stats['activity_days'] = (pd.to_numeric(user_stats['activity_days'], errors='coerce').astype('Int64'))\n",
    "\n",
    "\n",
    "# create user and movie specific columns\n",
    "movie_full = movie_full.rename(columns={\n",
    "    'total_ratings':     'movie_total_ratings',\n",
    "    'avg_rating':        'movie_avg_rating',\n",
    "    'std_rating':        'movie_std_rating',\n",
    "    'min_rating':        'movie_min_rating',\n",
    "    'max_rating':        'movie_max_rating',\n",
    "    'first_rating_date': 'movie_first_rating_date',\n",
    "    'last_rating_date':  'movie_last_rating_date'\n",
    "})\n",
    "\n",
    "\n",
    "user_stats = user_stats.rename(columns={\n",
    "    'total_ratings':     'user_total_ratings',\n",
    "    'avg_rating':        'user_avg_rating',\n",
    "    'std_rating':        'user_std_rating',\n",
    "    'min_rating':        'user_min_rating',\n",
    "    'max_rating':        'user_max_rating',\n",
    "    'first_rating_date': 'user_first_rating_date',\n",
    "    'last_rating_date':  'user_last_rating_date'\n",
    "})\n",
    "\n",
    "# drop anomalous date for user stats\n",
    "if 'anomalous_date' in viewer_ratings.columns:\n",
    "    viewer_ratings = viewer_ratings.drop(columns=['anomalous_date'])\n",
    "\n",
    "# merge user stats and viewer ratings\n",
    "viewer_ratings_with_stats = viewer_ratings.merge(user_stats, on = 'customer_id', how = 'inner')\n",
    "\n",
    "merged_data = viewer_ratings_with_stats.merge(movie_full, on = 'movie_id', how = 'inner')\n",
    "\n",
    "# this will drop all rows with any null\n",
    "merged_data = merged_data.dropna(how = 'any')\n",
    "\n",
    "add_new_df('merged_data', 'merged_data', merged_data)\n",
    "dfs['movie_full'] = movie_full\n",
    "dfs['user_statistics'] = user_stats\n",
    "dfs['viewer_ratings'] = viewer_ratings\n",
    "\n",
    "print(merged_data.dtypes)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2a03e0bdd0ee078c",
   "metadata": {},
   "source": [
    "# Features for clustering\n",
    "movie_features = [\n",
    "    \"movie_total_ratings\",\n",
    "    \"movie_avg_rating\",\n",
    "    \"movie_std_rating\",\n",
    "    \"movie_min_rating\",\n",
    "    \"movie_max_rating\",\n",
    "]\n",
    "\n",
    "user_features_full = [\n",
    "    'user_total_ratings',\n",
    "    'user_avg_rating',\n",
    "    'user_std_rating',\n",
    "    'user_min_rating',\n",
    "    'user_max_rating',\n",
    "    'unique_movies',\n",
    "    'activity_days'\n",
    "]\n",
    "\n",
    "user_features = [\n",
    "    'user_total_ratings',\n",
    "    'activity_days',\n",
    "]\n",
    "\n",
    "movies_for_clustering = (\n",
    "    merged_data\n",
    "    .groupby(\"movie_id\")[movie_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "users_for_clustering = (\n",
    "    merged_data\n",
    "    .groupby(\"customer_id\")[user_features_full]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# since the user dataset is huge, only using a random 10% sample of the set for diagnostics\n",
    "reduced_user_sample = users_for_clustering[user_features_full].sample(\n",
    "    frac=0.1,\n",
    "    random_state=42\n",
    ").astype(float).dropna()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8f739a9d121fa80d",
   "metadata": {},
   "source": [
    "### Pre-modeling Diagnostics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1b35e47bd96d4f",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>User Feature Importance Screening</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9723eaf64b7cd1",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "To identify which user-level variables contain the most meaningful information for clustering. Because the user dataset is extremely large, we need to reduce the feature space to only the variables that meaningfully contribute to differences between users.\n",
    "\n",
    "**Behaviour:**<br>\n",
    "The .describe().T[[\"std\"]] part computes the standard deviation of each feature.<br>\n",
    "The .corr() part computes the pairwise correlation matrix.\n",
    "\n",
    "**Interpretation:**<br>\n",
    "The bigger the std - the more variance that variable has, therefore, the more useful it is for us"
   ]
  },
  {
   "cell_type": "code",
   "id": "ab47f2869e6f8d95",
   "metadata": {},
   "source": [
    "user_stats[user_features_full].describe().T[[\"std\"]]\n",
    "#user_stats[user_features].corr()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e651d902b3886eea",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Data normal distribution test</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e46e8e68ea1645",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "\n",
    "**Inputs:**<br>\n",
    "\n",
    "**Behaviour:**<br>\n",
    "\n",
    "**Output:**<br>"
   ]
  },
  {
   "cell_type": "code",
   "id": "897021858fc502bd",
   "metadata": {},
   "source": [
    "def plot_data_distribution(data):\n",
    "    stats.probplot(data, dist=\"norm\", plot=plt)\n",
    "    return plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6d787e9c36e16b56",
   "metadata": {},
   "source": [
    "for ud in user_features_full:\n",
    "    print(ud)\n",
    "    plot_data_distribution(merged_data[ud])\n",
    "\n",
    "for md in movie_features:\n",
    "    print(md)\n",
    "    plot_data_distribution(merged_data[md])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "203fd4852ee1ea2e",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Kernel Density Estimation Test</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8dca058a895bb5",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "KDE reveals whether the data forms distinct dense regions (potential clusters) or whether it is one continuous mass with no meaningful separation.\n",
    "\n",
    "**Behaviour:**<br>\n",
    "Gets 2D PCA-transformed data (pca_data[:,0], pca_data[:,1]) as inputs, then estimates the probability density function of the data by smoothing individual points. It overlays a continuous heatmap on the scatter plot, showing where points densely accumulate v/s where they are sparse.\n",
    "\n",
    "**Interpretation:**<br>\n",
    "The brighter -  the higher density.\n",
    "Smooth gradients - no clear cluster structure."
   ]
  },
  {
   "cell_type": "code",
   "id": "ad867bcbfefe44cc",
   "metadata": {},
   "source": [
    "X = movies_for_clustering[movie_features].astype(float).dropna()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(\n",
    "    x=pca_data[:, 0],\n",
    "    y=pca_data[:, 1],\n",
    "    fill=True,\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.7,\n",
    "    thresh=0.05,\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    pca_data[:, 0],\n",
    "    pca_data[:, 1],\n",
    "    s=8,\n",
    "    color=\"white\",\n",
    "    alpha=0.3,\n",
    "    edgecolor=None\n",
    ")\n",
    "\n",
    "plt.title(\"KDE + Scatter (PCA projection)\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "b2dcc8b8ea802df5",
   "metadata": {},
   "source": [
    "Y = reduced_user_sample[user_features_full].astype(float).dropna()\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "Y_scaled = scaler.fit_transform(Y)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "pca_data = pca.fit_transform(Y_scaled)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(\n",
    "    x=pca_data[:, 0],\n",
    "    y=pca_data[:, 1],\n",
    "    fill=True,\n",
    "    cmap=\"viridis\",\n",
    "    alpha=0.7,\n",
    "    thresh=0.05,\n",
    ")\n",
    "\n",
    "plt.scatter(\n",
    "    pca_data[:, 0],\n",
    "    pca_data[:, 1],\n",
    "    s=8,\n",
    "    color=\"white\",\n",
    "    alpha=0.05,\n",
    "    edgecolor=None\n",
    ")\n",
    "\n",
    "plt.title(\"KDE + Scatter (PCA projection)\")\n",
    "plt.xlabel(\"PCA 1\")\n",
    "plt.ylabel(\"PCA 2\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "98b20684562394b1",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>KNN Stats</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b9aacd546b8933a",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Checks whether the dataset has natural cluster structure by analyzing how close each point is to its neighbors.\n",
    "If real clusters exist, points inside a cluster should have similar and small neighbor distances, while points between clusters should have larger distances.\n",
    "\n",
    "**Behaviour:**<br>\n",
    "For every point, the algorithm finds its 10 nearest neighbors. It then computes the mean distance to these 10 neighbors. And then it can get the standard deviation of these mean distances and the average (mean of means).\n",
    "\n",
    "**Interpretation:**<br>\n",
    "std > mean: strong signs of cluster-like structure or density separation.<br>\n",
    "std â‰ˆ mean: mild structure, maybe weak clusters.<br>\n",
    "std < mean: very uniform data which is poor for clustering.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "25ebce1e917fae06",
   "metadata": {},
   "source": [
    "nn = NearestNeighbors(n_neighbors=10)\n",
    "nn.fit(X_scaled)\n",
    "\n",
    "distances, _ = nn.kneighbors(X_scaled)\n",
    "mean_d = distances.mean(axis=1)\n",
    "\n",
    "print(\"Std of mean distances:\", np.std(mean_d))\n",
    "print(\"Mean of mean distances:\", np.mean(mean_d))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bb64d383cedd3c4c",
   "metadata": {},
   "source": [
    "nn = NearestNeighbors(n_neighbors=10)\n",
    "nn.fit(Y_scaled)\n",
    "\n",
    "distances, _ = nn.kneighbors(Y_scaled)\n",
    "mean_d = distances.mean(axis=1)\n",
    "\n",
    "print(\"Std of mean distances:\", np.std(mean_d))\n",
    "print(\"Mean of mean distances:\", np.mean(mean_d))"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f765044da33890b0",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Distance distribution test</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fbad74ca637bf86",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Helps to understand how all points in the dataset relate to each other in terms of distance.\n",
    "It checks whether the dataset contains distinct separation gaps (which would indicate clusters) or if distances form one continuous distribution (which suggests no clear cluster structure).\n",
    "\n",
    "**Behaviour:**<br>\n",
    "Computes all pairwise distances between all points, flattens them into a single vector (flat), and plots a histogram showing how frequently each distance value occurs.\n",
    "\n",
    "**Interpretation:**<br>\n",
    "Multiple peaks: different groups of distances, potential cluster separation.<br>\n",
    "One smooth peak: distances vary gradually, no meaningful clustering structure.<br>\n",
    "Long tails: possible outliers or density imbalance."
   ]
  },
  {
   "cell_type": "code",
   "id": "ae18af5e23815649",
   "metadata": {},
   "source": [
    "dists = pairwise_distances(X_scaled)\n",
    "flat = dists.flatten()\n",
    "\n",
    "plt.hist(flat, bins=100)\n",
    "plt.title(\"Distribution of Pairwise Distances\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8121ad674b9a7fe9",
   "metadata": {},
   "source": [
    "#scaler = StandardScaler()\n",
    "#reduced_user_sample_scaled = scaler.fit_transform(reduced_user_sample)\n",
    "\n",
    "#dists = pairwise_distances(reduced_user_sample_scaled)\n",
    "\n",
    "#flat = dists.flatten()\n",
    "\n",
    "#plt.figure(figsize=(7, 4))\n",
    "#plt.hist(flat, bins=100)\n",
    "#plt.title(\"Distribution of Pairwise Distances (reduced sample)\")\n",
    "#plt.xlabel(\"Distance\")\n",
    "#plt.ylabel(\"Frequency\")\n",
    "#plt.show()\n",
    "\n",
    "#too expensive to run for users, my pc dies twice"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8b710985d18ec3a6",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Hopkins test</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a754cc74cabdc12c",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Tells if our data is clusterrable at all\n",
    "\n",
    "**Behaviour:**<br>\n",
    "Randomly selects a subset of real points (X_sample), generates the same number of points uniformly at random within the data space (X_uniform). Then for each:\n",
    "* measures distance from uniform points to the nearest real point: U\n",
    "* measures distance from sampled real points to their nearest neighbor: W\n",
    "\n",
    "Finally, Computes Hopkins = U / (U + W).\n",
    "\n",
    "**Interpretation:**<br>\n",
    "H â‰ˆ 0.5: random / uniform distribution (no clusters). <br>\n",
    "H = 1.0: data is highly clusterable.<br>\n",
    "H = 0.0: data is overly regular (grid-like), also no natural clusters."
   ]
  },
  {
   "cell_type": "code",
   "id": "8d7f5bb87c68a832",
   "metadata": {},
   "source": [
    "def hopkins(X, sampling=0.1, random_state=42):\n",
    "    import numpy as np\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "    np.random.seed(random_state)\n",
    "\n",
    "    X = np.asarray(X)\n",
    "    n_samples, d = X.shape\n",
    "\n",
    "    m = int(np.ceil(sampling * n_samples))\n",
    "    if m <= 0:\n",
    "        raise ValueError(\"Sampling too small, m=0\")\n",
    "\n",
    "    idx = np.random.choice(n_samples, m, replace=False)\n",
    "    X_sample = X[idx]\n",
    "\n",
    "    X_min = X.min(axis=0)\n",
    "    X_max = X.max(axis=0)\n",
    "    X_uniform = np.random.uniform(X_min, X_max, size=(m, d))\n",
    "\n",
    "    nn_for_uniform = NearestNeighbors(n_neighbors=1).fit(X)\n",
    "    nn_for_sample = NearestNeighbors(n_neighbors=2).fit(X)\n",
    "\n",
    "    u_dist, _ = nn_for_uniform.kneighbors(X_uniform)\n",
    "    w_dist_full, _ = nn_for_sample.kneighbors(X_sample)\n",
    "    w_dist = w_dist_full[:, 1]\n",
    "\n",
    "    U = u_dist[:, 0].sum()\n",
    "    W = w_dist.sum()\n",
    "\n",
    "    return U / (U + W)\n",
    "\n",
    "H = hopkins(X_scaled, sampling=0.1) # hopkins for movies\n",
    "H_1 = hopkins(Y_scaled) # hopkins for users\n",
    "print(\"Hopkins statistic:\", H)\n",
    "print('-----------')\n",
    "print(\"Hopkins statistic:\", H_1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "21a1298cf545fed2",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Cumulative explained variance test</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ae9be749f8728a",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "To understand how much of the datasetâ€™s total variance can be captured with a reduced number of principal components.\n",
    "This helps evaluate whether the data is high-dimensional with complex structure, or whether most variation lies in just a few directions (which often indicates redundancy or low intrinsic dimensionality).\n",
    "\n",
    "**Behaviour:**<br>\n",
    "Fits PCA on all features without specifying the number of components. Then computes the explained variance ratio (variance contribution of each component) and cumulative explained variance (how much total variance is captured after adding each component). Lastly, it plots the cumulative curve to visualize how quickly variance saturates.\n",
    "\n",
    "**Interpretation:**<br>\n",
    "Shows a graph with PCA to variance corelation"
   ]
  },
  {
   "cell_type": "code",
   "id": "a7ca170b6abb3414",
   "metadata": {},
   "source": [
    "pca = PCA()\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "explained = pca.explained_variance_ratio_\n",
    "cum_explained = np.cumsum(explained)\n",
    "\n",
    "print(\"Explained variance ratio:\", np.round(explained, 3))\n",
    "print(\"Cumulative:\", np.round(cum_explained, 3))\n",
    "\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(range(1, len(explained) + 1), cum_explained, marker=\"o\")\n",
    "plt.xlabel(\"Number of principal components\")\n",
    "plt.ylabel(\"Cumulative explained variance\")\n",
    "plt.title(\"PCA â€“ cumulative explained variance\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "#too expensive for users"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc227b9c0a65cecd",
   "metadata": {},
   "source": [
    "### Plotting Statistics and Overall findings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7561da8",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Plotting Ditribution of Viewer Ratings</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e767dc73",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Show case the Distribution of the Viewer ratings\n",
    "\n",
    "**Showcasing:**<br>\n",
    "There is a clear over positive bias. This can be due to the fact that this was done on a streaming platform where users are more likely to rate higher\n",
    "\n",
    "**Output:**<br>\n",
    "Histogram showcasing Distribution of viewer ratings\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "2df1e09f",
   "metadata": {},
   "source": [
    "sync_dataframe(viewer_ratings)\n",
    "plt.figure(figsize=(11, 6)) \n",
    "ax = sns.histplot(\n",
    "    data=dfs['viewer_ratings'],\n",
    "    x='rating',\n",
    "    bins=5,\n",
    "    discrete=True, \n",
    "    color=\"#0062ff\",\n",
    "    edgecolor='white',\n",
    "    alpha=0.85,\n",
    "    linewidth=1.5\n",
    ")\n",
    "\n",
    "# Add exact counts on top of each bar\n",
    "for rect in ax.patches:\n",
    "    height = rect.get_height()\n",
    "    ax.text(\n",
    "        rect.get_x() + rect.get_width()/2., \n",
    "        height + 200_000,                    # a bit above the bar\n",
    "        f'{int(height):,}',                  # adds commas\n",
    "        ha='center', va='bottom', fontsize=12, fontweight='bold'\n",
    "    )\n",
    "\n",
    "# y-axis\n",
    "plt.ylabel('Number of Ratings (millions)', fontsize=12)\n",
    "plt.gca().yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f'{x/1e6:.1f}M')) # format in millions\n",
    "\n",
    "plt.title('Distribution of Viewer Ratings\\n(Strong Positive Bias)', \n",
    "          fontsize=15, pad=20)\n",
    "plt.xlabel('Rating (1â€“5)', fontsize=12)\n",
    "plt.xticks(range(0, 7))\n",
    "plt.ylim(0, 2_000_000)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "92b2c2fc",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Plotting Average Rating Over Time</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38eebdd0",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Viewing the years with the most engaging/interacted with movies\n",
    "\n",
    "**Showcasing:**<br>\n",
    "Plotting a timeline of average ratings over time, it showcases that the movies released in the 2000s have the most engagment, and we can clearly see that majority of the ratings over the years seem to be inbetween 2 and 4\n",
    "\n",
    "**Output:**<br>\n",
    "Plots a scatter plot showing all the average ratings with the x-axis containing the year the movie was released"
   ]
  },
  {
   "cell_type": "code",
   "id": "d15d29082834d37d",
   "metadata": {},
   "source": [
    "'Plotting a timeline of ratings over time'\n",
    "\n",
    "sync_dataframe(movies_stats)\n",
    "\n",
    "plt.figure(figsize=(15, 10))\n",
    "sns.scatterplot(data=movies_stats, x='year_of_release', y='avg_rating', alpha=0.4, color='#0062ff', edgecolor=None, s=35)\n",
    "plt.title('Movie Rating VS. Year of Release\\n(Trends Over Time)', fontsize=15, pad=20)\n",
    "plt.xlabel('Year of Release', fontsize=12)\n",
    "plt.ylabel('Average Rating Given (1â€“5)', fontsize=12)\n",
    "plt.xlim(1890, 2010) #this included the data for all the movies in the data set as it goes from 1896 to 2005\n",
    "plt.ylim(0, 7)  \n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b1bf9e56",
   "metadata": {},
   "source": [
    "<div style=\"display:flex; align-items:center;\">\n",
    "  <hr style=\"flex:1;\">\n",
    "  <span style=\"padding:0 10px;\"><b>Plotting Top 10 Movies</b></span>\n",
    "  <hr style=\"flex:1;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5284fe9",
   "metadata": {},
   "source": [
    "**Purpose:**<br>\n",
    "Showcasing top seller movies visually\n",
    "\n",
    "**Showcasing:**<br>\n",
    "Showcases the top 10 movies in the movie statistics based on average rating. Also we make a minimum of 1000 rating to get rid of the problem for getting movies with 1 really high rating. In the plot we can see that still the most popular movie has less engagement then the ones on the bottom, thius showcases that problem regarding the minimum number of interaction/ratings.\n",
    "\n",
    "**Output:**<br>\n",
    "Plots a horizontal bar chart that shows the top rated movies with there average rating, total ratings, year of release, and movie title"
   ]
  },
  {
   "cell_type": "code",
   "id": "f0548ebf42abcac1",
   "metadata": {},
   "source": [
    "def get_top_rated_movies(min_ratings=200, top_n=10):\n",
    "    sync_dataframe(movies_stats)\n",
    "    \n",
    "    # Filter and sort â€” pure movie_statistics\n",
    "    filtered = movies_stats[movies_stats['total_ratings'] >= min_ratings]\n",
    "    top_movies = filtered.sort_values('avg_rating', ascending=False).head(top_n).copy()\n",
    "    \n",
    "    # Reset index to get ranking 1 to 10\n",
    "    top_movies = top_movies.reset_index(drop=True)\n",
    "    top_movies.index = top_movies.index + 1\n",
    "    \n",
    "    # Get titles\n",
    "    titles = []\n",
    "    for movie_id in top_movies['movie_id']:\n",
    "        result = search_by_parameter(\"movies\", \"movie_id\", movie_id)\n",
    "        title = result['title'].iloc[0] if not result.empty else f\"Unknown ({movie_id})\"\n",
    "        year = int(result['year_of_release'].iloc[0]) if not result.empty and pd.notna(result['year_of_release'].iloc[0]) else \"\"\n",
    "        titles.append(f\"{title} ({year})\" if year else title)\n",
    "    \n",
    "    top_movies['title_display'] = titles\n",
    "    return top_movies\n",
    "\n",
    "# Calling the function for top 10 movies with at least 1000 ratings\n",
    "top10 = get_top_rated_movies(min_ratings=1000, top_n=10)\n",
    "\n",
    "plt.figure(figsize=(14, 9))\n",
    "\n",
    "top10_Reversed = top10.sort_values('avg_rating', ascending=True)\n",
    "\n",
    "bars = plt.barh(\n",
    "    y=top10_Reversed['title_display'],\n",
    "    width=top10_Reversed['avg_rating'],\n",
    "    color='#0062ff',\n",
    "    edgecolor='navy',\n",
    "    height=0.7,\n",
    "    alpha=0.95\n",
    ")\n",
    "\n",
    "# Adding the exact rating + number of ratings on each bar\n",
    "for bar, row in zip(bars, top10_Reversed.itertuples()):\n",
    "    plt.text(\n",
    "        x=bar.get_width() + 0.005,                    # slightly right of bar\n",
    "        y=bar.get_y() + bar.get_height()/2,           # center vertically\n",
    "        s=f\"{row.avg_rating:.3f}  ({row.total_ratings:,} ratings)\",\n",
    "        va='center',\n",
    "        ha='left',\n",
    "        fontsize=11,\n",
    "        fontweight='bold',\n",
    "        color='darkblue'\n",
    "    )\n",
    "\n",
    "plt.title('Top 10 Highest-Rated Movies (min 1000 ratings)', \n",
    "          fontsize=20, pad=40, fontweight='bold')\n",
    "plt.xlabel('Average Rating (1â€“5)', fontsize=14)\n",
    "plt.ylabel('Movie Title', fontsize=14)\n",
    "plt.xlim(4.0, 5.0)\n",
    "plt.grid(True, axis='x', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.yticks(fontsize=13)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b91a5400",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7e3da7",
   "metadata": {},
   "source": [
    "K-means clustering model for movies"
   ]
  },
  {
   "cell_type": "code",
   "id": "2a82b6c4",
   "metadata": {},
   "source": [
    "sync_dataframe(merged_data)\n",
    "\n",
    "# making a table 1 row - 1 movie\n",
    "movies_for_clustering = (\n",
    "    merged_data\n",
    "    .groupby(\"movie_id\")[movie_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# getting all number variables except for movie_id\n",
    "X = movies_for_clustering[movie_features].astype(float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# number of clusters\n",
    "n_clusters = 4\n",
    "\n",
    "kmeans = KMeans(\n",
    "    n_clusters=n_clusters,\n",
    "    random_state=42,\n",
    "    n_init=10\n",
    ")\n",
    "\n",
    "cluster_labels = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# adding cluster ids to the table\n",
    "movies_for_clustering[\"cluster_id\"] = cluster_labels\n",
    "\n",
    "# saving\n",
    "add_new_df('movie_clusters', 'movie_clusters', movies_for_clustering)\n",
    "\n",
    "elbow_df = elbow_method(X_scaled, 1, 20)\n",
    "print(elbow_df)\n",
    "\n",
    "# printing how many movies in each cluster, only for debugging\n",
    "cluster_movies_amount_str = movies_for_clustering[\"cluster_id\"].value_counts().sort_index()\n",
    "print(f\"---movies in each cluster--- \\n{cluster_movies_amount_str}\")\n",
    "# Silhouette score\n",
    "print(compute_silhouette_score(X_scaled, cluster_labels))\n",
    "\n",
    "inspect_movie_cluster(movies_for_clustering, merged_data, 0)\n",
    "plot_clusters_pca_2d(X_scaled, cluster_labels, max_points_per_cluster=800)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4537ce683798714d",
   "metadata": {},
   "source": [
    "K-means clustering model for movies without the outliers"
   ]
  },
  {
   "cell_type": "code",
   "id": "d32340e9be7d2ae7",
   "metadata": {},
   "source": [
    "mask_no3 = movies_for_clustering[\"cluster_id\"] != 2\n",
    "movies_no3 = movies_for_clustering[mask_no3].copy()\n",
    "\n",
    "X_scaled_no3 = X_scaled[mask_no3.values]\n",
    "\n",
    "# Re-run KMeans on the reduced dataset\n",
    "n_clusters_new = 3  # keep 4 to see how cluster structure changes after removing old cluster 3\n",
    "\n",
    "kmeans_no3 = KMeans(\n",
    "    n_clusters=n_clusters_new,\n",
    "    random_state=42,\n",
    "    n_init=10\n",
    ")\n",
    "\n",
    "cluster_labels_no3 = kmeans_no3.fit_predict(X_scaled_no3)\n",
    "\n",
    "# Attach the *new* cluster ids\n",
    "# (original movies_for_clustering is untouched, we work only on movies_no3)\n",
    "movies_no3[\"cluster_id\"] = cluster_labels_no3\n",
    "\n",
    "# Save as a separate df in dfs\n",
    "add_new_df(\"movie_clusters_no3\", \"movie_clusters_no3\", movies_no3)\n",
    "\n",
    "# Elbow on the reduced dataset (optional, to see if k changed)\n",
    "elbow_df_no3 = elbow_method(X_scaled_no3, 1, 20)\n",
    "print(elbow_df_no3)\n",
    "\n",
    "# Cluster sizes for the new run\n",
    "cluster_movies_amount_str_no3 = movies_no3[\"cluster_id\"].value_counts().sort_index()\n",
    "print(cluster_movies_amount_str_no3)\n",
    "\n",
    "# Silhouette score for the new clustering\n",
    "print(f\"Silhouette score: {compute_silhouette_score(X_scaled_no3, cluster_labels_no3)}\")\n",
    "\n",
    "# Quick inspection of one of the new clusters\n",
    "inspect_movie_cluster(movies_no3, merged_data, 0)\n",
    "\n",
    "# PCA 2D visualization for the new clustering\n",
    "plot_clusters_pca_2d(X_scaled_no3, cluster_labels_no3, max_points_per_cluster=800)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ffd7a4fd5c8f4bc0",
   "metadata": {},
   "source": [
    "Agglomerative clustering model for movies"
   ]
  },
  {
   "cell_type": "code",
   "id": "14b898e91d33f897",
   "metadata": {},
   "source": [
    "sync_dataframe(merged_data)\n",
    "\n",
    "# One row per movie\n",
    "movies_for_h_clustering = (\n",
    "    merged_data\n",
    "    .groupby(\"movie_id\")[movie_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(movies_for_h_clustering.head())\n",
    "\n",
    "# Matrix of fs\n",
    "X = movies_for_h_clustering[movie_features].astype(float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "Z = linkage(X_scaled, method=\"ward\")\n",
    "\n",
    "# Building a dendogram\n",
    "plt.figure(figsize=(12, 6))\n",
    "dendrogram(\n",
    "    Z,\n",
    "    truncate_mode=\"lastp\",\n",
    "    p=50,\n",
    "    leaf_rotation=90.,\n",
    "    leaf_font_size=8.,\n",
    "    show_contracted=True\n",
    ")\n",
    "plt.title(\"Movie clustering dendrogram (truncated)\")\n",
    "plt.xlabel(\"Cluster index or movie groups\")\n",
    "plt.ylabel(\"Distance\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "n_clusters = 4  # number of clusters\n",
    "\n",
    "h_cluster_labels = fcluster(Z, t=n_clusters, criterion=\"maxclust\")\n",
    "\n",
    "movies_for_h_clustering[\"cluster_id\"] = h_cluster_labels - 1\n",
    "\n",
    "# printing how many movies in each cluster, only for debugging\n",
    "h_cluster_movies_amount_str = movies_for_h_clustering[\"cluster_id\"].value_counts().sort_index()\n",
    "print(f\"---movies in each cluster--- \\n{h_cluster_movies_amount_str}\")\n",
    "# Silhouette score\n",
    "hier_labels = movies_for_h_clustering[\"cluster_id\"].to_numpy()\n",
    "print(compute_silhouette_score(X_scaled, hier_labels))\n",
    "\n",
    "# saving\n",
    "add_new_df(\"movie_clusters_h\", \"movie_clusters_h\", movies_for_h_clustering)\n",
    "inspect_movie_cluster(movie_clusters_h, merged_data, 0)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "60ba9807348b7f6a",
   "metadata": {},
   "source": [
    "GMM clustering model for movies"
   ]
  },
  {
   "cell_type": "code",
   "id": "a0bd0c95a05074c5",
   "metadata": {},
   "source": [
    "sync_dataframe(merged_data)\n",
    "\n",
    "movies_for_gmm = (\n",
    "    merged_data\n",
    "    .groupby(\"movie_id\")[movie_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X = movies_for_gmm[movie_features].astype(float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "k_values = range(1, 11)\n",
    "bic_values = []\n",
    "aic_values = []\n",
    "\n",
    "for k in k_values:\n",
    "    gmm = GaussianMixture(\n",
    "        n_components=k,\n",
    "        covariance_type=\"full\",\n",
    "        random_state=42,\n",
    "        n_init=5\n",
    "    )\n",
    "    gmm.fit(X_scaled)\n",
    "\n",
    "    bic_values.append(gmm.bic(X_scaled))\n",
    "    aic_values.append(gmm.aic(X_scaled))\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(k_values, bic_values, marker=\"o\", label=\"BIC\")\n",
    "plt.plot(k_values, aic_values, marker=\"o\", label=\"AIC\")\n",
    "plt.title(\"BIC/AIC for different n_components\")\n",
    "plt.xlabel(\"Number of (k)\")\n",
    "plt.ylabel(\"BIC/AIC\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "73ce7c5132a21a19",
   "metadata": {},
   "source": [
    "sync_dataframe(merged_data)\n",
    "\n",
    "movies_for_clustering_gmm = (\n",
    "    merged_data\n",
    "    .groupby(\"movie_id\")[movie_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "X = movies_for_clustering_gmm[movie_features].astype(float)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "gmm = GaussianMixture(\n",
    "    n_components=4,\n",
    "    covariance_type=\"full\",\n",
    "    random_state=42,\n",
    "    n_init=10\n",
    ")\n",
    "\n",
    "gmm_labels = gmm.fit_predict(X_scaled)\n",
    "\n",
    "movies_for_clustering_gmm[\"gmm_cluster_id\"] = gmm_labels\n",
    "\n",
    "add_new_df(\"movie_clusters_gmm\",\"movie_clusters_gmm\",movies_for_clustering_gmm)\n",
    "\n",
    "gmm_s_score = compute_silhouette_score(\n",
    "    X_scaled,\n",
    "    gmm_labels,\n",
    ")\n",
    "\n",
    "print(f\"GMM silhouette score: {gmm_s_score:.4f}\")\n",
    "plot_clusters_pca_2d(X_scaled, gmm_labels)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "125eb1d544b412c5",
   "metadata": {},
   "source": [
    "DBSCAN clustering model for movies"
   ]
  },
  {
   "cell_type": "code",
   "id": "7beef9d5c5a125ee",
   "metadata": {},
   "source": [
    "eps = 0.7\n",
    "min_samples = 15\n",
    "\n",
    "dbscan = DBSCAN(\n",
    "    eps=eps,\n",
    "    min_samples=min_samples,\n",
    "    metric=\"euclidean\",\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "labels = dbscan.fit_predict(X_scaled)\n",
    "\n",
    "movies_for_clustering_db = (\n",
    "    merged_data\n",
    "    .groupby(\"movie_id\")[movie_features]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "movies_for_clustering_db[\"dbscan_cluster_id\"] = labels\n",
    "\n",
    "cluster_counts = (\n",
    "    movies_for_clustering_db[\"dbscan_cluster_id\"]\n",
    "    .value_counts()\n",
    "    .sort_index()\n",
    ")\n",
    "\n",
    "print(cluster_counts)\n",
    "\n",
    "unique_labels = set(labels)\n",
    "n_clusters = len(unique_labels) - (1 if -1 in unique_labels else 0)\n",
    "noise = (labels == -1).sum()\n",
    "\n",
    "print(f\"Clusters: {n_clusters}\")\n",
    "print(f\"Noise: {noise}\")\n",
    "\n",
    "mask = labels != -1\n",
    "if n_clusters > 1 and mask.sum() > 0:\n",
    "    sil = compute_silhouette_score(X_scaled[mask], labels[mask])\n",
    "    print(\"Silhouette:\", sil)\n",
    "\n",
    "plot_dbscan_pca_2d(\n",
    "    X_scaled,\n",
    "    labels,\n",
    "    max_points_per_cluster=1000,\n",
    "    title=f\"DBSCAN movie clusters (eps={eps}, min_samples={min_samples})\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "eab44a3ab5106803",
   "metadata": {},
   "source": [
    "eps_list = [.3, .4, 0.50, 0.60, 0.70, 0.75, 0.85, 1.0]\n",
    "\n",
    "def test_eps_values(X_scaled, eps_list, min_samples=15):\n",
    "    print(\"eps  | clusters | noise | silhouette\")\n",
    "    print(\"-------------------------------------\")\n",
    "\n",
    "    for eps in eps_list:\n",
    "        model = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "        labels = model.fit_predict(X_scaled)\n",
    "\n",
    "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
    "        noise = (labels == -1).sum()\n",
    "\n",
    "        if n_clusters > 1:\n",
    "            mask = labels != -1\n",
    "            sil = compute_silhouette_score(X_scaled[mask], labels[mask])\n",
    "            sil = round(sil, 3)\n",
    "        else:\n",
    "            sil = None\n",
    "\n",
    "        print(f\"{eps:<4} | {n_clusters:<8} | {noise:<5} | {sil}\")\n",
    "\n",
    "test_eps_values(X_scaled, eps_list)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "98fa8f162da14edc",
   "metadata": {},
   "source": [
    "K-means clustering model for users"
   ]
  },
  {
   "cell_type": "code",
   "id": "f8b3aef28c1f0350",
   "metadata": {},
   "source": [
    "sync_dataframe(merged_data)\n",
    "data = merged_data.copy()\n",
    "\n",
    "# Taking first row per user\n",
    "users_for_clustering = (\n",
    "    data.groupby(\"customer_id\")[user_features_full]\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "data = reduced_user_sample[user_features_full].astype(float)\n",
    "data = data[user_features_full].dropna()\n",
    "\n",
    "scalar = StandardScaler()\n",
    "scaled_data = scalar.fit_transform(data)\n",
    "\n",
    "print(elbow_method(data))\n",
    "\n",
    "# k-means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto')\n",
    "cluster_labels = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "reduced_user_sample[\"cluster_id\"] = cluster_labels\n",
    "print(reduced_user_sample[\"cluster_id\"].value_counts().sort_index())\n",
    "print(compute_silhouette_score(scaled_data, cluster_labels, 20000))\n",
    "\n",
    "plot_clusters_pca_2d(scaled_data, cluster_labels)\n",
    "\n",
    "#add_new_df(\"user_clusters\", \"user_clusters\", users_for_clustering)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f729ed027bec6d89",
   "metadata": {},
   "source": [
    "K-means clustering model for users (two most significant)"
   ]
  },
  {
   "cell_type": "code",
   "id": "22aa1216d868f3dd",
   "metadata": {},
   "source": [
    "sync_dataframe(merged_data)\n",
    "data = merged_data.copy()\n",
    "\n",
    "data = users_for_clustering[user_features].astype(float)\n",
    "data = data[user_features].dropna()\n",
    "\n",
    "scalar = StandardScaler()\n",
    "scaled_data = scalar.fit_transform(data)\n",
    "\n",
    "print(elbow_method(data))\n",
    "\n",
    "# k-means\n",
    "kmeans = KMeans(n_clusters=3, random_state=42, n_init='auto')\n",
    "cluster_labels = kmeans.fit_predict(scaled_data)\n",
    "\n",
    "users_for_clustering[\"cluster_id\"] = cluster_labels\n",
    "print(users_for_clustering[\"cluster_id\"].value_counts().sort_index())\n",
    "print(compute_silhouette_score(scaled_data, cluster_labels, 20000))\n",
    "\n",
    "plot_clusters_pca_2d(scaled_data, cluster_labels)\n",
    "\n",
    "#add_new_df(\"user_clusters\", \"user_clusters\", users_for_clustering)"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
